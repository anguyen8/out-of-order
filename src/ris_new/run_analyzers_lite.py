import numpy as np
import pickle
import datetime
import statistics
from os.path import join, exists, isfile
from attribution_example import AttrExample
from transformers.data.processors.utils import InputExample

from tqdm import tqdm
from sklearn.metrics import auc
from scipy.special import softmax
from scipy import stats
import copy
import math

import time
from functools import wraps

# from lime.lime_text import LimeTextExplainer
from lime_text import LimeTextExplainer

from src.ris.attribution_visualization import AttributionVisualization
from src.ris.roberta_model_wrapper import RoBERTa_Model_Wrapper
from src.ris.utils import *

import itertools
import random
from collections import Counter

from matplotlib import pylab as plt
from matplotlib.transforms import Affine2D
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']

import spacy
nlp = spacy.load("en_core_web_sm")

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForMaskedLM
from scipy.special import softmax


class Phrase(object):
    def __init__(self, id, text, score):
        self.id = id
        self.text = text
        self.score = score

class Sentence(object):
    def __init__(self, id, text, score):
        self.id = id
        self.text = text
        self.score = score
        self.phrases = []
        self.split = 0  # 1: train / 2: dev / 3: test

    def add_phrase(self, id, text, score):
        phrase = Phrase(id, text, score)
        self.phrases.append(phrase)

class Example(object):
    def __init__(self, id, text_a, text_b, label):
        self.id = id
        self.text_a = text_a
        self.text_b = text_b
        self.label = label
        self.highlights = {"annotator1": {"text_a": [], "text_b": []},
                           "annotator2": {"text_a": [], "text_b": []},
                           "annotator3": {"text_a": [], "text_b": []},}
        self.sentences = []   # For MultiRC ONLY

        self.processed_length = 0
        self.split = 0        # 1: train / 3: dev / 2: test


# Python program to print colored text and background
class colors:
    '''Colors class:reset all colors with colors.reset; two sub classes fg for foreground
    and bg for background; use as colors.subclass.colorname.
    i.e. colors.fg.red or colors.bg.greenalso, the generic bold, disable, underline, reverse, strike through,
    and invisible work with the main class i.e. colors.bold'''

    reset = '\033[0m'
    bold = '\033[01m'
    disable = '\033[02m'
    underline = '\033[04m'
    reverse = '\033[07m'
    strikethrough = '\033[09m'
    invisible = '\033[08m'

    class fg:
        black = '\033[30m'
        red = '\033[31m'
        green = '\033[32m'
        orange = '\033[33m'
        blue = '\033[34m'
        purple = '\033[35m'
        cyan = '\033[36m'
        lightgrey = '\033[37m'
        darkgrey = '\033[90m'
        lightred = '\033[91m'
        lightgreen = '\033[92m'
        yellow = '\033[93m'
        lightblue = '\033[94m'
        pink = '\033[95m'
        lightcyan = '\033[96m'

    class bg:
        black = '\033[40m'
        red = '\033[41m'
        green = '\033[42m'
        orange = '\033[43m'
        blue = '\033[44m'
        purple = '\033[45m'
        cyan = '\033[46m'
        lightgrey = '\033[47m'


MASKED_EXAMPLES = "masked_examples"
FILLED_EXAMPLES = "filled_examples"
INPUT_EXAMPLES = "input_examples"
CHUNK_SIZE_LIST = "chunk_size_list"
ALL_CONF_SCORES = "all_conf_scores"

folder_name_dict = {
    "esnli": "ESNLI",
    "sst": "SST",
    "sst-2": "SST-2",
    "multirc": "MultiRC",
}

skipped_indices = {

    "sst-2": [],
    "sst": [],
    "esnli": [],

    # Skip 3447 examples to get 6395 human-annotated examples in both premise and hypothesis (3 annotators)
    # "esnli": [0, 1, 4, 7, 11, 12, 16, 22, 24, 32, 35, 38, 40, 41, 44, 47, 48, 51, 54, 59, 61, 64, 67, 70, 73, 76, 77, 80, 85, 89, 92, 98, 101, 103, 105, 107, 111, 115, 121, 122, 123, 125, 130, 131, 133, 134, 136, 139, 140, 141, 142, 143, 151, 153, 155, 156, 160, 161, 164, 167, 168, 170, 174, 175, 177, 180, 184, 186, 189, 192, 196, 200, 204, 207, 209, 212, 213, 215, 217, 218, 220, 223, 226, 229, 234, 235, 239, 240, 245, 249, 253, 254, 256, 258, 264, 272, 273, 277, 279, 281, 286, 288, 292, 294, 297, 302, 306, 308, 311, 316, 317, 319, 321, 324, 326, 329, 330, 332, 333, 336, 337, 340, 341, 345, 347, 356, 357, 358, 360, 362, 365, 369, 373, 374, 377, 381, 384, 387, 390, 394, 396, 399, 401, 406, 408, 412, 415, 418, 419, 421, 422, 431, 433, 437, 438, 442, 448, 451, 454, 459, 461, 466, 467, 474, 475, 477, 483, 485, 490, 492, 493, 495, 502, 504, 508, 510, 512, 513, 516, 518, 519, 522, 524, 528, 531, 534, 536, 539, 541, 544, 549, 550, 551, 554, 557, 560, 562, 566, 569, 576, 579, 580, 583, 585, 587, 590, 592, 595, 598, 603, 604, 608, 612, 615, 616, 618, 621, 623, 624, 625, 627, 628, 631, 633, 635, 636, 640, 642, 646, 647, 651, 654, 658, 659, 664, 667, 670, 671, 675, 679, 681, 685, 688, 694, 697, 699, 700, 702, 704, 705, 707, 709, 711, 714, 722, 723, 726, 730, 733, 736, 737, 739, 741, 744, 746, 747, 749, 756, 758, 761, 765, 766, 771, 776, 779, 781, 784, 786, 793, 795, 799, 800, 804, 807, 809, 812, 816, 819, 820, 821, 826, 827, 830, 833, 837, 840, 843, 846, 847, 850, 853, 854, 857, 859, 860, 866, 870, 872, 876, 877, 881, 883, 887, 890, 891, 892, 896, 899, 902, 903, 905, 908, 911, 912, 915, 916, 920, 924, 927, 929, 932, 934, 936, 937, 940, 944, 946, 950, 952, 955, 961, 965, 966, 967, 970, 974, 977, 978, 980, 983, 985, 991, 993, 998, 999, 1000, 1001, 1004, 1005, 1009, 1012, 1016, 1017, 1022, 1025, 1028, 1030, 1032, 1037, 1039, 1043, 1044, 1045, 1047, 1048, 1049, 1052, 1054, 1057, 1059, 1061, 1062, 1067, 1071, 1074, 1075, 1078, 1081, 1085, 1086, 1087, 1091, 1093, 1097, 1098, 1104, 1105, 1107, 1110, 1114, 1115, 1118, 1121, 1126, 1132, 1134, 1138, 1140, 1144, 1147, 1150, 1153, 1154, 1158, 1159, 1162, 1163, 1167, 1169, 1172, 1177, 1180, 1183, 1185, 1189, 1192, 1193, 1196, 1197, 1199, 1202, 1206, 1207, 1210, 1213, 1216, 1218, 1219, 1221, 1224, 1227, 1228, 1231, 1233, 1236, 1238, 1242, 1243, 1248, 1250, 1251, 1254, 1256, 1258, 1263, 1264, 1267, 1272, 1274, 1278, 1280, 1287, 1289, 1290, 1291, 1292, 1293, 1297, 1299, 1302, 1305, 1306, 1310, 1312, 1313, 1314, 1317, 1318, 1319, 1326, 1330, 1331, 1333, 1338, 1341, 1343, 1345, 1346, 1350, 1352, 1354, 1355, 1358, 1360, 1362, 1368, 1371, 1373, 1378, 1379, 1384, 1387, 1390, 1392, 1395, 1398, 1401, 1404, 1406, 1408, 1410, 1412, 1416, 1418, 1422, 1424, 1429, 1432, 1435, 1437, 1440, 1444, 1446, 1450, 1451, 1454, 1459, 1461, 1465, 1468, 1469, 1474, 1476, 1478, 1480, 1481, 1483, 1487, 1488, 1491, 1495, 1496, 1499, 1504, 1505, 1510, 1512, 1515, 1524, 1525, 1528, 1529, 1532, 1533, 1536, 1539, 1540, 1543, 1545, 1548, 1549, 1554, 1557, 1560, 1561, 1565, 1567, 1570, 1572, 1575, 1576, 1577, 1579, 1580, 1584, 1587, 1588, 1593, 1594, 1597, 1600, 1605, 1608, 1609, 1614, 1617, 1619, 1623, 1624, 1627, 1633, 1638, 1640, 1642, 1647, 1648, 1650, 1651, 1654, 1657, 1660, 1663, 1665, 1668, 1671, 1672, 1674, 1680, 1681, 1684, 1688, 1690, 1694, 1696, 1699, 1700, 1704, 1707, 1710, 1711, 1713, 1714, 1715, 1717, 1718, 1722, 1725, 1727, 1731, 1735, 1738, 1739, 1741, 1744, 1745, 1750, 1753, 1754, 1756, 1757, 1760, 1764, 1766, 1771, 1773, 1775, 1777, 1780, 1783, 1784, 1787, 1788, 1794, 1795, 1798, 1801, 1803, 1807, 1810, 1812, 1816, 1819, 1822, 1825, 1829, 1832, 1836, 1837, 1842, 1843, 1848, 1852, 1856, 1859, 1863, 1864, 1865, 1868, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1885, 1886, 1893, 1894, 1899, 1900, 1905, 1906, 1911, 1913, 1914, 1916, 1919, 1920, 1921, 1924, 1925, 1931, 1935, 1936, 1938, 1940, 1942, 1944, 1945, 1948, 1952, 1953, 1954, 1963, 1965, 1966, 1967, 1971, 1972, 1981, 1985, 1988, 1995, 1998, 2001, 2003, 2007, 2008, 2011, 2012, 2014, 2017, 2018, 2020, 2021, 2024, 2026, 2030, 2034, 2037, 2038, 2042, 2045, 2047, 2053, 2056, 2059, 2062, 2064, 2069, 2072, 2074, 2078, 2081, 2083, 2087, 2089, 2090, 2093, 2096, 2099, 2102, 2103, 2107, 2108, 2109, 2112, 2114, 2116, 2117, 2118, 2121, 2126, 2128, 2131, 2133, 2134, 2137, 2139, 2141, 2143, 2145, 2148, 2150, 2151, 2156, 2159, 2162, 2164, 2166, 2171, 2174, 2176, 2179, 2182, 2188, 2189, 2191, 2192, 2195, 2197, 2199, 2201, 2202, 2205, 2206, 2209, 2211, 2218, 2220, 2222, 2224, 2226, 2229, 2234, 2236, 2240, 2242, 2244, 2246, 2248, 2251, 2253, 2257, 2258, 2263, 2266, 2267, 2269, 2271, 2275, 2276, 2279, 2283, 2286, 2288, 2291, 2298, 2299, 2300, 2301, 2302, 2303, 2308, 2311, 2312, 2316, 2320, 2322, 2324, 2327, 2328, 2331, 2334, 2338, 2339, 2343, 2345, 2348, 2349, 2351, 2355, 2359, 2362, 2363, 2367, 2368, 2370, 2373, 2377, 2379, 2381, 2383, 2387, 2391, 2394, 2396, 2399, 2400, 2401, 2404, 2405, 2408, 2414, 2415, 2418, 2419, 2422, 2424, 2425, 2430, 2434, 2437, 2442, 2444, 2447, 2448, 2451, 2453, 2457, 2458, 2465, 2468, 2471, 2474, 2476, 2478, 2481, 2483, 2486, 2489, 2494, 2496, 2497, 2501, 2510, 2512, 2517, 2519, 2524, 2527, 2530, 2535, 2538, 2540, 2543, 2544, 2545, 2548, 2552, 2553, 2555, 2558, 2560, 2565, 2566, 2569, 2572, 2576, 2579, 2582, 2585, 2589, 2590, 2593, 2598, 2601, 2602, 2606, 2609, 2611, 2615, 2619, 2621, 2624, 2629, 2632, 2634, 2636, 2637, 2642, 2645, 2647, 2648, 2651, 2652, 2656, 2658, 2659, 2661, 2665, 2667, 2674, 2678, 2682, 2685, 2686, 2689, 2692, 2696, 2699, 2704, 2705, 2711, 2713, 2716, 2718, 2720, 2722, 2726, 2733, 2734, 2738, 2744, 2747, 2757, 2759, 2763, 2764, 2768, 2771, 2775, 2778, 2783, 2787, 2790, 2793, 2794, 2798, 2801, 2802, 2806, 2807, 2809, 2811, 2815, 2816, 2821, 2823, 2830, 2833, 2837, 2838, 2844, 2850, 2852, 2855, 2860, 2864, 2867, 2871, 2874, 2875, 2879, 2882, 2885, 2887, 2889, 2892, 2893, 2896, 2900, 2904, 2906, 2913, 2914, 2917, 2924, 2927, 2931, 2935, 2936, 2938, 2940, 2943, 2946, 2949, 2954, 2960, 2964, 2968, 2971, 2973, 2977, 2978, 2983, 2984, 2989, 2996, 3000, 3001, 3005, 3009, 3011, 3015, 3018, 3019, 3020, 3022, 3026, 3031, 3034, 3039, 3040, 3043, 3048, 3051, 3052, 3054, 3057, 3060, 3061, 3064, 3069, 3072, 3073, 3081, 3084, 3086, 3090, 3091, 3096, 3098, 3100, 3106, 3109, 3110, 3111, 3115, 3119, 3120, 3123, 3125, 3128, 3131, 3133, 3136, 3139, 3143, 3145, 3148, 3153, 3157, 3159, 3163, 3164, 3166, 3170, 3173, 3174, 3175, 3177, 3179, 3181, 3183, 3188, 3191, 3193, 3197, 3199, 3201, 3204, 3207, 3212, 3214, 3219, 3223, 3227, 3230, 3232, 3235, 3237, 3242, 3243, 3247, 3248, 3254, 3256, 3259, 3262, 3264, 3266, 3272, 3273, 3274, 3277, 3279, 3282, 3286, 3291, 3295, 3303, 3305, 3308, 3312, 3314, 3319, 3322, 3323, 3326, 3336, 3339, 3343, 3344, 3348, 3351, 3355, 3359, 3364, 3367, 3370, 3372, 3376, 3377, 3380, 3383, 3385, 3388, 3391, 3395, 3397, 3401, 3404, 3406, 3408, 3411, 3414, 3415, 3418, 3423, 3426, 3427, 3430, 3432, 3436, 3439, 3440, 3443, 3446, 3449, 3452, 3455, 3459, 3464, 3470, 3472, 3474, 3477, 3483, 3484, 3487, 3490, 3494, 3497, 3499, 3502, 3503, 3504, 3507, 3512, 3514, 3516, 3520, 3521, 3526, 3527, 3529, 3531, 3534, 3535, 3536, 3540, 3543, 3544, 3546, 3549, 3551, 3552, 3555, 3558, 3560, 3564, 3566, 3567, 3572, 3575, 3576, 3579, 3581, 3585, 3586, 3593, 3595, 3600, 3601, 3602, 3605, 3606, 3607, 3613, 3616, 3617, 3623, 3626, 3630, 3631, 3633, 3636, 3637, 3639, 3647, 3648, 3651, 3655, 3658, 3660, 3663, 3670, 3673, 3674, 3677, 3680, 3683, 3686, 3689, 3694, 3695, 3699, 3702, 3704, 3707, 3712, 3713, 3717, 3719, 3722, 3725, 3728, 3731, 3736, 3739, 3742, 3744, 3749, 3756, 3757, 3759, 3762, 3763, 3765, 3768, 3772, 3773, 3778, 3781, 3782, 3786, 3791, 3793, 3798, 3800, 3804, 3809, 3810, 3813, 3815, 3817, 3821, 3823, 3827, 3835, 3836, 3837, 3845, 3847, 3850, 3853, 3854, 3855, 3864, 3866, 3868, 3871, 3873, 3877, 3879, 3886, 3887, 3890, 3893, 3898, 3900, 3903, 3904, 3906, 3908, 3911, 3912, 3916, 3918, 3920, 3925, 3926, 3931, 3932, 3933, 3935, 3938, 3939, 3945, 3947, 3950, 3952, 3953, 3954, 3961, 3963, 3966, 3967, 3969, 3972, 3975, 3979, 3980, 3988, 3990, 3994, 3996, 4001, 4010, 4015, 4018, 4022, 4024, 4026, 4027, 4030, 4035, 4036, 4037, 4040, 4044, 4045, 4050, 4053, 4054, 4059, 4064, 4067, 4069, 4071, 4073, 4075, 4082, 4083, 4086, 4087, 4089, 4094, 4096, 4100, 4103, 4106, 4107, 4110, 4114, 4118, 4121, 4124, 4125, 4129, 4130, 4131, 4133, 4135, 4137, 4138, 4140, 4144, 4147, 4149, 4151, 4157, 4159, 4163, 4165, 4168, 4170, 4172, 4173, 4178, 4179, 4182, 4183, 4185, 4189, 4191, 4196, 4197, 4199, 4200, 4201, 4203, 4204, 4210, 4213, 4215, 4219, 4221, 4225, 4227, 4230, 4235, 4236, 4242, 4244, 4247, 4250, 4252, 4255, 4257, 4260, 4262, 4264, 4265, 4266, 4269, 4271, 4275, 4279, 4282, 4284, 4287, 4288, 4290, 4295, 4298, 4301, 4302, 4305, 4309, 4310, 4312, 4315, 4317, 4319, 4323, 4329, 4331, 4336, 4339, 4340, 4343, 4347, 4350, 4356, 4358, 4362, 4364, 4367, 4373, 4375, 4377, 4379, 4384, 4388, 4390, 4391, 4393, 4394, 4397, 4398, 4402, 4403, 4407, 4409, 4413, 4416, 4417, 4418, 4422, 4426, 4428, 4429, 4431, 4434, 4436, 4440, 4441, 4444, 4446, 4452, 4458, 4460, 4464, 4467, 4473, 4475, 4479, 4483, 4486, 4487, 4489, 4493, 4494, 4497, 4498, 4502, 4507, 4511, 4512, 4518, 4523, 4527, 4529, 4531, 4534, 4538, 4540, 4541, 4544, 4548, 4550, 4552, 4557, 4559, 4560, 4563, 4564, 4568, 4570, 4572, 4574, 4578, 4584, 4586, 4589, 4591, 4595, 4597, 4601, 4609, 4612, 4617, 4622, 4623, 4626, 4628, 4635, 4637, 4639, 4642, 4645, 4646, 4648, 4649, 4652, 4655, 4659, 4663, 4667, 4672, 4674, 4683, 4686, 4688, 4689, 4693, 4695, 4698, 4700, 4704, 4706, 4711, 4713, 4715, 4719, 4722, 4725, 4729, 4731, 4733, 4739, 4740, 4742, 4749, 4750, 4755, 4756, 4759, 4763, 4765, 4768, 4770, 4773, 4779, 4782, 4783, 4788, 4789, 4794, 4798, 4802, 4806, 4808, 4810, 4815, 4817, 4820, 4825, 4830, 4831, 4833, 4835, 4838, 4841, 4844, 4845, 4848, 4851, 4853, 4854, 4855, 4859, 4863, 4864, 4866, 4868, 4870, 4875, 4877, 4881, 4884, 4885, 4887, 4892, 4895, 4899, 4902, 4905, 4907, 4910, 4911, 4914, 4915, 4918, 4920, 4923, 4925, 4928, 4932, 4934, 4938, 4941, 4944, 4946, 4948, 4951, 4955, 4961, 4964, 4966, 4970, 4973, 4977, 4980, 4984, 4988, 4991, 4997, 5004, 5006, 5010, 5012, 5015, 5017, 5020, 5021, 5024, 5027, 5028, 5032, 5035, 5037, 5038, 5041, 5042, 5045, 5047, 5051, 5052, 5056, 5057, 5060, 5063, 5064, 5068, 5069, 5072, 5075, 5077, 5079, 5084, 5085, 5091, 5099, 5102, 5106, 5110, 5112, 5115, 5119, 5122, 5125, 5128, 5129, 5130, 5135, 5136, 5140, 5144, 5146, 5149, 5153, 5156, 5157, 5160, 5164, 5166, 5171, 5175, 5177, 5181, 5184, 5187, 5189, 5192, 5196, 5202, 5204, 5205, 5207, 5210, 5211, 5214, 5217, 5220, 5223, 5226, 5230, 5233, 5234, 5239, 5247, 5248, 5254, 5257, 5261, 5270, 5271, 5274, 5279, 5281, 5283, 5285, 5294, 5297, 5300, 5304, 5305, 5307, 5311, 5313, 5321, 5324, 5326, 5331, 5334, 5335, 5338, 5340, 5343, 5348, 5352, 5355, 5357, 5360, 5364, 5367, 5371, 5373, 5379, 5380, 5386, 5387, 5391, 5393, 5395, 5396, 5398, 5402, 5403, 5405, 5407, 5410, 5416, 5420, 5423, 5425, 5427, 5430, 5431, 5434, 5437, 5440, 5447, 5450, 5452, 5456, 5459, 5462, 5466, 5469, 5472, 5473, 5479, 5481, 5484, 5487, 5490, 5493, 5498, 5500, 5503, 5509, 5513, 5516, 5517, 5520, 5521, 5523, 5528, 5531, 5532, 5534, 5535, 5538, 5542, 5547, 5549, 5552, 5560, 5561, 5565, 5566, 5568, 5575, 5577, 5580, 5582, 5588, 5590, 5593, 5595, 5599, 5601, 5602, 5606, 5608, 5612, 5613, 5616, 5617, 5622, 5625, 5626, 5628, 5629, 5633, 5635, 5638, 5640, 5642, 5646, 5647, 5650, 5652, 5655, 5661, 5664, 5665, 5670, 5672, 5674, 5678, 5680, 5684, 5685, 5688, 5690, 5691, 5692, 5693, 5695, 5698, 5703, 5704, 5708, 5710, 5715, 5716, 5717, 5721, 5722, 5726, 5728, 5732, 5734, 5740, 5742, 5743, 5744, 5748, 5750, 5754, 5757, 5760, 5767, 5771, 5773, 5778, 5782, 5785, 5786, 5790, 5792, 5795, 5796, 5797, 5800, 5804, 5806, 5809, 5810, 5812, 5817, 5819, 5822, 5826, 5828, 5830, 5833, 5834, 5837, 5841, 5842, 5847, 5850, 5851, 5855, 5858, 5862, 5865, 5867, 5870, 5872, 5873, 5879, 5881, 5883, 5886, 5889, 5890, 5894, 5897, 5901, 5903, 5907, 5910, 5913, 5915, 5917, 5918, 5921, 5922, 5924, 5925, 5931, 5932, 5934, 5937, 5941, 5944, 5948, 5951, 5952, 5955, 5957, 5958, 5959, 5965, 5967, 5975, 5976, 5981, 5984, 5986, 5990, 5991, 5996, 5997, 6000, 6004, 6006, 6007, 6011, 6014, 6015, 6019, 6021, 6022, 6026, 6029, 6030, 6034, 6037, 6040, 6046, 6047, 6048, 6053, 6054, 6057, 6060, 6064, 6068, 6070, 6072, 6076, 6080, 6082, 6083, 6089, 6092, 6095, 6097, 6103, 6105, 6109, 6115, 6118, 6120, 6125, 6133, 6134, 6136, 6139, 6141, 6144, 6148, 6150, 6153, 6157, 6160, 6164, 6165, 6170, 6171, 6174, 6177, 6178, 6179, 6183, 6185, 6186, 6190, 6193, 6196, 6197, 6200, 6205, 6206, 6210, 6213, 6215, 6217, 6218, 6220, 6222, 6227, 6229, 6234, 6236, 6243, 6247, 6250, 6252, 6254, 6257, 6262, 6263, 6265, 6268, 6271, 6272, 6273, 6274, 6277, 6282, 6283, 6287, 6289, 6292, 6295, 6300, 6303, 6307, 6308, 6309, 6315, 6316, 6319, 6323, 6325, 6329, 6330, 6332, 6333, 6338, 6339, 6344, 6345, 6349, 6352, 6354, 6357, 6361, 6362, 6365, 6369, 6373, 6374, 6379, 6382, 6385, 6387, 6389, 6394, 6396, 6397, 6403, 6405, 6407, 6409, 6415, 6416, 6420, 6421, 6422, 6426, 6427, 6430, 6434, 6438, 6441, 6442, 6445, 6446, 6450, 6455, 6457, 6461, 6462, 6465, 6466, 6469, 6472, 6475, 6480, 6481, 6485, 6488, 6490, 6493, 6497, 6500, 6503, 6504, 6505, 6512, 6514, 6517, 6519, 6522, 6526, 6530, 6533, 6536, 6537, 6542, 6544, 6545, 6547, 6549, 6550, 6554, 6556, 6563, 6566, 6567, 6570, 6573, 6577, 6579, 6581, 6587, 6591, 6593, 6597, 6600, 6601, 6603, 6605, 6610, 6612, 6614, 6616, 6619, 6621, 6625, 6627, 6631, 6633, 6635, 6638, 6641, 6646, 6648, 6649, 6650, 6652, 6655, 6657, 6661, 6662, 6669, 6671, 6675, 6681, 6683, 6687, 6689, 6693, 6697, 6698, 6705, 6708, 6712, 6713, 6718, 6720, 6723, 6727, 6728, 6732, 6734, 6735, 6736, 6737, 6738, 6743, 6744, 6749, 6751, 6755, 6757, 6762, 6765, 6769, 6771, 6774, 6779, 6781, 6784, 6787, 6791, 6795, 6798, 6803, 6805, 6808, 6809, 6812, 6814, 6816, 6818, 6821, 6824, 6825, 6828, 6831, 6834, 6838, 6841, 6843, 6846, 6853, 6857, 6860, 6861, 6864, 6865, 6870, 6873, 6874, 6878, 6881, 6884, 6886, 6888, 6892, 6895, 6897, 6900, 6901, 6902, 6903, 6908, 6910, 6912, 6915, 6916, 6920, 6923, 6924, 6928, 6931, 6932, 6934, 6936, 6937, 6939, 6943, 6946, 6950, 6951, 6956, 6958, 6961, 6962, 6964, 6966, 6969, 6973, 6976, 6979, 6982, 6984, 6985, 6988, 6990, 6993, 6998, 6999, 7001, 7002, 7006, 7010, 7011, 7016, 7017, 7022, 7024, 7028, 7032, 7034, 7039, 7040, 7043, 7047, 7051, 7053, 7056, 7059, 7065, 7070, 7075, 7076, 7083, 7087, 7089, 7092, 7094, 7099, 7101, 7105, 7108, 7110, 7115, 7122, 7123, 7127, 7129, 7135, 7136, 7138, 7141, 7143, 7146, 7150, 7153, 7155, 7159, 7162, 7168, 7172, 7174, 7181, 7183, 7185, 7188, 7193, 7196, 7197, 7199, 7200, 7204, 7208, 7209, 7212, 7215, 7218, 7223, 7226, 7227, 7232, 7235, 7237, 7239, 7244, 7249, 7253, 7255, 7258, 7259, 7260, 7264, 7265, 7266, 7268, 7269, 7274, 7280, 7283, 7286, 7289, 7293, 7298, 7299, 7303, 7306, 7310, 7311, 7316, 7320, 7325, 7327, 7328, 7333, 7337, 7342, 7344, 7345, 7349, 7352, 7354, 7357, 7359, 7361, 7363, 7366, 7368, 7371, 7373, 7375, 7381, 7382, 7386, 7387, 7391, 7394, 7396, 7400, 7403, 7405, 7409, 7411, 7413, 7418, 7421, 7427, 7432, 7434, 7437, 7438, 7441, 7445, 7446, 7449, 7452, 7454, 7456, 7458, 7463, 7465, 7467, 7471, 7474, 7476, 7482, 7484, 7485, 7488, 7489, 7491, 7494, 7496, 7498, 7502, 7503, 7508, 7510, 7511, 7514, 7515, 7518, 7521, 7525, 7529, 7531, 7534, 7538, 7540, 7545, 7547, 7550, 7552, 7553, 7554, 7558, 7562, 7563, 7564, 7565, 7568, 7569, 7570, 7571, 7572, 7576, 7577, 7582, 7583, 7585, 7587, 7591, 7592, 7597, 7599, 7602, 7606, 7608, 7611, 7613, 7619, 7620, 7621, 7625, 7628, 7631, 7633, 7635, 7638, 7641, 7646, 7647, 7649, 7653, 7658, 7663, 7664, 7665, 7667, 7670, 7680, 7683, 7684, 7687, 7690, 7694, 7696, 7702, 7705, 7716, 7719, 7721, 7724, 7725, 7726, 7730, 7731, 7734, 7738, 7741, 7746, 7748, 7756, 7757, 7761, 7765, 7767, 7771, 7772, 7775, 7776, 7778, 7782, 7783, 7784, 7789, 7792, 7795, 7798, 7801, 7804, 7807, 7809, 7810, 7813, 7814, 7819, 7820, 7824, 7826, 7830, 7834, 7836, 7841, 7845, 7849, 7853, 7858, 7865, 7868, 7870, 7871, 7874, 7876, 7879, 7883, 7884, 7888, 7894, 7896, 7900, 7902, 7905, 7907, 7908, 7911, 7912, 7914, 7917, 7919, 7923, 7925, 7929, 7933, 7935, 7937, 7941, 7944, 7948, 7951, 7952, 7954, 7955, 7957, 7967, 7969, 7973, 7976, 7980, 7981, 7983, 7986, 7989, 7990, 7995, 7996, 7998, 8002, 8005, 8009, 8013, 8014, 8017, 8019, 8022, 8024, 8026, 8030, 8036, 8040, 8042, 8045, 8052, 8058, 8060, 8061, 8062, 8065, 8067, 8071, 8073, 8074, 8078, 8080, 8083, 8085, 8088, 8095, 8096, 8099, 8100, 8102, 8105, 8106, 8107, 8109, 8110, 8113, 8117, 8119, 8121, 8125, 8128, 8130, 8138, 8140, 8144, 8150, 8151, 8154, 8158, 8160, 8162, 8165, 8167, 8169, 8171, 8172, 8174, 8175, 8179, 8181, 8185, 8188, 8189, 8192, 8193, 8195, 8196, 8200, 8202, 8205, 8209, 8212, 8214, 8219, 8221, 8224, 8227, 8231, 8233, 8234, 8238, 8240, 8244, 8245, 8249, 8252, 8256, 8258, 8264, 8267, 8268, 8269, 8273, 8275, 8279, 8280, 8286, 8289, 8293, 8294, 8295, 8299, 8302, 8304, 8307, 8312, 8313, 8316, 8320, 8322, 8326, 8329, 8331, 8335, 8338, 8340, 8344, 8345, 8348, 8349, 8353, 8356, 8359, 8362, 8364, 8366, 8367, 8375, 8376, 8378, 8381, 8383, 8387, 8390, 8393, 8394, 8400, 8405, 8406, 8407, 8411, 8413, 8414, 8416, 8418, 8419, 8421, 8424, 8425, 8428, 8430, 8434, 8438, 8440, 8444, 8447, 8449, 8451, 8456, 8457, 8461, 8462, 8463, 8467, 8469, 8472, 8474, 8475, 8476, 8478, 8479, 8482, 8484, 8487, 8491, 8498, 8500, 8501, 8507, 8509, 8511, 8514, 8518, 8522, 8523, 8526, 8527, 8532, 8536, 8539, 8540, 8542, 8544, 8549, 8550, 8553, 8557, 8561, 8565, 8566, 8567, 8570, 8571, 8574, 8577, 8580, 8583, 8585, 8589, 8591, 8594, 8598, 8600, 8604, 8606, 8608, 8611, 8614, 8619, 8622, 8623, 8625, 8627, 8630, 8632, 8637, 8642, 8646, 8649, 8653, 8655, 8661, 8665, 8667, 8670, 8671, 8672, 8675, 8677, 8679, 8682, 8685, 8687, 8691, 8694, 8696, 8698, 8701, 8704, 8706, 8711, 8713, 8715, 8718, 8721, 8724, 8727, 8731, 8734, 8737, 8738, 8743, 8744, 8748, 8749, 8752, 8753, 8756, 8759, 8761, 8766, 8769, 8772, 8775, 8777, 8780, 8782, 8787, 8789, 8790, 8793, 8795, 8798, 8800, 8802, 8804, 8807, 8810, 8812, 8814, 8817, 8823, 8824, 8829, 8832, 8834, 8837, 8838, 8839, 8840, 8842, 8847, 8849, 8851, 8853, 8854, 8855, 8859, 8864, 8866, 8870, 8871, 8874, 8879, 8881, 8882, 8883, 8888, 8889, 8890, 8894, 8895, 8898, 8900, 8903, 8906, 8907, 8911, 8913, 8917, 8919, 8923, 8924, 8927, 8928, 8932, 8934, 8935, 8938, 8939, 8944, 8945, 8948, 8951, 8956, 8957, 8959, 8962, 8965, 8966, 8970, 8973, 8976, 8979, 8981, 8982, 8984, 8986, 8990, 8992, 8997, 9000, 9002, 9004, 9005, 9009, 9012, 9016, 9020, 9023, 9027, 9038, 9041, 9043, 9045, 9049, 9051, 9055, 9056, 9057, 9060, 9062, 9064, 9068, 9069, 9072, 9076, 9077, 9080, 9081, 9086, 9088, 9091, 9094, 9096, 9097, 9103, 9105, 9107, 9108, 9115, 9117, 9118, 9119, 9124, 9125, 9129, 9131, 9138, 9140, 9141, 9143, 9147, 9151, 9152, 9154, 9157, 9160, 9162, 9165, 9168, 9170, 9172, 9173, 9174, 9175, 9176, 9180, 9182, 9185, 9186, 9189, 9192, 9195, 9197, 9202, 9203, 9208, 9210, 9214, 9220, 9223, 9224, 9229, 9230, 9232, 9236, 9239, 9241, 9249, 9252, 9255, 9257, 9258, 9259, 9264, 9267, 9269, 9272, 9273, 9275, 9279, 9282, 9284, 9286, 9290, 9291, 9294, 9295, 9298, 9301, 9303, 9305, 9307, 9310, 9317, 9320, 9323, 9325, 9329, 9331, 9335, 9341, 9343, 9345, 9347, 9349, 9351, 9352, 9354, 9356, 9358, 9362, 9366, 9367, 9372, 9374, 9376, 9377, 9380, 9383, 9385, 9389, 9393, 9398, 9402, 9405, 9406, 9407, 9410, 9412, 9417, 9418, 9423, 9427, 9432, 9436, 9438, 9440, 9442, 9447, 9450, 9453, 9456, 9457, 9459, 9462, 9465, 9466, 9468, 9472, 9474, 9478, 9480, 9485, 9486, 9489, 9492, 9495, 9497, 9500, 9502, 9504, 9507, 9510, 9515, 9516, 9521, 9523, 9525, 9530, 9531, 9533, 9534, 9537, 9540, 9541, 9545, 9546, 9547, 9551, 9553, 9555, 9556, 9559, 9563, 9564, 9569, 9572, 9574, 9579, 9584, 9587, 9588, 9591, 9594, 9598, 9600, 9602, 9603, 9610, 9615, 9623, 9625, 9627, 9630, 9641, 9642, 9645, 9650, 9651, 9656, 9658, 9662, 9663, 9669, 9673, 9674, 9675, 9677, 9678, 9680, 9682, 9686, 9687, 9688, 9692, 9694, 9696, 9698, 9699, 9700, 9703, 9704, 9709, 9712, 9716, 9719, 9722, 9724, 9726, 9730, 9734, 9735, 9738, 9741, 9744, 9749, 9752, 9753, 9757, 9759, 9764, 9765, 9769, 9771, 9774, 9777, 9778, 9780, 9782, 9784, 9786, 9789, 9793, 9796, 9800, 9802, 9805, 9810, 9811, 9814, 9816, 9821, 9824, 9825, 9829, 9830, 9841],

    "multirc_split0": [497, 498, 517, 518, 519, 520, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735],
    "multirc_split1": [66, 81, 90, 102, 108, 109, 110, 111, 112, 128, 131, 132, 136, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 1079, 1080, 1082, 1086, 1088, 1089, 1092, 1093, 1094, 1095, 1096, 1097, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1144, 1145, 1146, 1147, 1148, 1149, 1150],
    "multirc_split2": [626, 629, 630, 645, 646, 647, 648, 678, 679, 681, 1048, 1049, 1050, 1051, 1052, 1055, 1056, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076, 1077, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1105, 1106, 1109, 1111, 1113, 1114, 1117, 1118, 1119, 1120, 1121, 1124, 1127, 1128, 1129, 1131, 1134, 1135, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1147, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1161, 1163],

    # IF RE-RUN MULTI-RC TASK --> USE THE LATEST ONES BELOW
    #"multirc_split0": [497, 498, 500, 506, 513, 517, 518, 519, 520, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735],
    #"multirc_split2": [626, 629, 630, 645, 646, 647, 648, 650, 677, 678, 679, 681, 685, 1048, 1049, 1050, 1051, 1052, 1055, 1056, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076, 1077, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1097, 1103, 1105, 1106, 1109, 1111, 1113, 1114, 1116, 1117, 1118, 1119, 1120, 1121, 1124, 1126, 1127, 1128, 1129, 1130, 1131, 1133, 1134, 1135, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1147, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1161, 1163],
}

kept_indices = {
    # Handle 3278 contradiction-selected examples
    # "esnli": [2, 5, 6, 10, 14, 17, 19, 20, 23, 26, 27, 28, 29, 31, 34, 37, 39, 42, 45, 49, 52, 56, 58, 60, 63, 66, 68, 71, 74, 78, 82, 83, 88, 90, 93, 96, 99, 102, 106, 108, 112, 113, 118, 119, 120, 124, 126, 129, 132, 135, 138, 145, 147, 149, 152, 158, 163, 166, 169, 171, 173, 178, 179, 182, 185, 188, 193, 194, 197, 199, 201, 202, 205, 208, 211, 214, 221, 224, 225, 230, 231, 233, 236, 237, 241, 243, 248, 250, 252, 255, 260, 263, 265, 268, 270, 275, 278, 280, 283, 285, 289, 291, 295, 298, 299, 301, 304, 305, 309, 312, 315, 318, 322, 325, 328, 331, 335, 339, 342, 344, 348, 351, 352, 359, 363, 366, 367, 368, 372, 376, 379, 380, 385, 386, 389, 393, 395, 398, 402, 405, 407, 410, 413, 417, 420, 423, 425, 428, 430, 434, 436, 439, 441, 445, 446, 449, 453, 457, 458, 463, 464, 468, 471, 473, 478, 479, 482, 487, 489, 491, 496, 497, 500, 503, 506, 509, 517, 520, 523, 527, 530, 533, 537, 538, 543, 545, 548, 552, 553, 558, 561, 564, 567, 568, 572, 575, 578, 581, 584, 586, 589, 593, 596, 600, 601, 606, 607, 611, 614, 619, 622, 626, 630, 634, 637, 639, 643, 645, 649, 650, 655, 657, 661, 663, 666, 669, 673, 678, 682, 684, 687, 690, 691, 693, 696, 698, 703, 706, 708, 712, 715, 716, 719, 720, 724, 727, 728, 732, 734, 738, 740, 745, 751, 753, 757, 760, 762, 764, 769, 770, 774, 778, 780, 782, 785, 788, 791, 794, 797, 801, 805, 808, 810, 814, 815, 818, 822, 824, 829, 831, 834, 838, 839, 842, 845, 849, 851, 855, 861, 864, 867, 869, 873, 875, 878, 882, 884, 888, 889, 894, 895, 898, 906, 907, 910, 914, 918, 921, 922, 925, 928, 931, 935, 938, 942, 945, 948, 949, 951, 956, 958, 959, 960, 964, 968, 971, 973, 976, 979, 981, 984, 986, 988, 990, 995, 996, 1002, 1007, 1008, 1013, 1014, 1018, 1021, 1024, 1026, 1029, 1034, 1036, 1038, 1042, 1050, 1055, 1058, 1063, 1066, 1070, 1073, 1076, 1079, 1082, 1084, 1088, 1090, 1095, 1100, 1101, 1106, 1109, 1113, 1117, 1120, 1122, 1125, 1128, 1129, 1130, 1133, 1137, 1139, 1143, 1145, 1149, 1151, 1156, 1157, 1160, 1165, 1168, 1171, 1173, 1176, 1178, 1182, 1186, 1188, 1191, 1194, 1198, 1201, 1204, 1205, 1208, 1211, 1214, 1220, 1223, 1229, 1232, 1234, 1239, 1241, 1245, 1247, 1253, 1255, 1260, 1262, 1266, 1268, 1269, 1270, 1275, 1277, 1281, 1283, 1285, 1294, 1296, 1300, 1304, 1307, 1308, 1311, 1315, 1316, 1321, 1322, 1323, 1327, 1329, 1332, 1334, 1336, 1339, 1344, 1348, 1353, 1356, 1357, 1364, 1365, 1367, 1370, 1374, 1377, 1380, 1382, 1385, 1388, 1393, 1396, 1399, 1400, 1403, 1407, 1409, 1413, 1415, 1420, 1421, 1426, 1428, 1430, 1434, 1438, 1441, 1443, 1447, 1449, 1452, 1456, 1457, 1460, 1464, 1467, 1471, 1473, 1475, 1479, 1482, 1486, 1489, 1490, 1494, 1497, 1501, 1503, 1507, 1509, 1511, 1516, 1519, 1520, 1523, 1527, 1530, 1534, 1537, 1541, 1544, 1547, 1551, 1552, 1556, 1559, 1562, 1566, 1569, 1571, 1573, 1578, 1581, 1582, 1585, 1590, 1591, 1596, 1598, 1602, 1603, 1606, 1610, 1613, 1615, 1620, 1621, 1626, 1628, 1630, 1634, 1636, 1641, 1644, 1646, 1649, 1653, 1655, 1659, 1661, 1664, 1667, 1669, 1673, 1676, 1679, 1682, 1685, 1689, 1692, 1693, 1697, 1701, 1702, 1705, 1709, 1712, 1716, 1719, 1720, 1723, 1726, 1729, 1730, 1734, 1736, 1740, 1742, 1747, 1749, 1752, 1759, 1761, 1763, 1768, 1770, 1774, 1779, 1781, 1786, 1790, 1792, 1793, 1796, 1799, 1802, 1805, 1808, 1811, 1814, 1818, 1820, 1824, 1826, 1830, 1831, 1834, 1839, 1840, 1845, 1846, 1849, 1851, 1853, 1855, 1858, 1861, 1866, 1869, 1871, 1875, 1876, 1883, 1887, 1892, 1895, 1898, 1901, 1904, 1907, 1910, 1912, 1917, 1918, 1923, 1926, 1928, 1930, 1933, 1941, 1943, 1946, 1950, 1951, 1956, 1959, 1960, 1961, 1962, 1964, 1968, 1970, 1974, 1975, 1979, 1982, 1984, 1989, 1992, 1994, 1996, 2000, 2002, 2005, 2009, 2013, 2015, 2019, 2025, 2027, 2031, 2032, 2036, 2040, 2041, 2044, 2048, 2051, 2052, 2055, 2060, 2061, 2066, 2068, 2070, 2075, 2077, 2079, 2082, 2085, 2092, 2095, 2098, 2100, 2105, 2106, 2111, 2115, 2119, 2123, 2124, 2129, 2132, 2135, 2138, 2140, 2142, 2146, 2149, 2153, 2155, 2157, 2160, 2163, 2168, 2170, 2172, 2173, 2175, 2178, 2181, 2186, 2187, 2190, 2193, 2198, 2200, 2203, 2208, 2212, 2215, 2217, 2223, 2227, 2230, 2233, 2235, 2238, 2243, 2245, 2247, 2250, 2254, 2256, 2259, 2262, 2264, 2268, 2270, 2273, 2277, 2281, 2284, 2287, 2290, 2292, 2294, 2297, 2304, 2306, 2310, 2314, 2317, 2318, 2323, 2325, 2329, 2332, 2333, 2336, 2340, 2342, 2347, 2350, 2352, 2356, 2358, 2361, 2364, 2366, 2371, 2372, 2375, 2378, 2382, 2384, 2386, 2389, 2393, 2397, 2398, 2402, 2406, 2409, 2412, 2413, 2417, 2420, 2426, 2428, 2432, 2435, 2438, 2441, 2443, 2446, 2449, 2452, 2455, 2460, 2466, 2469, 2472, 2473, 2477, 2479, 2482, 2487, 2488, 2491, 2495, 2498, 2502, 2503, 2508, 2509, 2513, 2516, 2518, 2521, 2525, 2528, 2531, 2533, 2536, 2539, 2547, 2549, 2554, 2557, 2561, 2562, 2564, 2567, 2570, 2573, 2575, 2578, 2583, 2584, 2587, 2592, 2595, 2596, 2599, 2604, 2607, 2610, 2613, 2616, 2618, 2622, 2626, 2627, 2630, 2631, 2635, 2638, 2640, 2644, 2646, 2649, 2653, 2657, 2660, 2663, 2666, 2668, 2672, 2675, 2677, 2679, 2681, 2687, 2691, 2693, 2695, 2698, 2700, 2703, 2706, 2709, 2714, 2717, 2721, 2724, 2727, 2728, 2732, 2735, 2739, 2741, 2745, 2746, 2750, 2752, 2754, 2756, 2760, 2762, 2765, 2769, 2770, 2773, 2780, 2782, 2785, 2789, 2792, 2796, 2797, 2799, 2804, 2812, 2813, 2817, 2819, 2824, 2825, 2829, 2832, 2835, 2839, 2841, 2845, 2848, 2849, 2854, 2857, 2858, 2859, 2861, 2862, 2866, 2869, 2870, 2873, 2877, 2880, 2883, 2886, 2888, 2890, 2895, 2898, 2899, 2902, 2905, 2909, 2912, 2916, 2918, 2919, 2920, 2923, 2925, 2930, 2932, 2934, 2939, 2941, 2945, 2947, 2950, 2952, 2955, 2962, 2965, 2966, 2969, 2974, 2976, 2979, 2982, 2985, 2988, 2990, 2997, 2999, 3003, 3004, 3008, 3010, 3014, 3016, 3021, 3024, 3025, 3028, 3032, 3035, 3038, 3042, 3044, 3046, 3049, 3055, 3059, 3063, 3065, 3067, 3070, 3075, 3077, 3079, 3082, 3085, 3088, 3093, 3094, 3099, 3101, 3104, 3107, 3108, 3113, 3116, 3117, 3121, 3126, 3130, 3132, 3135, 3138, 3142, 3146, 3149, 3150, 3154, 3156, 3161, 3162, 3167, 3169, 3172, 3176, 3178, 3182, 3185, 3187, 3190, 3194, 3195, 3200, 3202, 3205, 3208, 3211, 3217, 3218, 3220, 3224, 3225, 3228, 3233, 3236, 3239, 3241, 3245, 3246, 3249, 3253, 3255, 3260, 3263, 3265, 3267, 3271, 3276, 3281, 3284, 3285, 3289, 3290, 3294, 3296, 3297, 3298, 3301, 3304, 3306, 3310, 3311, 3315, 3318, 3320, 3324, 3328, 3330, 3334, 3335, 3340, 3341, 3346, 3347, 3350, 3354, 3357, 3361, 3363, 3366, 3369, 3371, 3374, 3379, 3381, 3384, 3387, 3389, 3392, 3396, 3399, 3400, 3403, 3407, 3409, 3412, 3416, 3419, 3422, 3424, 3429, 3431, 3435, 3438, 3441, 3444, 3447, 3450, 3453, 3456, 3457, 3462, 3463, 3467, 3469, 3473, 3476, 3479, 3481, 3482, 3485, 3486, 3491, 3492, 3495, 3500, 3505, 3509, 3511, 3517, 3519, 3523, 3524, 3530, 3532, 3537, 3538, 3542, 3548, 3550, 3554, 3556, 3561, 3562, 3565, 3569, 3573, 3574, 3578, 3580, 3584, 3587, 3591, 3592, 3596, 3598, 3603, 3609, 3611, 3614, 3615, 3619, 3620, 3621, 3625, 3628, 3632, 3634, 3638, 3641, 3643, 3646, 3650, 3652, 3654, 3657, 3661, 3665, 3668, 3669, 3672, 3675, 3679, 3682, 3684, 3687, 3691, 3692, 3696, 3697, 3698, 3703, 3705, 3709, 3710, 3714, 3718, 3720, 3723, 3726, 3729, 3732, 3735, 3738, 3740, 3745, 3746, 3750, 3752, 3760, 3761, 3766, 3767, 3770, 3775, 3776, 3780, 3784, 3785, 3789, 3792, 3795, 3796, 3799, 3803, 3807, 3808, 3812, 3816, 3819, 3822, 3825, 3826, 3829, 3832, 3834, 3838, 3842, 3844, 3846, 3848, 3852, 3856, 3857, 3860, 3861, 3863, 3867, 3870, 3872, 3876, 3880, 3881, 3884, 3888, 3891, 3894, 3897, 3899, 3902, 3907, 3909, 3913, 3914, 3919, 3922, 3924, 3928, 3929, 3934, 3936, 3941, 3943, 3944, 3948, 3955, 3957, 3958, 3959, 3964, 3965, 3968, 3973, 3974, 3977, 3982, 3983, 3986, 3989, 3992, 3997, 3998, 4002, 4004, 4008, 4011, 4012, 4016, 4020, 4023, 4029, 4031, 4034, 4038, 4041, 4043, 4046, 4049, 4052, 4055, 4058, 4060, 4063, 4066, 4068, 4072, 4074, 4079, 4081, 4085, 4088, 4090, 4092, 4095, 4099, 4101, 4104, 4109, 4111, 4113, 4116, 4119, 4123, 4126, 4128, 4134, 4136, 4141, 4142, 4145, 4150, 4153, 4155, 4158, 4160, 4162, 4166, 4169, 4171, 4174, 4176, 4180, 4184, 4187, 4190, 4193, 4195, 4202, 4205, 4207, 4209, 4214, 4216, 4220, 4223, 4224, 4229, 4231, 4233, 4238, 4240, 4243, 4246, 4248, 4251, 4253, 4256, 4259, 4263, 4268, 4273, 4274, 4278, 4280, 4285, 4286, 4289, 4293, 4297, 4299, 4303, 4306, 4308, 4311, 4313, 4318, 4321, 4324, 4326, 4330, 4333, 4335, 4338, 4341, 4345, 4348, 4349, 4353, 4354, 4357, 4361, 4365, 4366, 4368, 4370, 4371, 4374, 4376, 4380, 4381, 4385, 4386, 4387, 4389, 4392, 4395, 4399, 4400, 4404, 4408, 4410, 4412, 4415, 4420, 4421, 4424, 4425, 4432, 4433, 4438, 4439, 4442, 4445, 4448, 4451, 4456, 4459, 4461, 4463, 4466, 4469, 4471, 4474, 4477, 4478, 4481, 4484, 4488, 4491, 4496, 4499, 4500, 4501, 4503, 4508, 4510, 4514, 4515, 4516, 4517, 4519, 4525, 4526, 4530, 4532, 4535, 4539, 4542, 4545, 4546, 4549, 4554, 4555, 4558, 4562, 4566, 4569, 4571, 4575, 4576, 4581, 4582, 4587, 4588, 4592, 4596, 4599, 4600, 4604, 4606, 4607, 4608, 4610, 4614, 4615, 4620, 4621, 4624, 4627, 4632, 4633, 4636, 4640, 4643, 4647, 4650, 4654, 4656, 4660, 4662, 4665, 4666, 4668, 4670, 4675, 4677, 4680, 4682, 4687, 4690, 4691, 4696, 4697, 4701, 4703, 4708, 4710, 4712, 4716, 4720, 4721, 4723, 4726, 4727, 4730, 4734, 4737, 4738, 4743, 4744, 4746, 4747, 4752, 4753, 4758, 4761, 4762, 4766, 4769, 4771, 4774, 4778, 4781, 4784, 4786, 4790, 4792, 4795, 4799, 4801, 4805, 4807, 4811, 4814, 4816, 4819, 4824, 4826, 4829, 4832, 4839, 4840, 4843, 4847, 4849, 4852, 4856, 4860, 4862, 4865, 4869, 4871, 4873, 4876, 4879, 4882, 4886, 4888, 4893, 4894, 4898, 4901, 4903, 4908, 4909, 4912, 4917, 4919, 4922, 4926, 4929, 4930, 4933, 4937, 4940, 4942, 4947, 4949, 4953, 4954, 4958, 4960, 4965, 4968, 4971, 4972, 4976, 4979, 4981, 4983, 4986, 4987, 4993, 4994, 4998, 5001, 5002, 5007, 5008, 5013, 5014, 5018, 5022, 5023, 5025, 5030, 5031, 5036, 5043, 5048, 5050, 5053, 5055, 5058, 5061, 5066, 5067, 5071, 5073, 5078, 5080, 5083, 5087, 5088, 5092, 5095, 5098, 5101, 5105, 5108, 5109, 5114, 5116, 5118, 5123, 5124, 5127, 5131, 5134, 5138, 5139, 5142, 5145, 5150, 5151, 5155, 5159, 5163, 5165, 5168, 5173, 5174, 5179, 5182, 5185, 5188, 5190, 5193, 5197, 5200, 5201, 5206, 5209, 5212, 5213, 5216, 5219, 5224, 5227, 5229, 5232, 5236, 5238, 5240, 5241, 5242, 5243, 5246, 5249, 5253, 5255, 5260, 5263, 5264, 5267, 5269, 5273, 5276, 5277, 5280, 5284, 5286, 5289, 5290, 5293, 5295, 5298, 5301, 5308, 5312, 5315, 5316, 5318, 5320, 5323, 5325, 5329, 5330, 5332, 5336, 5339, 5341, 5345, 5346, 5349, 5350, 5354, 5356, 5358, 5362, 5365, 5369, 5372, 5375, 5376, 5378, 5381, 5383, 5385, 5388, 5392, 5397, 5400, 5401, 5406, 5409, 5412, 5414, 5418, 5419, 5422, 5426, 5428, 5432, 5436, 5438, 5441, 5443, 5445, 5446, 5449, 5453, 5455, 5460, 5463, 5465, 5468, 5470, 5474, 5476, 5480, 5482, 5486, 5489, 5492, 5495, 5497, 5501, 5504, 5507, 5510, 5511, 5514, 5519, 5525, 5527, 5530, 5533, 5537, 5540, 5543, 5544, 5548, 5550, 5554, 5556, 5558, 5562, 5567, 5570, 5573, 5576, 5579, 5583, 5586, 5587, 5592, 5594, 5596, 5598, 5600, 5604, 5607, 5610, 5611, 5615, 5619, 5621, 5624, 5631, 5634, 5636, 5639, 5641, 5645, 5649, 5654, 5656, 5657, 5660, 5663, 5667, 5669, 5673, 5675, 5679, 5681, 5683, 5686, 5696, 5699, 5701, 5706, 5707, 5712, 5713, 5718, 5720, 5723, 5727, 5730, 5733, 5735, 5738, 5741, 5745, 5746, 5749, 5753, 5756, 5758, 5762, 5764, 5769, 5772, 5774, 5777, 5780, 5783, 5787, 5789, 5791, 5799, 5801, 5805, 5807, 5811, 5813, 5815, 5818, 5821, 5825, 5827, 5831, 5835, 5836, 5840, 5843, 5845, 5849, 5852, 5854, 5859, 5860, 5861, 5864, 5866, 5869, 5871, 5875, 5876, 5877, 5880, 5882, 5884, 5887, 5892, 5895, 5899, 5902, 5905, 5908, 5912, 5914, 5919, 5923, 5927, 5930, 5933, 5935, 5938, 5942, 5945, 5947, 5949, 5954, 5956, 5960, 5962, 5964, 5969, 5972, 5973, 5978, 5980, 5983, 5985, 5988, 5992, 5995, 5998, 6002, 6005, 6008, 6010, 6012, 6017, 6020, 6023, 6025, 6027, 6031, 6033, 6038, 6039, 6042, 6043, 6050, 6051, 6056, 6059, 6061, 6063, 6067, 6071, 6074, 6077, 6079, 6081, 6086, 6087, 6090, 6093, 6098, 6099, 6101, 6102, 6106, 6108, 6113, 6116, 6119, 6121, 6123, 6127, 6131, 6132, 6137, 6138, 6142, 6145, 6147, 6152, 6155, 6156, 6162, 6166, 6168, 6173, 6175, 6180, 6181, 6184, 6187, 6192, 6195, 6199, 6202, 6204, 6207, 6211, 6214, 6216, 6219, 6223, 6224, 6226, 6228, 6230, 6232, 6233, 6237, 6241, 6242, 6246, 6249, 6251, 6256, 6258, 6261, 6264, 6267, 6270, 6275, 6278, 6280, 6285, 6286, 6294, 6297, 6298, 6302, 6305, 6306, 6310, 6312, 6317, 6318, 6322, 6324, 6327, 6331, 6335, 6337, 6341, 6343, 6346, 6348, 6350, 6353, 6355, 6360, 6364, 6366, 6370, 6371, 6375, 6378, 6380, 6383, 6388, 6390, 6392, 6398, 6399, 6402, 6406, 6408, 6412, 6413, 6417, 6418, 6423, 6425, 6429, 6431, 6433, 6436, 6439, 6443, 6447, 6449, 6451, 6452, 6456, 6458, 6460, 6463, 6468, 6471, 6473, 6476, 6478, 6483, 6484, 6489, 6491, 6494, 6496, 6499, 6502, 6507, 6508, 6510, 6513, 6516, 6521, 6524, 6527, 6528, 6531, 6534, 6538, 6540, 6543, 6548, 6551, 6553, 6557, 6558, 6559, 6562, 6564, 6568, 6569, 6574, 6575, 6578, 6583, 6584, 6588, 6592, 6595, 6596, 6599, 6602, 6606, 6608, 6613, 6615, 6617, 6622, 6623, 6628, 6630, 6634, 6637, 6639, 6642, 6644, 6647, 6651, 6654, 6656, 6660, 6663, 6666, 6667, 6670, 6673, 6674, 6678, 6682, 6685, 6688, 6690, 6694, 6696, 6700, 6701, 6706, 6707, 6711, 6714, 6716, 6721, 6724, 6726, 6730, 6733, 6740, 6742, 6746, 6747, 6750, 6754, 6758, 6761, 6763, 6766, 6768, 6772, 6775, 6778, 6780, 6783, 6788, 6790, 6792, 6796, 6797, 6800, 6801, 6806, 6807, 6810, 6815, 6817, 6820, 6822, 6826, 6830, 6832, 6835, 6839, 6842, 6845, 6847, 6849, 6854, 6856, 6859, 6863, 6866, 6868, 6872, 6876, 6879, 6882, 6885, 6890, 6891, 6898, 6904, 6907, 6909, 6914, 6917, 6919, 6921, 6926, 6929, 6933, 6938, 6941, 6942, 6945, 6949, 6953, 6954, 6957, 6965, 6968, 6971, 6974, 6977, 6978, 6981, 6986, 6987, 6991, 6994, 6996, 7000, 7004, 7005, 7008, 7013, 7014, 7019, 7020, 7025, 7026, 7029, 7033, 7036, 7038, 7042, 7045, 7048, 7050, 7054, 7055, 7058, 7063, 7066, 7067, 7072, 7074, 7078, 7081, 7084, 7085, 7090, 7091, 7096, 7098, 7100, 7104, 7106, 7109, 7112, 7116, 7117, 7119, 7120, 7124, 7126, 7131, 7132, 7134, 7137, 7140, 7144, 7148, 7149, 7154, 7156, 7160, 7161, 7165, 7166, 7169, 7171, 7175, 7178, 7180, 7184, 7186, 7190, 7191, 7195, 7198, 7202, 7203, 7207, 7211, 7213, 7216, 7219, 7221, 7225, 7228, 7230, 7233, 7236, 7240, 7243, 7245, 7246, 7247, 7248, 7251, 7254, 7257, 7262, 7263, 7267, 7270, 7273, 7276, 7278, 7282, 7285, 7287, 7290, 7292, 7295, 7296, 7301, 7302, 7307, 7309, 7313, 7315, 7319, 7322, 7324, 7329, 7331, 7335, 7338, 7339, 7343, 7346, 7350, 7353, 7356, 7358, 7360, 7362, 7365, 7369, 7377, 7385, 7388, 7392, 7393, 7398, 7401, 7402, 7407, 7408, 7412, 7414, 7417, 7420, 7423, 7425, 7426, 7429, 7431, 7433, 7435, 7440, 7443, 7444, 7448, 7451, 7453, 7455, 7459, 7461, 7466, 7469, 7472, 7473, 7477, 7478, 7479, 7483, 7486, 7492, 7495, 7497, 7500, 7504, 7506, 7509, 7513, 7516, 7519, 7523, 7526, 7527, 7530, 7533, 7537, 7539, 7542, 7543, 7546, 7549, 7555, 7559, 7561, 7567, 7573, 7574, 7578, 7580, 7584, 7586, 7590, 7594, 7596, 7598, 7603, 7605, 7607, 7610, 7615, 7616, 7617, 7623, 7626, 7627, 7629, 7632, 7637, 7642, 7645, 7648, 7651, 7654, 7657, 7659, 7661, 7666, 7668, 7669, 7671, 7673, 7676, 7682, 7686, 7689, 7692, 7693, 7697, 7700, 7704, 7707, 7710, 7713, 7714, 7717, 7720, 7722, 7727, 7729, 7732, 7736, 7739, 7742, 7743, 7745, 7749, 7752, 7755, 7759, 7760, 7763, 7766, 7769, 7773, 7777, 7779, 7781, 7785, 7788, 7790, 7793, 7796, 7800, 7803, 7805, 7811, 7816, 7817, 7822, 7825, 7827, 7831, 7833, 7837, 7838, 7840, 7842, 7844, 7848, 7852, 7856, 7857, 7862, 7864, 7867, 7869, 7872, 7875, 7880, 7881, 7886, 7889, 7891, 7895, 7897, 7899, 7901, 7904, 7909, 7910, 7913, 7918, 7920, 7924, 7926, 7928, 7931, 7934, 7939, 7942, 7943, 7946, 7950, 7953, 7956, 7958, 7962, 7965, 7968, 7971, 7972, 7977, 7979, 7985, 7988, 7992, 7993, 7997, 8000, 8003, 8006, 8008, 8011, 8016, 8018, 8021, 8023, 8028, 8031, 8034, 8037, 8038, 8041, 8044, 8049, 8051, 8054, 8057, 8059, 8064, 8066, 8068, 8070, 8075, 8076, 8081, 8082, 8086, 8089, 8092, 8094, 8098, 8103, 8108, 8111, 8112, 8115, 8120, 8122, 8126, 8127, 8131, 8134, 8135, 8136, 8141, 8143, 8146, 8148, 8152, 8155, 8157, 8161, 8164, 8168, 8170, 8173, 8176, 8178, 8183, 8184, 8187, 8190, 8194, 8197, 8201, 8204, 8206, 8208, 8213, 8216, 8218, 8220, 8225, 8228, 8229, 8232, 8235, 8239, 8246, 8250, 8253, 8254, 8257, 8261, 8263, 8265, 8270, 8272, 8276, 8278, 8282, 8284, 8288, 8290, 8296, 8300, 8303, 8306, 8309, 8311, 8314, 8317, 8321, 8323, 8325, 8328, 8332, 8333, 8336, 8337, 8342, 8343, 8346, 8350, 8352, 8355, 8358, 8361, 8365, 8369, 8372, 8374, 8377, 8379, 8382, 8386, 8389, 8392, 8396, 8399, 8402, 8404, 8408, 8410, 8412, 8417, 8420, 8422, 8426, 8427, 8432, 8433, 8437, 8441, 8442, 8445, 8448, 8453, 8454, 8459, 8460, 8464, 8466, 8470, 8471, 8473, 8477, 8483, 8485, 8488, 8490, 8495, 8497, 8499, 8502, 8506, 8510, 8513, 8515, 8517, 8521, 8524, 8528, 8530, 8531, 8534, 8535, 8538, 8541, 8546, 8548, 8551, 8554, 8556, 8558, 8560, 8562, 8564, 8569, 8572, 8575, 8576, 8581, 8584, 8587, 8590, 8592, 8595, 8597, 8601, 8602, 8605, 8610, 8612, 8615, 8618, 8620, 8624, 8626, 8631, 8634, 8635, 8639, 8641, 8645, 8648, 8650, 8651, 8652, 8656, 8659, 8660, 8664, 8668, 8673, 8676, 8678, 8681, 8686, 8688, 8690, 8693, 8697, 8699, 8703, 8707, 8710, 8712, 8716, 8717, 8722, 8725, 8726, 8729, 8732, 8736, 8739, 8741, 8746, 8747, 8751, 8755, 8757, 8758, 8763, 8765, 8767, 8770, 8773, 8778, 8779, 8784, 8786, 8788, 8791, 8796, 8797, 8801, 8805, 8806, 8811, 8813, 8815, 8820, 8821, 8826, 8827, 8831, 8835, 8836, 8844, 8845, 8846, 8848, 8852, 8856, 8858, 8860, 8863, 8867, 8868, 8873, 8876, 8878, 8880, 8885, 8887, 8892, 8896, 8899, 8902, 8904, 8909, 8910, 8915, 8918, 8920, 8922, 8926, 8930, 8931, 8933, 8937, 8941, 8942, 8946, 8949, 8952, 8955, 8958, 8960, 8964, 8967, 8969, 8974, 8975, 8978, 8983, 8985, 8988, 8991, 8994, 8995, 8998, 9003, 9008, 9010, 9011, 9013, 9014, 9017, 9019, 9022, 9026, 9030, 9033, 9035, 9037, 9040, 9047, 9048, 9053, 9054, 9058, 9061, 9063, 9066, 9070, 9074, 9078, 9082, 9084, 9089, 9090, 9095, 9098, 9100, 9104, 9106, 9109, 9111, 9114, 9120, 9122, 9126, 9128, 9133, 9134, 9137, 9142, 9144, 9148, 9149, 9156, 9159, 9163, 9166, 9167, 9171, 9178, 9181, 9184, 9187, 9190, 9193, 9196, 9199, 9200, 9205, 9206, 9209, 9213, 9215, 9216, 9218, 9222, 9226, 9227, 9231, 9234, 9235, 9238, 9242, 9244, 9247, 9250, 9254, 9256, 9260, 9263, 9265, 9268, 9271, 9276, 9278, 9281, 9285, 9288, 9292, 9296, 9300, 9306, 9309, 9312, 9315, 9318, 9321, 9324, 9326, 9330, 9333, 9336, 9338, 9342, 9344, 9346, 9350, 9355, 9357, 9363, 9364, 9369, 9371, 9375, 9378, 9379, 9384, 9386, 9388, 9391, 9396, 9399, 9401, 9404, 9409, 9414, 9415, 9419, 9421, 9425, 9429, 9430, 9435, 9437, 9439, 9443, 9446, 9448, 9452, 9454, 9458, 9460, 9464, 9467, 9470, 9471, 9475, 9477, 9482, 9483, 9487, 9490, 9494, 9496, 9499, 9503, 9505, 9508, 9512, 9514, 9517, 9520, 9522, 9527, 9528, 9532, 9535, 9539, 9544, 9548, 9550, 9552, 9557, 9560, 9561, 9566, 9568, 9571, 9575, 9577, 9578, 9580, 9582, 9586, 9589, 9593, 9596, 9597, 9601, 9606, 9611, 9614, 9617, 9619, 9622, 9624, 9629, 9631, 9633, 9637, 9639, 9644, 9647, 9648, 9652, 9655, 9657, 9661, 9664, 9666, 9670, 9672, 9676, 9681, 9685, 9691, 9695, 9702, 9705, 9707, 9710, 9711, 9714, 9717, 9720, 9723, 9727, 9729, 9733, 9736, 9739, 9742, 9745, 9747, 9751, 9754, 9756, 9758, 9760, 9763, 9767, 9768, 9772, 9775, 9779, 9781, 9783, 9787, 9791, 9794, 9795, 9799, 9803, 9804, 9809, 9813, 9818, 9820, 9823, 9826, 9828, 9832, 9834, 9838, 9840]

    # Handle 3117 entailment-selected examples
    # "esnli": [3, 8, 9, 13, 15, 18, 21, 25, 30, 33, 36, 43, 46, 50, 53, 55, 57, 62, 65, 69, 72, 75, 79, 81, 84, 86, 87, 91, 94, 95, 97, 100, 104, 109, 110, 114, 116, 117, 127, 128, 137, 144, 146, 148, 150, 154, 157, 159, 162, 165, 172, 176, 181, 183, 187, 190, 191, 195, 198, 203, 206, 210, 216, 219, 222, 227, 228, 232, 238, 242, 244, 246, 247, 251, 257, 259, 261, 262, 266, 267, 269, 271, 274, 276, 282, 284, 287, 290, 293, 296, 300, 303, 307, 310, 313, 314, 320, 323, 327, 334, 338, 343, 346, 349, 350, 353, 354, 355, 361, 364, 370, 371, 375, 378, 382, 383, 388, 391, 392, 397, 400, 403, 404, 409, 411, 414, 416, 424, 426, 427, 429, 432, 435, 440, 443, 444, 447, 450, 452, 455, 456, 460, 462, 465, 469, 470, 472, 476, 480, 481, 484, 486, 488, 494, 498, 499, 501, 505, 507, 511, 514, 515, 521, 525, 526, 529, 532, 535, 540, 542, 546, 547, 555, 556, 559, 563, 565, 570, 571, 573, 574, 577, 582, 588, 591, 594, 597, 599, 602, 605, 609, 610, 613, 617, 620, 629, 632, 638, 641, 644, 648, 652, 653, 656, 660, 662, 665, 668, 672, 674, 676, 677, 680, 683, 686, 689, 692, 695, 701, 710, 713, 717, 718, 721, 725, 729, 731, 735, 742, 743, 748, 750, 752, 754, 755, 759, 763, 767, 768, 772, 773, 775, 777, 783, 787, 789, 790, 792, 796, 798, 802, 803, 806, 811, 813, 817, 823, 825, 828, 832, 835, 836, 841, 844, 848, 852, 856, 858, 862, 863, 865, 868, 871, 874, 879, 880, 885, 886, 893, 897, 900, 901, 904, 909, 913, 917, 919, 923, 926, 930, 933, 939, 941, 943, 947, 953, 954, 957, 962, 963, 969, 972, 975, 982, 987, 989, 992, 994, 997, 1003, 1006, 1010, 1011, 1015, 1019, 1020, 1023, 1027, 1031, 1033, 1035, 1040, 1041, 1046, 1051, 1053, 1056, 1060, 1064, 1065, 1068, 1069, 1072, 1077, 1080, 1083, 1089, 1092, 1094, 1096, 1099, 1102, 1103, 1108, 1111, 1112, 1116, 1119, 1123, 1124, 1127, 1131, 1135, 1136, 1141, 1142, 1146, 1148, 1152, 1155, 1161, 1164, 1166, 1170, 1174, 1175, 1179, 1181, 1184, 1187, 1190, 1195, 1200, 1203, 1209, 1212, 1215, 1217, 1222, 1225, 1226, 1230, 1235, 1237, 1240, 1244, 1246, 1249, 1252, 1257, 1259, 1261, 1265, 1271, 1273, 1276, 1279, 1282, 1284, 1286, 1288, 1295, 1298, 1301, 1303, 1309, 1320, 1324, 1325, 1328, 1335, 1337, 1340, 1342, 1347, 1349, 1351, 1359, 1361, 1363, 1366, 1369, 1372, 1375, 1376, 1381, 1383, 1386, 1389, 1391, 1394, 1397, 1402, 1405, 1411, 1414, 1417, 1419, 1423, 1425, 1427, 1431, 1433, 1436, 1439, 1442, 1445, 1448, 1453, 1455, 1458, 1462, 1463, 1466, 1470, 1472, 1477, 1484, 1485, 1492, 1493, 1498, 1500, 1502, 1506, 1508, 1513, 1514, 1517, 1518, 1521, 1522, 1526, 1531, 1535, 1538, 1542, 1546, 1550, 1553, 1555, 1558, 1563, 1564, 1568, 1574, 1583, 1586, 1589, 1592, 1595, 1599, 1601, 1604, 1607, 1611, 1612, 1616, 1618, 1622, 1625, 1629, 1631, 1632, 1635, 1637, 1639, 1643, 1645, 1652, 1656, 1658, 1662, 1666, 1670, 1675, 1677, 1678, 1683, 1686, 1687, 1691, 1695, 1698, 1703, 1706, 1708, 1721, 1724, 1728, 1732, 1733, 1737, 1743, 1746, 1748, 1751, 1755, 1758, 1762, 1765, 1767, 1769, 1772, 1776, 1778, 1782, 1785, 1789, 1791, 1797, 1800, 1804, 1806, 1809, 1813, 1815, 1817, 1821, 1823, 1827, 1828, 1833, 1835, 1838, 1841, 1844, 1847, 1850, 1854, 1857, 1860, 1862, 1867, 1870, 1881, 1884, 1888, 1889, 1890, 1891, 1896, 1897, 1902, 1903, 1908, 1909, 1915, 1922, 1927, 1929, 1932, 1934, 1937, 1939, 1947, 1949, 1955, 1957, 1958, 1969, 1973, 1976, 1977, 1978, 1980, 1983, 1986, 1987, 1990, 1991, 1993, 1997, 1999, 2004, 2006, 2010, 2016, 2022, 2023, 2028, 2029, 2033, 2035, 2039, 2043, 2046, 2049, 2050, 2054, 2057, 2058, 2063, 2065, 2067, 2071, 2073, 2076, 2080, 2084, 2086, 2088, 2091, 2094, 2097, 2101, 2104, 2110, 2113, 2120, 2122, 2125, 2127, 2130, 2136, 2144, 2147, 2152, 2154, 2158, 2161, 2165, 2167, 2169, 2177, 2180, 2183, 2184, 2185, 2194, 2196, 2204, 2207, 2210, 2213, 2214, 2216, 2219, 2221, 2225, 2228, 2231, 2232, 2237, 2239, 2241, 2249, 2252, 2255, 2260, 2261, 2265, 2272, 2274, 2278, 2280, 2282, 2285, 2289, 2293, 2295, 2296, 2305, 2307, 2309, 2313, 2315, 2319, 2321, 2326, 2330, 2335, 2337, 2341, 2344, 2346, 2353, 2354, 2357, 2360, 2365, 2369, 2374, 2376, 2380, 2385, 2388, 2390, 2392, 2395, 2403, 2407, 2410, 2411, 2416, 2421, 2423, 2427, 2429, 2431, 2433, 2436, 2439, 2440, 2445, 2450, 2454, 2456, 2459, 2461, 2462, 2463, 2464, 2467, 2470, 2475, 2480, 2484, 2485, 2490, 2492, 2493, 2499, 2500, 2504, 2505, 2506, 2507, 2511, 2514, 2515, 2520, 2522, 2523, 2526, 2529, 2532, 2534, 2537, 2541, 2542, 2546, 2550, 2551, 2556, 2559, 2563, 2568, 2571, 2574, 2577, 2580, 2581, 2586, 2588, 2591, 2594, 2597, 2600, 2603, 2605, 2608, 2612, 2614, 2617, 2620, 2623, 2625, 2628, 2633, 2639, 2641, 2643, 2650, 2654, 2655, 2662, 2664, 2669, 2670, 2671, 2673, 2676, 2680, 2683, 2684, 2688, 2690, 2694, 2697, 2701, 2702, 2707, 2708, 2710, 2712, 2715, 2719, 2723, 2725, 2729, 2730, 2731, 2736, 2737, 2740, 2742, 2743, 2748, 2749, 2751, 2753, 2755, 2758, 2761, 2766, 2767, 2772, 2774, 2776, 2777, 2779, 2781, 2784, 2786, 2788, 2791, 2795, 2800, 2803, 2805, 2808, 2810, 2814, 2818, 2820, 2822, 2826, 2827, 2828, 2831, 2834, 2836, 2840, 2842, 2843, 2846, 2847, 2851, 2853, 2856, 2863, 2865, 2868, 2872, 2876, 2878, 2881, 2884, 2891, 2894, 2897, 2901, 2903, 2907, 2908, 2910, 2911, 2915, 2921, 2922, 2926, 2928, 2929, 2933, 2937, 2942, 2944, 2948, 2951, 2953, 2956, 2957, 2958, 2959, 2961, 2963, 2967, 2970, 2972, 2975, 2980, 2981, 2986, 2987, 2991, 2992, 2993, 2994, 2995, 2998, 3002, 3006, 3007, 3012, 3013, 3017, 3023, 3027, 3029, 3030, 3033, 3036, 3037, 3041, 3045, 3047, 3050, 3053, 3056, 3058, 3062, 3066, 3068, 3071, 3074, 3076, 3078, 3080, 3083, 3087, 3089, 3092, 3095, 3097, 3102, 3103, 3105, 3112, 3114, 3118, 3122, 3124, 3127, 3129, 3134, 3137, 3140, 3141, 3144, 3147, 3151, 3152, 3155, 3158, 3160, 3165, 3168, 3171, 3180, 3184, 3186, 3189, 3192, 3196, 3198, 3203, 3206, 3209, 3210, 3213, 3215, 3216, 3221, 3222, 3226, 3229, 3231, 3234, 3238, 3240, 3244, 3250, 3251, 3252, 3257, 3258, 3261, 3268, 3269, 3270, 3275, 3278, 3280, 3283, 3287, 3288, 3292, 3293, 3299, 3300, 3302, 3307, 3309, 3313, 3316, 3317, 3321, 3325, 3327, 3329, 3331, 3332, 3333, 3337, 3338, 3342, 3345, 3349, 3352, 3353, 3356, 3358, 3360, 3362, 3365, 3368, 3373, 3375, 3378, 3382, 3386, 3390, 3393, 3394, 3398, 3402, 3405, 3410, 3413, 3417, 3420, 3421, 3425, 3428, 3433, 3434, 3437, 3442, 3445, 3448, 3451, 3454, 3458, 3460, 3461, 3465, 3466, 3468, 3471, 3475, 3478, 3480, 3488, 3489, 3493, 3496, 3498, 3501, 3506, 3508, 3510, 3513, 3515, 3518, 3522, 3525, 3528, 3533, 3539, 3541, 3545, 3547, 3553, 3557, 3559, 3563, 3568, 3570, 3571, 3577, 3582, 3583, 3588, 3589, 3590, 3594, 3597, 3599, 3604, 3608, 3610, 3612, 3618, 3622, 3624, 3627, 3629, 3635, 3640, 3642, 3644, 3645, 3649, 3653, 3656, 3659, 3662, 3664, 3666, 3667, 3671, 3676, 3678, 3681, 3685, 3688, 3690, 3693, 3700, 3701, 3706, 3708, 3711, 3715, 3716, 3721, 3724, 3727, 3730, 3733, 3734, 3737, 3741, 3743, 3747, 3748, 3751, 3753, 3754, 3755, 3758, 3764, 3769, 3771, 3774, 3777, 3779, 3783, 3787, 3788, 3790, 3794, 3797, 3801, 3802, 3805, 3806, 3811, 3814, 3818, 3820, 3824, 3828, 3830, 3831, 3833, 3839, 3840, 3841, 3843, 3849, 3851, 3858, 3859, 3862, 3865, 3869, 3874, 3875, 3878, 3882, 3883, 3885, 3889, 3892, 3895, 3896, 3901, 3905, 3910, 3915, 3917, 3921, 3923, 3927, 3930, 3937, 3940, 3942, 3946, 3949, 3951, 3956, 3960, 3962, 3970, 3971, 3976, 3978, 3981, 3984, 3985, 3987, 3991, 3993, 3995, 3999, 4000, 4003, 4005, 4006, 4007, 4009, 4013, 4014, 4017, 4019, 4021, 4025, 4028, 4032, 4033, 4039, 4042, 4047, 4048, 4051, 4056, 4057, 4061, 4062, 4065, 4070, 4076, 4077, 4078, 4080, 4084, 4091, 4093, 4097, 4098, 4102, 4105, 4108, 4112, 4115, 4117, 4120, 4122, 4127, 4132, 4139, 4143, 4146, 4148, 4152, 4154, 4156, 4161, 4164, 4167, 4175, 4177, 4181, 4186, 4188, 4192, 4194, 4198, 4206, 4208, 4211, 4212, 4217, 4218, 4222, 4226, 4228, 4232, 4234, 4237, 4239, 4241, 4245, 4249, 4254, 4258, 4261, 4267, 4270, 4272, 4276, 4277, 4281, 4283, 4291, 4292, 4294, 4296, 4300, 4304, 4307, 4314, 4316, 4320, 4322, 4325, 4327, 4328, 4332, 4334, 4337, 4342, 4344, 4346, 4351, 4352, 4355, 4359, 4360, 4363, 4369, 4372, 4378, 4382, 4383, 4396, 4401, 4405, 4406, 4411, 4414, 4419, 4423, 4427, 4430, 4435, 4437, 4443, 4447, 4449, 4450, 4453, 4454, 4455, 4457, 4462, 4465, 4468, 4470, 4472, 4476, 4480, 4482, 4485, 4490, 4492, 4495, 4504, 4505, 4506, 4509, 4513, 4520, 4521, 4522, 4524, 4528, 4533, 4536, 4537, 4543, 4547, 4551, 4553, 4556, 4561, 4565, 4567, 4573, 4577, 4579, 4580, 4583, 4585, 4590, 4593, 4594, 4598, 4602, 4603, 4605, 4611, 4613, 4616, 4618, 4619, 4625, 4629, 4630, 4631, 4634, 4638, 4641, 4644, 4651, 4653, 4657, 4658, 4661, 4664, 4669, 4671, 4673, 4676, 4678, 4679, 4681, 4684, 4685, 4692, 4694, 4699, 4702, 4705, 4707, 4709, 4714, 4717, 4718, 4724, 4728, 4732, 4735, 4736, 4741, 4745, 4748, 4751, 4754, 4757, 4760, 4764, 4767, 4772, 4775, 4776, 4777, 4780, 4785, 4787, 4791, 4793, 4796, 4797, 4800, 4803, 4804, 4809, 4812, 4813, 4818, 4821, 4822, 4823, 4827, 4828, 4834, 4836, 4837, 4842, 4846, 4850, 4857, 4858, 4861, 4867, 4872, 4874, 4878, 4880, 4883, 4889, 4890, 4891, 4896, 4897, 4900, 4904, 4906, 4913, 4916, 4921, 4924, 4927, 4931, 4935, 4936, 4939, 4943, 4945, 4950, 4952, 4956, 4957, 4959, 4962, 4963, 4967, 4969, 4974, 4975, 4978, 4982, 4985, 4989, 4990, 4992, 4995, 4996, 4999, 5000, 5003, 5005, 5009, 5011, 5016, 5019, 5026, 5029, 5033, 5034, 5039, 5040, 5044, 5046, 5049, 5054, 5059, 5062, 5065, 5070, 5074, 5076, 5081, 5082, 5086, 5089, 5090, 5093, 5094, 5096, 5097, 5100, 5103, 5104, 5107, 5111, 5113, 5117, 5120, 5121, 5126, 5132, 5133, 5137, 5141, 5143, 5147, 5148, 5152, 5154, 5158, 5161, 5162, 5167, 5169, 5170, 5172, 5176, 5178, 5180, 5183, 5186, 5191, 5194, 5195, 5198, 5199, 5203, 5208, 5215, 5218, 5221, 5222, 5225, 5228, 5231, 5235, 5237, 5244, 5245, 5250, 5251, 5252, 5256, 5258, 5259, 5262, 5265, 5266, 5268, 5272, 5275, 5278, 5282, 5287, 5288, 5291, 5292, 5296, 5299, 5302, 5303, 5306, 5309, 5310, 5314, 5317, 5319, 5322, 5327, 5328, 5333, 5337, 5342, 5344, 5347, 5351, 5353, 5359, 5361, 5363, 5366, 5368, 5370, 5374, 5377, 5382, 5384, 5389, 5390, 5394, 5399, 5404, 5408, 5411, 5413, 5415, 5417, 5421, 5424, 5429, 5433, 5435, 5439, 5442, 5444, 5448, 5451, 5454, 5457, 5458, 5461, 5464, 5467, 5471, 5475, 5477, 5478, 5483, 5485, 5488, 5491, 5494, 5496, 5499, 5502, 5505, 5506, 5508, 5512, 5515, 5518, 5522, 5524, 5526, 5529, 5536, 5539, 5541, 5545, 5546, 5551, 5553, 5555, 5557, 5559, 5563, 5564, 5569, 5571, 5572, 5574, 5578, 5581, 5584, 5585, 5589, 5591, 5597, 5603, 5605, 5609, 5614, 5618, 5620, 5623, 5627, 5630, 5632, 5637, 5643, 5644, 5648, 5651, 5653, 5658, 5659, 5662, 5666, 5668, 5671, 5676, 5677, 5682, 5687, 5689, 5694, 5697, 5700, 5702, 5705, 5709, 5711, 5714, 5719, 5724, 5725, 5729, 5731, 5736, 5737, 5739, 5747, 5751, 5752, 5755, 5759, 5761, 5763, 5765, 5766, 5768, 5770, 5775, 5776, 5779, 5781, 5784, 5788, 5793, 5794, 5798, 5802, 5803, 5808, 5814, 5816, 5820, 5823, 5824, 5829, 5832, 5838, 5839, 5844, 5846, 5848, 5853, 5856, 5857, 5863, 5868, 5874, 5878, 5885, 5888, 5891, 5893, 5896, 5898, 5900, 5904, 5906, 5909, 5911, 5916, 5920, 5926, 5928, 5929, 5936, 5939, 5940, 5943, 5946, 5950, 5953, 5961, 5963, 5966, 5968, 5970, 5971, 5974, 5977, 5979, 5982, 5987, 5989, 5993, 5994, 5999, 6001, 6003, 6009, 6013, 6016, 6018, 6024, 6028, 6032, 6035, 6036, 6041, 6044, 6045, 6049, 6052, 6055, 6058, 6062, 6065, 6066, 6069, 6073, 6075, 6078, 6084, 6085, 6088, 6091, 6094, 6096, 6100, 6104, 6107, 6110, 6111, 6112, 6114, 6117, 6122, 6124, 6126, 6128, 6129, 6130, 6135, 6140, 6143, 6146, 6149, 6151, 6154, 6158, 6159, 6161, 6163, 6167, 6169, 6172, 6176, 6182, 6188, 6189, 6191, 6194, 6198, 6201, 6203, 6208, 6209, 6212, 6221, 6225, 6231, 6235, 6238, 6239, 6240, 6244, 6245, 6248, 6253, 6255, 6259, 6260, 6266, 6269, 6276, 6279, 6281, 6284, 6288, 6290, 6291, 6293, 6296, 6299, 6301, 6304, 6311, 6313, 6314, 6320, 6321, 6326, 6328, 6334, 6336, 6340, 6342, 6347, 6351, 6356, 6358, 6359, 6363, 6367, 6368, 6372, 6376, 6377, 6381, 6384, 6386, 6391, 6393, 6395, 6400, 6401, 6404, 6410, 6411, 6414, 6419, 6424, 6428, 6432, 6435, 6437, 6440, 6444, 6448, 6453, 6454, 6459, 6464, 6467, 6470, 6474, 6477, 6479, 6482, 6486, 6487, 6492, 6495, 6498, 6501, 6506, 6509, 6511, 6515, 6518, 6520, 6523, 6525, 6529, 6532, 6535, 6539, 6541, 6546, 6552, 6555, 6560, 6561, 6565, 6571, 6572, 6576, 6580, 6582, 6585, 6586, 6589, 6590, 6594, 6598, 6604, 6607, 6609, 6611, 6618, 6620, 6624, 6626, 6629, 6632, 6636, 6640, 6643, 6645, 6653, 6658, 6659, 6664, 6665, 6668, 6672, 6676, 6677, 6679, 6680, 6684, 6686, 6691, 6692, 6695, 6699, 6702, 6703, 6704, 6709, 6710, 6715, 6717, 6719, 6722, 6725, 6729, 6731, 6739, 6741, 6745, 6748, 6752, 6753, 6756, 6759, 6760, 6764, 6767, 6770, 6773, 6776, 6777, 6782, 6785, 6786, 6789, 6793, 6794, 6799, 6802, 6804, 6811, 6813, 6819, 6823, 6827, 6829, 6833, 6836, 6837, 6840, 6844, 6848, 6850, 6851, 6852, 6855, 6858, 6862, 6867, 6869, 6871, 6875, 6877, 6880, 6883, 6887, 6889, 6893, 6894, 6896, 6899, 6905, 6906, 6911, 6913, 6918, 6922, 6925, 6927, 6930, 6935, 6940, 6944, 6947, 6948, 6952, 6955, 6959, 6960, 6963, 6967, 6970, 6972, 6975, 6980, 6983, 6989, 6992, 6995, 6997, 7003, 7007, 7009, 7012, 7015, 7018, 7021, 7023, 7027, 7030, 7031, 7035, 7037, 7041, 7044, 7046, 7049, 7052, 7057, 7060, 7061, 7062, 7064, 7068, 7069, 7071, 7073, 7077, 7079, 7080, 7082, 7086, 7088, 7093, 7095, 7097, 7102, 7103, 7107, 7111, 7113, 7114, 7118, 7121, 7125, 7128, 7130, 7133, 7139, 7142, 7145, 7147, 7151, 7152, 7157, 7158, 7163, 7164, 7167, 7170, 7173, 7176, 7177, 7179, 7182, 7187, 7189, 7192, 7194, 7201, 7205, 7206, 7210, 7214, 7217, 7220, 7222, 7224, 7229, 7231, 7234, 7238, 7241, 7242, 7250, 7252, 7256, 7261, 7271, 7272, 7275, 7277, 7279, 7281, 7284, 7288, 7291, 7294, 7297, 7300, 7304, 7305, 7308, 7312, 7314, 7317, 7318, 7321, 7323, 7326, 7330, 7332, 7334, 7336, 7340, 7341, 7347, 7348, 7351, 7355, 7364, 7367, 7370, 7372, 7374, 7376, 7378, 7379, 7380, 7383, 7384, 7389, 7390, 7395, 7397, 7399, 7404, 7406, 7410, 7415, 7416, 7419, 7422, 7424, 7428, 7430, 7436, 7439, 7442, 7447, 7450, 7457, 7460, 7462, 7464, 7468, 7470, 7475, 7480, 7481, 7487, 7490, 7493, 7499, 7501, 7505, 7507, 7512, 7517, 7520, 7522, 7524, 7528, 7532, 7535, 7536, 7541, 7544, 7548, 7551, 7556, 7557, 7560, 7566, 7575, 7579, 7581, 7588, 7589, 7593, 7595, 7600, 7601, 7604, 7609, 7612, 7614, 7618, 7622, 7624, 7630, 7634, 7636, 7639, 7640, 7643, 7644, 7650, 7652, 7655, 7656, 7660, 7662, 7672, 7674, 7675, 7677, 7678, 7679, 7681, 7685, 7688, 7691, 7695, 7698, 7699, 7701, 7703, 7706, 7708, 7709, 7711, 7712, 7715, 7718, 7723, 7728, 7733, 7735, 7737, 7740, 7744, 7747, 7750, 7751, 7753, 7754, 7758, 7762, 7764, 7768, 7770, 7774, 7780, 7786, 7787, 7791, 7794, 7797, 7799, 7802, 7806, 7808, 7812, 7815, 7818, 7821, 7823, 7828, 7829, 7832, 7835, 7839, 7843, 7846, 7847, 7850, 7851, 7854, 7855, 7859, 7860, 7861, 7863, 7866, 7873, 7877, 7878, 7882, 7885, 7887, 7890, 7892, 7893, 7898, 7903, 7906, 7915, 7916, 7921, 7922, 7927, 7930, 7932, 7936, 7938, 7940, 7945, 7947, 7949, 7959, 7960, 7961, 7963, 7964, 7966, 7970, 7974, 7975, 7978, 7982, 7984, 7987, 7991, 7994, 7999, 8001, 8004, 8007, 8010, 8012, 8015, 8020, 8025, 8027, 8029, 8032, 8033, 8035, 8039, 8043, 8046, 8047, 8048, 8050, 8053, 8055, 8056, 8063, 8069, 8072, 8077, 8079, 8084, 8087, 8090, 8091, 8093, 8097, 8101, 8104, 8114, 8116, 8118, 8123, 8124, 8129, 8132, 8133, 8137, 8139, 8142, 8145, 8147, 8149, 8153, 8156, 8159, 8163, 8166, 8177, 8180, 8182, 8186, 8191, 8198, 8199, 8203, 8207, 8210, 8211, 8215, 8217, 8222, 8223, 8226, 8230, 8236, 8237, 8241, 8242, 8243, 8247, 8248, 8251, 8255, 8259, 8260, 8262, 8266, 8271, 8274, 8277, 8281, 8283, 8285, 8287, 8291, 8292, 8297, 8298, 8301, 8305, 8308, 8310, 8315, 8318, 8319, 8324, 8327, 8330, 8334, 8339, 8341, 8347, 8351, 8354, 8357, 8360, 8363, 8368, 8370, 8371, 8373, 8380, 8384, 8385, 8388, 8391, 8395, 8397, 8398, 8401, 8403, 8409, 8415, 8423, 8429, 8431, 8435, 8436, 8439, 8443, 8446, 8450, 8452, 8455, 8458, 8465, 8468, 8480, 8481, 8486, 8489, 8492, 8493, 8494, 8496, 8503, 8504, 8505, 8508, 8512, 8516, 8519, 8520, 8525, 8529, 8533, 8537, 8543, 8545, 8547, 8552, 8555, 8559, 8563, 8568, 8573, 8578, 8579, 8582, 8586, 8588, 8593, 8596, 8599, 8603, 8607, 8609, 8613, 8616, 8617, 8621, 8628, 8629, 8633, 8636, 8638, 8640, 8643, 8644, 8647, 8654, 8657, 8658, 8662, 8663, 8666, 8669, 8674, 8680, 8683, 8684, 8689, 8692, 8695, 8700, 8702, 8705, 8708, 8709, 8714, 8719, 8720, 8723, 8728, 8730, 8733, 8735, 8740, 8742, 8745, 8750, 8754, 8760, 8762, 8764, 8768, 8771, 8774, 8776, 8781, 8783, 8785, 8792, 8794, 8799, 8803, 8808, 8809, 8816, 8818, 8819, 8822, 8825, 8828, 8830, 8833, 8841, 8843, 8850, 8857, 8861, 8862, 8865, 8869, 8872, 8875, 8877, 8884, 8886, 8891, 8893, 8897, 8901, 8905, 8908, 8912, 8914, 8916, 8921, 8925, 8929, 8936, 8940, 8943, 8947, 8950, 8953, 8954, 8961, 8963, 8968, 8971, 8972, 8977, 8980, 8987, 8989, 8993, 8996, 8999, 9001, 9006, 9007, 9015, 9018, 9021, 9024, 9025, 9028, 9029, 9031, 9032, 9034, 9036, 9039, 9042, 9044, 9046, 9050, 9052, 9059, 9065, 9067, 9071, 9073, 9075, 9079, 9083, 9085, 9087, 9092, 9093, 9099, 9101, 9102, 9110, 9112, 9113, 9116, 9121, 9123, 9127, 9130, 9132, 9135, 9136, 9139, 9145, 9146, 9150, 9153, 9155, 9158, 9161, 9164, 9169, 9177, 9179, 9183, 9188, 9191, 9194, 9198, 9201, 9204, 9207, 9211, 9212, 9217, 9219, 9221, 9225, 9228, 9233, 9237, 9240, 9243, 9245, 9246, 9248, 9251, 9253, 9261, 9262, 9266, 9270, 9274, 9277, 9280, 9283, 9287, 9289, 9293, 9297, 9299, 9302, 9304, 9308, 9311, 9313, 9314, 9316, 9319, 9322, 9327, 9328, 9332, 9334, 9337, 9339, 9340, 9348, 9353, 9359, 9360, 9361, 9365, 9368, 9370, 9373, 9381, 9382, 9387, 9390, 9392, 9394, 9395, 9397, 9400, 9403, 9408, 9411, 9413, 9416, 9420, 9422, 9424, 9426, 9428, 9431, 9433, 9434, 9441, 9444, 9445, 9449, 9451, 9455, 9461, 9463, 9469, 9473, 9476, 9479, 9481, 9484, 9488, 9491, 9493, 9498, 9501, 9506, 9509, 9511, 9513, 9518, 9519, 9524, 9526, 9529, 9536, 9538, 9542, 9543, 9549, 9554, 9558, 9562, 9565, 9567, 9570, 9573, 9576, 9581, 9583, 9585, 9590, 9592, 9595, 9599, 9604, 9605, 9607, 9608, 9609, 9612, 9613, 9616, 9618, 9620, 9621, 9626, 9628, 9632, 9634, 9635, 9636, 9638, 9640, 9643, 9646, 9649, 9653, 9654, 9659, 9660, 9665, 9667, 9668, 9671, 9679, 9683, 9684, 9689, 9690, 9693, 9697, 9701, 9706, 9708, 9713, 9715, 9718, 9721, 9725, 9728, 9731, 9732, 9737, 9740, 9743, 9746, 9748, 9750, 9755, 9761, 9762, 9766, 9770, 9773, 9776, 9785, 9788, 9790, 9792, 9797, 9798, 9801, 9806, 9807, 9808, 9812, 9815, 9817, 9819, 9822, 9827, 9831, 9833, 9835, 9836, 9837, 9839],

    # Handle 200 selected examples (100 entailment + 100 contradiction)
    "esnli": [2, 3, 9, 18, 23, 25, 71, 79, 82, 84, 100, 120, 132, 138, 147, 152, 173, 193, 197, 210, 224, 248, 257, 289, 293, 298, 307, 346, 355, 359, 367, 372, 375, 385, 393, 402, 405, 410, 414, 426, 427, 473, 476, 478, 507, 520, 521, 537, 540, 546, 571, 577, 593, 610, 613, 619, 630, 657, 662, 665, 668, 673, 684, 689, 692, 721, 834, 844, 845, 851, 867, 878, 918, 941, 954, 971, 976, 997, 1020, 1035, 1040, 1063, 1065, 1073, 1103, 1120, 1123, 1139, 1152, 1156, 1160, 1161, 1166, 1168, 1173, 1182, 1198, 1215, 1226, 1229, 1230, 1276, 1288, 1329, 1339, 1376, 1386, 1388, 1411, 1415, 1425, 1428, 1434, 1457, 1462, 1523, 1563, 1578, 1589, 1606, 1611, 1682, 1719, 1723, 1729, 1747, 1779, 1789, 1791, 1797, 1806, 1858, 1887, 1908, 1928, 1929, 1934, 1973, 1992, 2033, 2071, 2076, 2079, 2085, 2092, 2106, 2111, 2135, 2144, 2146, 2165, 2170, 2208, 2215, 2230, 2231, 2247, 2255, 2264, 2278, 2294, 2297, 2304, 2330, 2340, 2341, 2371, 2380, 2388, 2395, 2407, 2445, 2450, 2456, 2484, 2533, 2537, 2580, 2588, 2594, 2597, 2612, 2630, 2669, 2676, 2693, 2694, 2723, 2899, 2910, 2944, 2981, 2987, 2995, 3037, 3093, 3116, 3592, 3603, 3621],

    # Handle 100 contradiction examples only
    # "esnli": [23, 2, 71, 82, 120, 132, 138, 147, 152, 173, 193, 197, 224, 248, 289, 298, 359, 367, 372, 385, 393, 402, 405, 410, 478, 473, 520, 537, 593, 619, 630, 657, 673, 684, 834, 845, 851, 867, 878, 918, 971, 976, 1073, 1063, 1120, 1139, 1156, 1160, 1182, 1168, 1173, 1198, 1229, 1329, 1339, 1388, 1415, 1428, 1434, 1457, 1523, 1578, 1606, 1682, 1747, 1729, 1723, 1719, 1779, 1858, 1887, 1928, 1992, 2079, 2085, 2092, 2106, 2111, 2135, 2146, 2170, 2208, 2215, 2230, 2247, 2264, 2294, 2297, 2304, 2340, 2371, 2533, 2630, 2693, 2899, 3093, 3116, 3592, 3603, 3621]
}


def my_timer(my_func):

    @wraps(my_func)
    def timed(*args, **kw):
        tstart = time.time()
        output = my_func(*args, **kw)
        tend = time.time()
        print('"{}" took {:.3f} s to execute\n'.format(my_func.__name__, (tend - tstart)))
        return output

    return timed


class Evaluation(object):
    def __init__(self, model_wrapper, dev_set, deletion_level=0):
        self.model_wrapper = model_wrapper
        self.dev_set = dev_set
        self.deletion_level = deletion_level
        self.del_examples = []
        self.chunk_size_list = []

    # AUC: Model
    def generate_examples_for_deletion_method(self, corr_pred_only):
        self.del_examples = []
        self.chunk_size_list = []   # For splitting 1-size chunk of incorrectly-predicted examples
                                    # or > 1-size chunks for correctly predicted ones.

        for item in tqdm(self.dev_set):
            example = item["ori_example"]
            self.del_examples.append(example.get_input_example())

            # ONLY consider correct predictions
            if corr_pred_only and self.model_wrapper.labels.index(example.label) != example.pred_label:
                self.chunk_size_list.append(1)
                continue

            del_example = copy.deepcopy(example)
            flags = [True] * del_example.get_length()
            deletion_level = self.deletion_level
            if self.deletion_level == 0 or self.deletion_level >= len(flags):
                deletion_level = len(flags) - 1
                deletion_level = math.ceil(deletion_level * 0.2)  # Only replace 20% tokens per example.
                if deletion_level <= 1:
                    deletion_level = 2

            for i in range(deletion_level):  # The last removed sentence should include the last word
                max_pos = np.argmax(np.array(del_example.get_attribution_scores()))
                assert flags[max_pos.item()] is True
                flags[max_pos.item()] = False

                del_input_example = del_example.get_input_example_for_deletion_method(flags)
                self.del_examples.append(del_input_example)
                del_example.get_attribution_scores()[max_pos] = -999

            # Number of input-removed examples (including original input example)
            self.chunk_size_list.append(deletion_level + 1)

    # AUC_rep
    def generate_examples_for_replacement_method(self, corr_pred_only):
        self.del_examples = []
        self.chunk_size_list = []   # For splitting 1-size chunk of incorrectly-predicted examples
                                    # or > 1-size chunks for correctly predicted ones.

        for item in tqdm(self.dev_set):
            example = item["ori_example"]
            self.del_examples.append(example.get_input_example())

            # ONLY consider correct predictions
            if corr_pred_only and self.model_wrapper.labels.index(example.label) != example.pred_label:
                self.chunk_size_list.append(1)
                continue

            masked_example = copy.deepcopy(example)
            flags = [True] * masked_example.get_length()
            deletion_level = self.deletion_level
            if self.deletion_level == 0 or self.deletion_level >= len(flags):
                deletion_level = len(flags) - 1
                deletion_level = math.ceil(deletion_level * 0.2)  # Only replace 20% tokens per example.
                if deletion_level <= 1:
                    deletion_level = 2

            for i in range(deletion_level):  # The last removed sentence should include the last word
                max_pos = np.argmax(np.array(masked_example.get_attribution_scores()))
                assert flags[max_pos.item()] is True
                flags[max_pos.item()] = False # Useless in this function

                del_input_example = masked_example.get_input_example_for_replacement_method(self.model_wrapper, max_pos.item())
                self.del_examples.append(del_input_example)
                masked_example.get_attribution_scores()[max_pos] = -999

            # Number of input-removed examples (including original input example)
            self.chunk_size_list.append(deletion_level + 1)

    def generate_examples_for_roar(self, prefix, suffix, del_rate=0.2, use_bert=False, random_baseline=False):
        self.del_examples = []

        for item in tqdm(self.dev_set):
            example = item["ori_example"]

            del_example = copy.deepcopy(example)
            flags = [True] * del_example.get_length()
            deletion_level = self.deletion_level
            if self.deletion_level == 0 or self.deletion_level >= len(flags):
                deletion_level = len(flags) - 1
                deletion_level = math.ceil(deletion_level * del_rate)  # Only replace 20% tokens per example.
                if deletion_level <= 1:
                    deletion_level = 1  # Remove at least 1 token

            if not random_baseline:
                for i in range(deletion_level):  # The last removed sentence should include the last word
                    max_pos = np.argmax(np.array(del_example.get_attribution_scores()))
                    assert flags[max_pos.item()] is True
                    flags[max_pos.item()] = False
                    del_example.get_attribution_scores()[max_pos] = -999
            else:
                random_indices = random.sample(list(range(0, len(flags), 1)), k=deletion_level)
                flags = [True if idx not in random_indices else False for idx in range(len(flags))]

            # ROAR
            if not use_bert:
                del_input_example = del_example.get_input_example_for_deletion_method(flags)
            else:  # ROAR-BERT
                del_input_example = del_example.get_input_example_for_bert_based_roar_method(self.model_wrapper, flags)

            self.del_examples.append(del_input_example)

        lite_pickle_fb = prefix + "del_examples_" + str(del_rate) + ("_use_bert" if use_bert else "") + ("_baseline" if random_baseline else "") + suffix
        with open(lite_pickle_fb, "wb") as file_path:
            pickle.dump(self.del_examples, file_path)

    def compute_auc_score(self, method="LOO", corr_pred_only=False):

        # AUC_rep
        if method == "replacement":
            self.generate_examples_for_replacement_method(corr_pred_only)
        # Vanilla AUC
        else:
            self.generate_examples_for_deletion_method(corr_pred_only)

        all_conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.del_examples)
        auc_scores, auc_scores_norm = [], []
        count = 0

        for idx in range(len(self.chunk_size_list)):
            if idx == 0:
                start = 0
                end = self.chunk_size_list[idx]
            else:
                start = sum(self.chunk_size_list[:idx])
                end = sum(self.chunk_size_list[:idx + 1])

            # SKIP INCORRECT PREDICTIONS
            if end - start == 1:
                continue

            conf_scores = all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])

            # Prediction of original example when evaluation and analyzing must be the same
            # assert ori_pred == self.model_wrapper.labels.index(self.del_examples[start].label)
            # assert ori_pred == self.dev_set[idx]["ori_example"].get_pred_label()

            if ori_pred != self.dev_set[idx]["ori_example"].get_pred_label():
                count += 1

            # Compute AUC score
            auc_conf_scores = [conf_score[ori_pred] for conf_score in conf_scores[1:]]
            x = np.arange(1, len(auc_conf_scores) + 1) / len(auc_conf_scores)
            y_pred = auc_conf_scores

            if len(x) > 1:
                auc_score = auc(x, y_pred)
                auc_scores.append(auc_score)

        auc_scores = random.sample(auc_scores, 700)
        print("Evaluation metric: " + ("AUC" if method == "LOO" else "AUC_rep"))
        print("Average AUC score = " + str(statistics.mean(auc_scores)))
        print("Number of examples: " + str(len(auc_scores)))
        print("Number of examples changing predictions: " + str(count))

        return statistics.mean(auc_scores)

    def compute_IoU_score(self, analyzer, groundtruth_sets, alpha=0.05, visualize=False):

        def intersection(lst1, lst2):
            # return list(set(lst1) & set(lst2))
            return list((Counter(lst1) & Counter(lst2)).elements())

        def union(lst1, lst2):
            # return list(set().union(lst1, lst2))
            return list((Counter(lst1) | Counter(lst2)).elements())

        def compute_scores(sets_1, sets_2):
            IoU_scores = []

            for set1, set2 in zip(sets_1, sets_2):
                if len(union(set1, set2)) == 0:
                    IoU_scores.append(1)    # Assume that this example has no important words
                    continue

                IoU_score = len(intersection(set1, set2)) / len(union(set1, set2))
                IoU_scores.append(IoU_score)

            precision_scores, recall_scores = [], []
            for set1, set2 in zip(sets_1, sets_2):
                if len(set2) == 0:
                    precision_score = 1 if len(set1) == 0 else 0
                    recall_score = 1
                elif len(set1) == 0:
                    precision_score = recall_score = 0
                else:
                    precision_score = len(intersection(set1, set2)) / len(set1)
                    recall_score = len(intersection(set1, set2)) / len(set2)

                precision_scores.append(precision_score)
                recall_scores.append(recall_score)

            avg_IoU_score = round(sum(IoU_scores) / len(IoU_scores), 4)
            avg_precision_score = round(sum(precision_scores) / len(precision_scores), 4)
            avg_recall_score = round(sum(recall_scores) / len(recall_scores), 4)

            return avg_IoU_score, avg_precision_score, avg_recall_score

        # Each example includes a set of high-attribution words for predicted label (pos/neg)
        sets_0, sets_1, sets_2 = [], [], []
        gt_dev_set = [sent for sent in groundtruth_sets if str(sent.split) == '3']   # split = 3 for dev set

        # Skip examples whose lengths after tokenization >= 512 (For MultiRC, max_length of BertMLM = 512)
        # Handle this since gt_dev_set ALWAYS has full examples per split
        task_name = self.model_wrapper.data_args.task_name if self.model_wrapper.data_args.task_name != "sst-2" else "sst"
        skipped_list = skipped_indices[task_name + "_split{}".format(self.model_wrapper.split)] if self.model_wrapper.max_split > 1 else skipped_indices[task_name]
        gt_dev_set = [example for idx, example in enumerate(gt_dev_set) if idx not in skipped_list]

        # USE ONLY FOR ANALYZING ESNLI
        # kept_list = kept_indices[task_name + "_split{}".format(self.model_wrapper.split)] if self.model_wrapper.max_split > 1 else kept_indices[task_name]
        # gt_dev_set = [example for idx, example in enumerate(gt_dev_set) if idx in kept_list]

        # Number of examples must be equal between dev_set and groundtruth
        assert(len(self.dev_set) == len(gt_dev_set))
        # print("***** Number of evaluated examples in total: {} *****".format(len(self.dev_set)))
        count_correct_preds = 0

        # For Statistics
        annotation_stats = {"analyzer": [], "human": [], "sent_len": []}

        # Generate_high_attribution_set
        for idx, (item, item2) in enumerate(zip(self.dev_set, gt_dev_set)):
        # for idx, (item, item2) in tqdm(enumerate(zip(self.dev_set, gt_dev_set))):

            example = item["ori_example"]
            pred_label = example.get_pred_label()
            all_tokens = [x.token for x in example.get_all_attr_tokens()]

            # CORRECT PREDICTIONS ONLY
            if example.label not in self.model_wrapper.labels:
                example.label = '1' if float(example.label) >= 0.5 else '0'
            if self.model_wrapper.labels.index(example.label) != example.pred_label:
                continue

            count_correct_preds += 1

            # Prepare set0 (baseline) and set1 (RIS or OccEmpty)
            # Normalize + Zero-out and binarize
            if max(example.get_attribution_scores()) > 0:
                attr_scores = example.get_attribution_scores() / max(example.get_attribution_scores())
                attr_scores = [1 if score >= alpha else 0 for score in attr_scores]
            else:
                attr_scores = [0] * len(example.get_attribution_scores())

            # Baseline: All words are important
            # attr_scores = [1] * len(attr_scores)

            # Number of attribution scores and tokens must be equal for each example
            # Attribution score must be in [0, 1]
            assert len(all_tokens) == len(attr_scores)
            assert min(attr_scores) >= 0 and max(attr_scores) <= 1

            high_attr_tokens = [idx for idx, attr_score in enumerate(attr_scores) if attr_score == 1]
            sets_1.append(high_attr_tokens)
            sets_0.append([idx for idx, attr_score in enumerate(attr_scores)])  # Baseline: All words are highlighted

            # For Statistics
            annotation_stats["analyzer"].append(len(high_attr_tokens))
            annotation_stats["sent_len"].append(len(attr_scores))

            # Prepare set2: Groundtruth
            set2 = []
            if hasattr(item2, "phrases"):   # SST
                for phrase in item2.phrases:
                    # positive = 1 or negative = 0
                    if (pred_label == 1 and float(phrase.score) >= 0.7) or (pred_label == 0 and float(phrase.score) <= 0.3):
                        doc = nlp(phrase.text)
                        phrase_tokens = [token.text for token in doc]
                        for i in range(len(all_tokens) - len(phrase_tokens) + 1):
                            # if sub tokens are found in all_tokens and its length <= 50% of sentence length
                            if (all_tokens[i:i + len(phrase_tokens)] == phrase_tokens) and (len(phrase_tokens) <= len(all_tokens) / 2):
                                set2.append([idx for token, idx in zip(phrase_tokens, range(i, i + len(phrase_tokens), 1))])
                sets_2.append(list(set(itertools.chain(*set2))))
                annotation_stats["human"].append(len(sets_2[-1]))

            elif hasattr(item2, "highlights"): # ESNLI and MultiRC
                # ThangPM: Temporarily hot fixed.
                # Handle 24 cases that have empty tokens in either hypothesis or premise of sets_1.
                # Reason: Due to redundant spaces in the Huggingface datasets used --> Need to process this next time.
                if item2.processed_length != len(all_tokens):
                    del_indices = [i for i, x in enumerate(all_tokens) if x == " "]
                    all_tokens = [value for idx, value in enumerate(all_tokens) if idx not in del_indices]
                    if item2.processed_length != len(all_tokens):
                        print(all_tokens)
                    else:
                        attr_scores = [value for idx, value in enumerate(attr_scores) if idx not in del_indices]
                        high_attr_tokens = [idx for idx, attr_score in enumerate(attr_scores) if attr_score == 1]
                        sets_1[-1] = high_attr_tokens
                        annotation_stats["analyzer"][-1] = len(high_attr_tokens)
                        annotation_stats["sent_len"][-1] = len(attr_scores)

                set2 = sorted([idx for idx, token in item2.highlights["text_a"] + item2.highlights["text_b"]])
                sets_2.append(set2)
                annotation_stats["human"].append(len(sets_2[-1]))

            # Visualize sentences with highlight words to compare RIS with OccEmpty.
            if visualize:
                self.highlight_text(analyzer, idx, tokens=all_tokens, set1=sets_1[-1], set2=sets_2[-1], label=example.label)
                IoU_score = len(intersection(sets_1[-1], sets_2[-1])) / len(union(sets_1[-1], sets_2[-1]))
                if len(sets_2[-1]) == 0:
                    precision_score = 1 if len(sets_1[-1]) == 0 else 0
                    recall_score = 1
                elif len(sets_1[-1]) == 0:
                    precision_score = recall_score = 0
                else:
                    precision_score = len(intersection(sets_1[-1], sets_2[-1])) / len(sets_1[-1])
                    recall_score = len(intersection(sets_1[-1], sets_2[-1])) / len(sets_2[-1])
                print("\nIoU = {:.2f}, Precision = {:.2f}, Recall = {:.2f}".format(IoU_score, precision_score, recall_score))

        # For Statistics
        if visualize:
            analyzer_coverage, human_coverage = 0, 0
            for analyzer_attr_len, human_ann_len, example_len in zip(annotation_stats["analyzer"], annotation_stats["human"], annotation_stats["sent_len"]):
                analyzer_coverage += analyzer_attr_len / example_len
                human_coverage += human_ann_len / example_len
            analyzer_coverage /= len(annotation_stats["sent_len"])
            human_coverage /= len(annotation_stats["sent_len"])
            print("{} = {:.2f}, Human = {:.2f}, Examples Length Average = {:.2f}".format(analyzer, analyzer_coverage, human_coverage, statistics.mean(annotation_stats["sent_len"])))

            results = []
            for x, y in zip(annotation_stats["analyzer"], annotation_stats["sent_len"]):
                results.append(x / y)
            print("{}: Min = {:.2f} Max = {:.2f}".format(analyzer, min(results), max(results)))

            results = []
            for x, y in zip(annotation_stats["human"], annotation_stats["sent_len"]):
                results.append(x / y)
            print("{}: Min = {:.2f} Max = {:.2f}".format("Human", min(results), max(results)))

        IoU_baseline, precision_baseline, recall_baseline = compute_scores(sets_1=sets_0, sets_2=sets_2)
        IoU, precision, recall = compute_scores(sets_1=sets_1, sets_2=sets_2)

        # print("***** Number of evaluated examples: {} *****".format(count_correct_preds))

        return {"scores": (IoU, precision, recall), "scores_baseline": (IoU_baseline, precision_baseline, recall_baseline)}

    def highlight_text(self, analyzer, idx, tokens, set1, set2, label):

        print("\n----- Example {} ----- Label: {}".format(idx, label))

        # token's foreground is black, token's background is orange/yellow
        format = ';'.join([str(0), str(30), str(43)])

        highlight_tokens_set1 = copy.deepcopy(tokens)
        highlight_tokens_set2 = copy.deepcopy(tokens)

        for idx, token in enumerate(tokens):
            highlight_tokens_set1[idx] = '\x1b[%sm%s\x1b[0m' % (format, token) if idx in set1 else token
            highlight_tokens_set2[idx] = '\x1b[%sm%s\x1b[0m' % (format, token) if idx in set2 else token

        print(analyzer + ": " + " ".join(highlight_tokens_set1))    # highlight important words for set1
        print("\nHuman: " + " ".join(highlight_tokens_set2))          # highlight important words for set2


class Analyzer(object):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn):
        self.task_name = task_name
        self.analyzer = analyzer
        self.examples = []
        self.chunk_size_list = []
        self.all_conf_scores = [] # Output as confidence scores of a model on ALL original and generated candidates.
        self.dev_set = []

        checkpoint = model_wrapper.data_args.checkpoint
        model_base = model_wrapper.data_args.model_base
        masked_lm = model_wrapper.data_args.masked_lm
        masked_lm = "MLM-" + masked_lm if masked_lm and analyzer == "RIS" else None

        self.prefix = "pickle_files/" + \
                      model_base + "/" + \
                      folder_name_dict[task_name] + "/" + \
                      checkpoint + "/" + \
                      analyzer + "/" + \
                      (masked_lm + "/" if masked_lm else "")

                      # Only used for sanity check with multiple seeds
                      # Add the line below between analyzer and masked_lm
                      # str(model_wrapper.training_args.seed) + "/" + \

        # ThangPM 03-29-21:
        # If there are multiple splits --> suffix = _splitX.pickle
        # Else suffix = .pickle like before
        self.ending = pickle_fn.split(".")[0].split("_")[-1]    # splitX
        self.suffix = "_" + self.ending + ".pickle" if self.ending.startswith("split") else ".pickle"

        if not exists(self.prefix):
            makedirs(self.prefix)

        self.model_wrapper = model_wrapper
        self.pickle_fn = pickle_fn
        self.processed_pickle_fn = processed_pickle_fn

    def load_dev_set_from_file(self, pickle_fn, processed_pickle_fn):
        '''
        :param pickle_fn: file path to the pickle file generated by the function 'general_masked_datasets'
        :return: dev_set: array of a dictionary {"ori_example": ori_attr_example,
                                                 "masked_examples": generated_list,
                                                 "total_len": len(generated_list) + 1}
        '''
        lite_pickle_fb = self.prefix + processed_pickle_fn + self.suffix

        if not exists(lite_pickle_fb):
            with open(pickle_fn, "rb") as file_path:
                key = self.task_name.replace("-", "") + ("_" + self.ending if self.ending.startswith("split") else "")
                dev_set = pickle.load(file_path)[key]

                # Skip examples whose lengths after tokenization >= 512 (max_length of BertMLM = 512)
                if self.model_wrapper.max_split > 1:
                    skipped_indices_list = skipped_indices[self.task_name + "_" + self.ending]
                else:
                    skipped_indices_list = skipped_indices[self.task_name]
                self.dev_set = [item for idx, item in enumerate(self.dev_set) if idx not in skipped_indices_list]

            # Dump list of objects to masked_examples.pickle file
            with open(lite_pickle_fb, "wb") as file_path:
                pickle.dump(dev_set, file_path)
        else:
            with open(lite_pickle_fb, "rb") as file_path:
                dev_set = pickle.load(file_path)

        return dev_set
    def load_dev_set(self):
        lite_pickle_fb = self.prefix + FILLED_EXAMPLES + self.suffix
        with open(lite_pickle_fb, "rb") as file_path:
            self.dev_set = pickle.load(file_path)
    def save_dev_set(self):
        if len(self.dev_set) > 0:
            lite_pickle_fb = self.prefix + FILLED_EXAMPLES + self.suffix
            with open(lite_pickle_fb, "wb") as file_path:
                pickle.dump(self.dev_set, file_path)

    def load_examples(self):
        lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
        with open(lite_pickle_fb, "rb") as file_path:
            self.examples = pickle.load(file_path)

        lite_pickle_fb = self.prefix + CHUNK_SIZE_LIST + self.suffix
        with open(lite_pickle_fb, "rb") as file_path:
            self.chunk_size_list = pickle.load(file_path)
    def save_examples(self):
        lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
        with open(lite_pickle_fb, "wb") as file_path:
            pickle.dump(self.examples, file_path)

        lite_pickle_fb = self.prefix + CHUNK_SIZE_LIST + self.suffix
        with open(lite_pickle_fb, "wb") as file_path:
            pickle.dump(self.chunk_size_list, file_path)

        if len(self.dev_set) > 0:
            lite_pickle_fb = self.prefix + FILLED_EXAMPLES + self.suffix
            with open(lite_pickle_fb, "wb") as file_path:
                pickle.dump(self.dev_set, file_path)

    def load_chunk_size_list(self):
        lite_pickle_fb = self.prefix + CHUNK_SIZE_LIST + self.suffix
        with open(lite_pickle_fb, "rb") as file_path:
            self.chunk_size_list = pickle.load(file_path)

    def load_all_conf_scores(self):
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        with open(lite_pickle_fb, "rb") as file_path:
            self.all_conf_scores = pickle.load(file_path)
    def save_all_conf_scores(self):
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        with open(lite_pickle_fb, "wb") as file_path:
            pickle.dump(self.all_conf_scores, file_path)

    def set_dev_set(self, dev_set):
        self.dev_set = dev_set
    def get_dev_set(self):
        if len(self.dev_set) == 0:
            self.load_dev_set()

        return self.dev_set
    def get_examples(self):
        return self.examples

    def prepare_examples_for_analysis(self):
        pass
    def run(self):
        pass

    def get_attribution_viz_list(self, attr_scores_only=False):
        all_attr_scores, all_tokens = [], []
        logits, pred_labels, ground_truths = [], [], []
        label_list = {"RTE": ["entailment", "not_entailment"],
                      "SST-2": ["negative", "positive"],
                      "QQP": ["not_duplicate", "duplicate"]}

        for example in tqdm(self.dev_set):
            original_example = example["ori_example"]

            attr_scores, tokens = [], {"text_a": [], "text_b": []}
            for attr_token in original_example.get_attr_token_list_from_text_a():
                attr_scores.append(attr_token.get_attr_score())
                tokens["text_a"].append(attr_token.get_token())

            for attr_token in original_example.get_attr_token_list_from_text_b():
                attr_scores.append(attr_token.get_attr_score())
                tokens["text_b"].append(attr_token.get_token())

            all_attr_scores.append(attr_scores)
            all_tokens.append(tokens)
            logits.append(original_example.get_confidence_score())
            pred_labels.append(label_list[self.task_name][original_example.get_pred_label()])
            ground_truths.append(original_example.get_ground_truth())

        if attr_scores_only:
            return all_attr_scores

        return all_attr_scores, all_tokens, logits, pred_labels, ground_truths


class OcclusionAnalyzer(Analyzer):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn=MASKED_EXAMPLES, replaced_token=""):
        super().__init__(model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn)
        self.replaced_token = replaced_token

    def prepare_examples_for_analysis(self):
        self.dev_set = self.load_dev_set_from_file(self.pickle_fn, self.processed_pickle_fn)

        # Convert AttributionExamples objects to InputExample objects for prediction
        for idx in tqdm(range(len(self.dev_set))):
            original_example = self.dev_set[idx]["ori_example"]
            self.examples.append(original_example.get_input_example())

            masked_examples = self.dev_set[idx]["masked_examples"]
            filled_examples = original_example.generate_candidates_for_occ_token(masked_examples, self.replaced_token)
            self.examples.extend(filled_examples)

            self.chunk_size_list.append(len(self.examples))

    def run(self):

        def odds(score):
            return score / (1.0 - score)

        # Load a pickle file of all confidence scores if existed for faster analyzing.
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        if not exists(lite_pickle_fb):

            # Load a pickle file of all input examples if existed for faster analyzing.
            lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
            if not exists(lite_pickle_fb):
                self.prepare_examples_for_analysis()
                self.save_examples()
                self.dev_set.clear()  # No longer needed --> Save memory
            else:
                self.load_examples()
                print("***** LOADING PRE-GENERATED EXAMPLES *****")

            self.all_conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.examples)
            self.save_all_conf_scores()
        else:
            self.load_chunk_size_list()
            self.load_all_conf_scores()
            print("***** LOADING PRE-GENERATED CONFIDENCE SCORES *****")

        # Load dev_set for computing attribution scores
        self.load_dev_set()

        # OUR ALGORITHM
        '''
        for idx in range(len(self.chunk_size_list)):
            start = 0 if idx == 0 else self.chunk_size_list[idx-1]
            end = self.chunk_size_list[idx]

            conf_scores = self.all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])
            attr_scores = [(conf_scores[0] - conf_score)[ori_pred] for conf_score in conf_scores[1:]]

            # Update results for the original example
            self.dev_set[idx]["ori_example"].set_pred_label(ori_pred)
            self.dev_set[idx]["ori_example"].set_confidence_score(conf_scores[0][ori_pred])
            self.dev_set[idx]["ori_example"].set_attribution_scores(attr_scores)

        self.save_dev_set()
        '''

        # EMNLP ALGORITHM
        for idx in tqdm(range(len(self.chunk_size_list))):
            start = 0 if idx == 0 else self.chunk_size_list[idx-1]
            end = self.chunk_size_list[idx]

            conf_scores = self.all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])
            ori_conf_score = conf_scores[0][ori_pred]
            masked_conf_scores = [conf_score[ori_pred] for conf_score in conf_scores[1:]]

            # Compute attribution score
            attr_scores = np.log2(odds(ori_conf_score)) - np.log2([odds(conf_score) for conf_score in masked_conf_scores])

            # Update results for the original example
            self.dev_set[idx]["ori_example"].set_pred_label(ori_pred)
            self.dev_set[idx]["ori_example"].set_confidence_score(conf_scores[0][ori_pred])
            self.dev_set[idx]["ori_example"].set_attribution_scores(attr_scores)

        # Save the latest dev_set for visualization
        self.save_dev_set()
class InputMarginalizationAnalyzer(Analyzer):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn=MASKED_EXAMPLES, threshold=10e-5):
        super().__init__(model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn)
        self.threshold = threshold

    def prepare_examples_for_analysis(self):
        self.dev_set = self.load_dev_set_from_file(self.pickle_fn, self.processed_pickle_fn)

        # Convert AttributionExamples objects to InputExample objects for prediction
        for idx in tqdm(range(len(self.dev_set))):
            original_example = self.dev_set[idx]["ori_example"]
            self.examples.append(original_example.get_input_example())

            masked_examples = self.dev_set[idx]["masked_examples"]
            # ThangPM-NOTE: FIXED using top_N=10 since it's better than threshold
            filled_examples = original_example.generate_candidates_for_masked_token(masked_examples, self.model_wrapper, threshold=self.threshold, top_N=10)
            self.examples.extend(filled_examples)

            self.chunk_size_list.append(len(self.examples))

    def run(self):

        # Load a pickle file of all confidence scores if existed for faster analyzing.
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        if not exists(lite_pickle_fb):

            # Load a pickle file of all input examples if existed for faster analyzing.
            lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
            if not exists(lite_pickle_fb):
                self.prepare_examples_for_analysis()
                self.save_examples()
                self.dev_set.clear()  # No longer needed --> Save memory
            else:
                self.load_examples()
                print("***** LOADING PRE-GENERATED EXAMPLES *****")

            split = 5 if self.task_name == "multirc" else 1  # split can be adjust to > 5 if the number of generated examples are large.
            chunk_size = int(len(self.examples) / split)
            for i in range(split+1):                    # +1 for handling the remaining examples
                if len(self.examples[:chunk_size]) > 0:
                    conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.examples[:chunk_size])
                    self.all_conf_scores.extend(conf_scores)
                    del self.examples[:chunk_size]      # No longer needed --> Save memory

            self.save_all_conf_scores()
        else:
            self.load_chunk_size_list()
            self.load_all_conf_scores()
            print("***** LOADING PRE-GENERATED CONFIDENCE SCORES *****")

        # Load dev_set for computing attribution scores
        self.load_dev_set()

        for idx in tqdm(range(len(self.chunk_size_list))):
            if idx == 129:
                print("debug")

            start = 0 if idx == 0 else self.chunk_size_list[idx-1]
            end = self.chunk_size_list[idx]

            conf_scores = self.all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])

            # ThangPM notes: Temporarily force the random classifiers to explain the target class 'positive'
            # since its prediction is 'negative' --> Cannot compare with the original classifier.
            # target_class = 1 if idx == 97 else None   # For SST task
            target_class = 2 if idx == 129 else None    # For ESNLI task
            # target_class = None

            # Compute attribution score
            masked_examples = self.dev_set[idx]["masked_examples"]
            attr_scores = self.compute_input_margination(masked_examples, conf_scores, target_class=target_class)

            # Update results for the original example
            self.dev_set[idx]["ori_example"].set_pred_label(ori_pred)
            self.dev_set[idx]["ori_example"].set_confidence_score(conf_scores[0][ori_pred])
            self.dev_set[idx]["ori_example"].set_attribution_scores(attr_scores)

        # Save the latest dev_set for visualization
        self.save_dev_set()

    def compute_input_margination(self, masked_examples, conf_scores, topN=None, target_class=None):

        def odds(score):
            return score / (1.0 - score)

        # ThangPM 10/13/21: Update target_class to force the classifier to explain
        # when the target_class is different with the predicted class.
        ori_pred = target_class if target_class else np.argmax(conf_scores[0])

        # Confidence scores
        ori_conf_score = conf_scores[0][ori_pred]
        all_masked_conf_scores = [conf_score[ori_pred] for conf_score in conf_scores[1:]]

        # Get probability scores for the candidate.
        all_mlm_probs = [masked_example.get_candidates()[1] for masked_example in masked_examples]
        start, end = 0, len(all_mlm_probs[0])
        masked_probs = []

        for idx, mlm_probs in enumerate(all_mlm_probs):
            if idx > 0:
                start += len(all_mlm_probs[idx - 1])
                end += len(all_mlm_probs[idx])

            masked_conf_scores = all_masked_conf_scores[start:end]
            assert (len(mlm_probs) == len(masked_conf_scores))

            if topN and len(mlm_probs) >= topN:
                mlm_probs = mlm_probs[:topN]
                masked_conf_scores = masked_conf_scores[:topN]

            masked_prob = [masked_conf_score * mlm_prob for masked_conf_score, mlm_prob in zip(masked_conf_scores, mlm_probs)]
            masked_prob = sum(masked_prob) / sum(mlm_probs)
            masked_probs.append(masked_prob)

        attr_scores = np.log2(odds(ori_conf_score)) - np.log2([odds(prob) for prob in masked_probs])
        return attr_scores
class LimeAnalyzer(Analyzer):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn=MASKED_EXAMPLES, replaced_token=""):
        super().__init__(model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn)
        self.replaced_token = replaced_token

    def prepare_examples_for_analysis(self):
        self.dev_set = self.load_dev_set_from_file(self.pickle_fn, self.processed_pickle_fn)

        # Convert AttributionExamples objects to InputExample objects for prediction
        for idx in tqdm(range(len(self.dev_set))):
            original_example = self.dev_set[idx]["ori_example"]
            self.examples.append(original_example.get_input_example())
            self.chunk_size_list.append(len(self.examples))

    def run(self):
        # Create a LIME explainer object
        explainer = LimeTextExplainer(class_names=self.model_wrapper.get_labels())

        # Load a pickle file of all confidence scores if existed for faster analyzing.
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        if not exists(lite_pickle_fb):

            # Load a pickle file of all input examples if existed for faster analyzing.
            lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
            if not exists(lite_pickle_fb):
                self.prepare_examples_for_analysis()
                self.save_examples()
                self.dev_set.clear()  # No longer needed --> Save memory
            else:
                self.load_examples()
                print("***** LOADING PRE-GENERATED EXAMPLES *****")

            self.all_conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.examples)
            self.save_all_conf_scores()
        else:
            self.load_chunk_size_list()
            self.load_all_conf_scores()
            print("***** LOADING PRE-GENERATED CONFIDENCE SCORES *****")

        # Load dev_set for computing attribution scores
        self.load_dev_set()

        # For debugging only
        # self.dev_set = self.dev_set[:10]
        # self.all_conf_scores = self.all_conf_scores[:10]

        for idx, (example, logits) in tqdm(enumerate(zip(self.dev_set, self.all_conf_scores))):
            pred_label = np.argmax(logits)
            conf_score = logits[pred_label]

            # Generate heatmap for text_a first
            self.model_wrapper.set_text_a("")
            self.model_wrapper.set_text_b(example["ori_example"].get_text_b())
            attr_scores_text_a = self.get_lime_score(explainer, self.model_wrapper, example["ori_example"].get_text_a(),
                                                     pred_label=pred_label, use_mlm=self.analyzer in ["LIME-BERT", "LIME-BERT-SST2"], num_samples=1000)

            # Then, generate heatmap for text_b if the task is ESNLI or MultiRC
            attr_scores_text_b = []
            if self.task_name == "esnli" or self.task_name == "multirc":
                self.model_wrapper.set_text_a(example["ori_example"].get_text_a())
                self.model_wrapper.set_text_b("")
                attr_scores_text_b = self.get_lime_score(explainer, self.model_wrapper, example["ori_example"].get_text_b(),
                                                         pred_label=pred_label, use_mlm=self.analyzer in ["LIME-BERT", "LIME-BERT-SST2"], num_samples=1000)

            attr_scores = attr_scores_text_a + (attr_scores_text_b if self.task_name == "esnli" or self.task_name == "multirc" else [])

            # Update results for the original example
            example["ori_example"].set_pred_label(pred_label)
            example["ori_example"].set_confidence_score(conf_score)
            example["ori_example"].set_attribution_scores(np.array(attr_scores))

        # Save the latest dev_set for visualization
        self.save_dev_set()

    def get_lime_score(self, explainer, model_wrapper, text_instance, pred_label, use_mlm, num_samples=1000):

        # ThangPM: LIME failed to handle cases where text_instance has ONE token --> Assign score 0 for those cases
        if len(text_instance.split(" ")) == 1:
            print("---------- text_instance: {}".format(text_instance))
            return [0]

        LIME_scores = []
        lime_explanations = explainer.explain_instance(text_instance=text_instance,
                                                       classifier_fn=model_wrapper.predict_proba,
                                                       top_labels=len(model_wrapper.get_labels()),
                                                       num_features=len(text_instance.split(' ')),
                                                       num_samples=num_samples,
                                                       use_mlm=use_mlm,
                                                       model_wrapper=model_wrapper)

        for token in text_instance.split(" "):
            t_score = 0
            for word, score in lime_explanations.as_list(label=pred_label):
                if token.lower() == word.lower():
                    t_score = score

            if t_score == 0:
                # Remove special characters and get the score one more time
                new_token = ''.join(c for c in token if c.isalnum())
                for word, score in lime_explanations.as_list(label=pred_label):
                    if new_token.lower() == word.lower():
                        t_score = score

            # Remember: DO NOT round score here. Just round score for visualization only
            LIME_scores.append(t_score)

        return LIME_scores


class RisAnalyzer(Analyzer):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn=MASKED_EXAMPLES, top_N=10, ris_type="local_softmax"):
        super().__init__(model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn)
        self.top_N = top_N
        self.ris_type = ris_type

    def prepare_examples_for_analysis(self):
        # Convert AttributionExamples objects to InputExample objects for prediction
        for idx in tqdm(range(len(self.dev_set))):
            original_example = self.dev_set[idx]["ori_example"]
            self.examples.append(original_example.get_input_example())

            masked_examples = self.dev_set[idx]["masked_examples"]
            filled_examples = original_example.generate_candidates_for_masked_token(masked_examples, self.model_wrapper, top_N=self.top_N)
            self.examples.extend(filled_examples)

            self.chunk_size_list.append(len(self.examples))

    def run(self):
        # Load a pickle file of all input examples if existed for faster analyzing.
        lite_pickle_fb = self.prefix + INPUT_EXAMPLES + self.suffix
        if not exists(lite_pickle_fb):
            self.prepare_examples_for_analysis()
            self.save_examples()
        else:
            self.load_examples()
            print("***** LOADING PRE-GENERATED EXAMPLES *****")

        # Load a pickle file of all confidence scores if existed for faster analyzing.
        lite_pickle_fb = self.prefix + ALL_CONF_SCORES + self.suffix
        if not exists(lite_pickle_fb):
            self.all_conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.examples)
            self.save_all_conf_scores()
        else:
            self.load_all_conf_scores()
            print("***** LOADING PRE-GENERATED CONFIDENCE SCORES *****")

        for idx in range(len(self.chunk_size_list)):
            start = 0 if idx == 0 else self.chunk_size_list[idx-1]
            end = self.chunk_size_list[idx]

            conf_scores = self.all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])
            all_attr_scores = [(conf_scores[0] - conf_score)[ori_pred] for conf_score in conf_scores[1:]]

            # self.top_N, self.ris_type
            # Compute AVERAGE attribution score based on top_N candidate for each <mask> token.
            weights, average_attr_scores = [], []
            masked_examples = self.dev_set[idx]["masked_examples"]

            # Get probability scores for corresponding ris_type. Default is average
            if self.ris_type == "global_softmax" or self.ris_type == "local_softmax":
                all_probs = []
                for masked_example in masked_examples:
                    if self.ris_type == "global_softmax":
                        probs = masked_example.get_candidates()[1] # 0 is token; 1 is probs
                    else:
                        probs = softmax(masked_example.get_candidates()[1])

                    # probs = masked_example.get_candidates()[1]        # FOR TUNING HYPER-PARAMETERS ONLY ---> TOP_N
                    all_probs.append(probs)
            else:
                all_probs = [1.0 / len(masked_examples)] * len(masked_examples)

            # Compute probability for each word
            for i in range(0, len(all_attr_scores), self.top_N):
                # Attribution and probability scores of top_N examples
                attr_scores = all_attr_scores[i:i+self.top_N]
                probs = all_probs[int(i / self.top_N)]

                # --------------------------------------------------------------------------------
                # ThangPM: Adjust the number of candidates proposed by MLM for one [MASK] token.
                #          FOR TUNING HYPER-PARAMETERS ONLY ---> TOP_N
                # --------------------------------------------------------------------------------
                # For THRESHOLD
                # threshold = 0.00001
                # probs = [prob for prob in probs if prob >= threshold]
                # attr_scores = attr_scores[:len(probs)]
                # probs = softmax(probs[:len(probs)])

                # attr_scores = attr_scores[:10]
                # probs = softmax(probs[:10])
                # --------------------------------------------------------------------------------

                average_attr_scores.append(sum([attr_score * weight for attr_score, weight in zip(attr_scores, probs)]))

            # Update results for the original example
            self.dev_set[idx]["ori_example"].set_pred_label(ori_pred)
            self.dev_set[idx]["ori_example"].set_confidence_score(conf_scores[0][ori_pred])
            self.dev_set[idx]["ori_example"].set_attribution_scores(average_attr_scores)

        # Save the latest dev_set for visualization
        self.save_dev_set()
class OcclusionEmptyAnalyzer(Analyzer):
    def __init__(self, model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn=MASKED_EXAMPLES):
        super().__init__(model_wrapper, task_name, analyzer, pickle_fn, processed_pickle_fn)

    def prepare_examples_for_analysis(self):
        # Convert AttributionExamples objects to InputExample objects for prediction
        for idx in range(len(self.dev_set)):
            self.examples.append(self.dev_set[idx]["ori_example"].get_input_example())
            self.examples.extend([attr_example.get_input_example() for attr_example in self.dev_set[idx]["masked_examples"]])
            self.chunk_size_list.append(len(self.examples))

    def run(self):
        self.prepare_examples_for_analysis()
        all_conf_scores, _ = self.model_wrapper.predict_proba_with_examples(self.examples)

        for idx in range(len(self.chunk_size_list)):
            start = 0 if idx == 0 else self.chunk_size_list[idx-1]
            end = self.chunk_size_list[idx]

            conf_scores = all_conf_scores[start:end]
            ori_pred = np.argmax(conf_scores[0])
            attr_scores = [(conf_scores[0] - conf_score)[ori_pred] for conf_score in conf_scores[1:]]

            # Update results for the original example
            self.dev_set[idx]["ori_example"].set_pred_label(ori_pred)
            self.dev_set[idx]["ori_example"].set_confidence_score(conf_scores[0][ori_pred])
            self.dev_set[idx]["ori_example"].set_attribution_scores(attr_scores)

        self.save_dev_set()


def run_occlusion(model_wrapper, task_name, analyzer, pickle_fn, replaced_token):
    occ_analyzer = OcclusionAnalyzer(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn,
                                     # FOR TESTING NEW EVALUATION METHOD --> IF USE, DISABLE RUN() AS WELL
                                     processed_pickle_fn=FILLED_EXAMPLES,
                                     replaced_token=replaced_token)
    occ_analyzer.run()

    # Handle this temporarily since I haven't skipped those examples when running OccEmpty

    if task_name == "multirc" or task_name == "esnli":
        skipped_list = skipped_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else skipped_indices[task_name]
        # kept_list = kept_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else kept_indices[task_name]

        dev_set = [example for idx, example in enumerate(occ_analyzer.get_dev_set()) if idx not in skipped_list]
        occ_analyzer.set_dev_set(dev_set)

    occ_evaluation = Evaluation(model_wrapper, occ_analyzer.get_dev_set())
    # occ_evaluation.compute_auc_score("replacement", corr_pred_only=False)
    occ_evaluation.compute_auc_score(corr_pred_only=True)                   # AUC
    occ_evaluation.compute_auc_score("replacement", corr_pred_only=True)    # AUC_rep

    # -----------------------------------------------------------------------------
    # FOR ROAR ONLY
    # -----------------------------------------------------------------------------
    '''
    # del_rates = [0.2, 0.5, 0.8]
    del_rates = [0.1, 0.2, 0.3]
    for del_rate in del_rates:
        occ_evaluation.generate_examples_for_roar(prefix=occ_analyzer.prefix,
                                                  suffix=occ_analyzer.suffix,
                                                  del_rate=del_rate, use_bert=True,
                                                  random_baseline=False)
    '''
    # -----------------------------------------------------------------------------
    # FOR IoU ONLY
    # -----------------------------------------------------------------------------
    '''
    alphas = np.arange(0.05, 1, 0.05)
    # alphas = [0.05] # levels = 0.05, 0.2, 0.45

    groundtruth_path = "pickle_files/{}_preprocessed{}.pickle".format(task_name, ("_2" if task_name == "esnli" else ""))
    with open(groundtruth_path, "rb") as file_path:
        groundtruth_sets = pickle.load(file_path)

        if model_wrapper.split != None:
            chunk_size = int(len(groundtruth_sets) / model_wrapper.max_split)
            groundtruth_sets = groundtruth_sets[model_wrapper.split*chunk_size:(model_wrapper.split+1)*chunk_size]

    scores_baseline = []
    for alpha in alphas:
        results = occ_evaluation.compute_IoU_score(analyzer, groundtruth_sets, alpha, visualize=True if len(alphas) == 1 else False)
        IoU, precision, recall = results["scores"]
        scores_baseline.append(results["scores_baseline"])
        print("\t".join([str(alpha), str(IoU), str(precision), str(recall)]))   # For easily copying to Google Sheets.

    for (IoU_baseline, precision_baseline, recall_baseline) in tuple(set(scores_baseline)):
        print("\t".join(["Baseline:", str(IoU_baseline), str(precision_baseline), str(recall_baseline)]))    # For easily copying to Google Sheets.
    '''
    del occ_analyzer
def run_input_marginalization(model_wrapper, task_name, analyzer, pickle_fn, threshold):
    input_margin_analyzer = InputMarginalizationAnalyzer(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn,
                                                         # FOR TESTING NEW EVALUATION METHOD --> IF USE, DISABLE RUN() AS WELL
                                                         processed_pickle_fn=FILLED_EXAMPLES,
                                                         threshold=threshold,)
    input_margin_analyzer.run()

    # Handle this temporarily since I haven't skipped those examples when running
    '''
    if task_name == "esnli":
        skipped_list = skipped_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else skipped_indices[task_name]
        # kept_list = kept_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else kept_indices[task_name]

        dev_set = [example for idx, example in enumerate(input_margin_analyzer.get_dev_set()) if idx not in skipped_list]
        input_margin_analyzer.set_dev_set(dev_set)

    input_margin_evaluation = Evaluation(model_wrapper, input_margin_analyzer.get_dev_set())
    # input_margin_evaluation.compute_auc_score("replacement", corr_pred_only=False)
    input_margin_evaluation.compute_auc_score(corr_pred_only=True)                  # AUC
    input_margin_evaluation.compute_auc_score("replacement", corr_pred_only=True)   # AUC_rep
    '''
    # -----------------------------------------------------------------------------
    # FOR ROAR ONLY
    # -----------------------------------------------------------------------------
    '''
    # del_rates = [0.2, 0.5, 0.8]
    del_rates = [0.1, 0.2, 0.3]
    for del_rate in del_rates:
        input_margin_evaluation.generate_examples_for_roar(prefix=input_margin_analyzer.prefix,
                                                           suffix=input_margin_analyzer.suffix,
                                                           del_rate=del_rate, use_bert=False,
                                                           random_baseline=True)
    '''
    # -----------------------------------------------------------------------------
    # FOR IoU ONLY
    # -----------------------------------------------------------------------------
    '''
    alphas = np.arange(0.05, 1, 0.05)
    # alphas = [0.05] # levels = 0.05, 0.15, 0.45

    groundtruth_path = "pickle_files/{}_preprocessed{}.pickle".format(task_name, ("_2" if task_name == "esnli" else ""))
    with open(groundtruth_path, "rb") as file_path:
        groundtruth_sets = pickle.load(file_path)

        if model_wrapper.split != None:
            chunk_size = int(len(groundtruth_sets) / model_wrapper.max_split)
            groundtruth_sets = groundtruth_sets[model_wrapper.split*chunk_size:(model_wrapper.split+1)*chunk_size]

    scores_baseline = []
    for alpha in alphas:
        results = input_margin_evaluation.compute_IoU_score(analyzer, groundtruth_sets, alpha, visualize=True if len(alphas) == 1 else False)
        IoU, precision, recall = results["scores"]
        scores_baseline.append(results["scores_baseline"])
        print("\t".join([str(alpha), str(IoU), str(precision), str(recall)]))   # For easily copying to Google Sheets.

    for (IoU_baseline, precision_baseline, recall_baseline) in tuple(set(scores_baseline)):
        print("\t".join(["Baseline:", str(IoU_baseline), str(precision_baseline), str(recall_baseline)]))    # For easily copying to Google Sheets.
    '''
    del input_margin_analyzer
def run_lime(model_wrapper, task_name, analyzer, pickle_fn, replaced_token):
    lime_analyzer = LimeAnalyzer(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn,
                                 # FOR TESTING NEW EVALUATION METHOD --> IF USE, DISABLE RUN() AS WELL
                                 processed_pickle_fn=FILLED_EXAMPLES,
                                 replaced_token=replaced_token)
    # lime_analyzer.run()

    # Handle this temporarily since I haven't skipped those examples when running OccEmpty
    if task_name == "multirc" or task_name == "esnli":
        skipped_list = skipped_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else skipped_indices[task_name]
        # kept_list = kept_indices[task_name + "_split{}".format(model_wrapper.split)] if model_wrapper.max_split > 1 else kept_indices[task_name]

        dev_set = [example for idx, example in enumerate(lime_analyzer.get_dev_set()) if idx in skipped_list]
        lime_analyzer.set_dev_set(dev_set)

    lime_evaluation = Evaluation(model_wrapper, lime_analyzer.get_dev_set())
    # lime_evaluation.compute_auc_score("replacement", corr_pred_only=False)
    # lime_evaluation.compute_auc_score("replacement", corr_pred_only=True)

    # -----------------------------------------------------------------------------
    # FOR ROAR ONLY
    # -----------------------------------------------------------------------------

    # del_rates = [0.2, 0.5, 0.8]
    del_rates = [0.1, 0.2, 0.3]
    for del_rate in del_rates:
        lime_evaluation.generate_examples_for_roar(prefix=lime_analyzer.prefix,
                                                   suffix=lime_analyzer.suffix,
                                                   del_rate=del_rate, use_bert=True,
                                                   random_baseline=False)

    # -----------------------------------------------------------------------------
    # FOR IoU ONLY
    # -----------------------------------------------------------------------------
    '''
    alphas = np.arange(0.05, 1, 0.05)
    # alphas = [0.05] # levels = 0.05, 0.2, 0.45

    groundtruth_path = "pickle_files/{}_preprocessed{}.pickle".format(task_name, ("_2" if task_name == "esnli" else ""))
    with open(groundtruth_path, "rb") as file_path:
        groundtruth_sets = pickle.load(file_path)

        if model_wrapper.split != None:
            chunk_size = int(len(groundtruth_sets) / model_wrapper.max_split)
            groundtruth_sets = groundtruth_sets[model_wrapper.split * chunk_size:(model_wrapper.split + 1) * chunk_size]

    scores_baseline = []
    for alpha in alphas:
        results = lime_evaluation.compute_IoU_score(analyzer, groundtruth_sets, alpha,
                                                    visualize=True if len(alphas) == 1 else False)
        IoU, precision, recall = results["scores"]
        scores_baseline.append(results["scores_baseline"])
        print("\t".join([str(alpha), str(IoU), str(precision), str(recall)]))  # For easily copying to Google Sheets.

    for (IoU_baseline, precision_baseline, recall_baseline) in tuple(set(scores_baseline)):
        print("\t".join(["Baseline:", str(IoU_baseline), str(precision_baseline), str(recall_baseline)]))  # For easily copying to Google Sheets.

    del lime_analyzer
    '''


# OLD IMPLEMENTATION
def run_occ_empty(model_wrapper, task_name):
    occ_analyzer = OcclusionEmptyAnalyzer(model_wrapper, task_name, analyzer="OccEmpty",
                                          pickle_fn="pickle_files/2021-02-10_21-41-58_masked_examples_glue_occlusion.pickle",
                                          # processed_pickle_fn=FILLED_EXAMPLES, # FOR TESTING NEW EVALUATION METHOD
                                          )
    occ_analyzer.run()

    occ_evaluation = Evaluation(model_wrapper, occ_analyzer.get_dev_set())
    occ_evaluation.compute_auc_score()
def run_ris(model_wrapper, task_name, top_N, ris_type, analyzer):
    ris_analyzer = RisAnalyzer(model_wrapper, task_name, analyzer=analyzer,
                               pickle_fn="pickle_files/2021-02-10_21-19-55_masked_examples_glue_ris.pickle",
                               # processed_pickle_fn=FILLED_EXAMPLES,   # FOR TUNING HYPER-PARAMETERS ONLY ---> TOP_N
                               top_N=top_N, ris_type=ris_type,)
    ris_analyzer.run()

    ris_evaluation = Evaluation(model_wrapper, ris_analyzer.get_dev_set())
    ris_evaluation.compute_auc_score()


def run_vizualization(model_wrapper, task_name, top_N, ris_type):
    attribution_viz_list = []

    # The file filled_examples.pickle must be existed for visualization.
    occ_zero_analyzer = OcclusionAnalyzer(model_wrapper, task_name, analyzer="OccZero",
                                          pickle_fn="pickle_files/2021-02-10_21-19-55_masked_examples_glue_ris.pickle",
                                          processed_pickle_fn=FILLED_EXAMPLES)

    occ_analyzer = OcclusionEmptyAnalyzer(model_wrapper, task_name, analyzer="OccEmpty",
                                          pickle_fn="pickle_files/2021-02-10_21-41-58_masked_examples_glue_occlusion.pickle",
                                          processed_pickle_fn=FILLED_EXAMPLES)

    ris_analyzer = RisAnalyzer(model_wrapper, task_name, analyzer="RIS",
                               pickle_fn="pickle_files/2021-02-10_21-19-55_masked_examples_glue_ris.pickle",
                               processed_pickle_fn=FILLED_EXAMPLES, top_N=top_N, ris_type=ris_type)

    all_occ_zero_attr_scores = occ_zero_analyzer.get_attribution_viz_list(attr_scores_only=True)
    all_occ_attr_scores = occ_analyzer.get_attribution_viz_list(attr_scores_only=True)
    all_ris_attr_scores, all_tokens, logits, pred_labels, ground_truths = ris_analyzer.get_attribution_viz_list()

    if len(all_occ_zero_attr_scores) == 0:
        all_occ_zero_attr_scores = [0] * len(all_ris_attr_scores)
    if len(all_occ_attr_scores) == 0:
        all_occ_attr_scores = [0] * len(all_ris_attr_scores)

    for (tokens, pred_label_ris, logit, ground_truth, ris_attr_scores, occ_zero_scores, occ_scores) in \
        zip(all_tokens, pred_labels, logits, ground_truths, all_ris_attr_scores, all_occ_zero_attr_scores, all_occ_attr_scores):

        attribution_viz = AttributionVisualization(tokens, "", pred_label_ris, logit, ground_truth,
                                                   ris_attr_scores, [], [], [], occ_zero_scores, occ_scores)
        attribution_viz_list.append(attribution_viz)

    # Visualize results
    visualize_results_miniQ1(attribution_viz_list, task_name=task_name, model=model_wrapper.model_args.model_name_or_path.split("/")[4])
@my_timer
def run_analyzers(top_N, ris_type, threshold):

    # Prepare a classifier for LIME to generate explanations
    model_wrapper = RoBERTa_Model_Wrapper()
    task_name = model_wrapper.data_args.task_name if model_wrapper.data_args.task_name != "sst-2" else "sst" # "sst" or "sst-2" (when SST-2?)
    analyzer = model_wrapper.data_args.analyzer
    model_wrapper.max_split = 1 if task_name != "multirc" else 3

    for i in range(model_wrapper.max_split):

        mode = "train" if model_wrapper.data_args.checkpoint.find("train") != -1 else "dev"
        pickle_fn = "pickle_files/masked_examples_{}_{}_split{}.pickle".format(mode, task_name, i)
        model_wrapper.split = i
        if model_wrapper.max_split == 1:
            pickle_fn = "pickle_files/masked_examples_{}_{}.pickle".format(mode, task_name)
            model_wrapper.split = None

        if analyzer == "RIS":
            run_ris(model_wrapper, task_name, top_N=top_N, ris_type=ris_type, analyzer=analyzer)
        elif analyzer == "OccEmpty":
            # run_occ_empty(model_wrapper, task_name) # Old fashion --> Use the new one below
            run_occlusion(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn, replaced_token="")
        elif analyzer == "LIME" or analyzer == "LIME-BERT" or analyzer == "LIME-BERT-SST2":
            replace_token = "" if analyzer == "LIME" else model_wrapper.mask_token
            run_lime(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn, replaced_token=replace_token)

        elif analyzer == "InputMargin":
            run_input_marginalization(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn, threshold=threshold)
        elif analyzer == "OccZero":
            run_occlusion(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn, replaced_token="[PAD]")
        elif analyzer == "OccUnk":
            run_occlusion(model_wrapper, task_name, analyzer=analyzer, pickle_fn=pickle_fn, replaced_token="[UNK]")

        elif analyzer == "visualization":
            run_vizualization(model_wrapper, task_name, top_N=top_N, ris_type=ris_type)


def compute_spearman_correlation(visualize=True):

    def get_attribution_viz_list(examples, task_name, attr_scores_only=False):
        all_attr_scores, all_tokens = [], []
        logits, pred_labels, ground_truths = [], [], []
        label_list = {"SST": ["negative", "positive"],
                      "SST-2": ["negative", "positive"],
                      "ESNLI": ["entailment", "neutral", "contradiction"],}

        for example in examples:
            original_example = example["ori_example"]

            attr_scores, tokens = [], {"text_a": [], "text_b": []}
            for attr_token in original_example.get_attr_token_list_from_text_a():
                attr_scores.append(attr_token.get_attr_score())
                tokens["text_a"].append(attr_token.get_token())

            for attr_token in original_example.get_attr_token_list_from_text_b():
                attr_scores.append(attr_token.get_attr_score())
                tokens["text_b"].append(attr_token.get_token())

            all_attr_scores.append(attr_scores)
            all_tokens.append(tokens)
            logits.append(original_example.get_confidence_score())
            pred_labels.append(label_list[task_name][original_example.get_pred_label()])
            if task_name == "SST":
                ground_truths.append(label_list[task_name][int(0 if float(original_example.get_ground_truth()) < 0.5 else 1)])
            else:
                ground_truths.append(label_list[task_name][int(original_example.get_ground_truth())])

        if attr_scores_only:
            return all_attr_scores

        return all_attr_scores, all_tokens, logits, pred_labels, ground_truths

    def normalize_attribution_scores(attr_scores):
        return attr_scores / max(abs(attr_scores))
        # return np.interp(np.array(attr_scores), (min(attr_scores), max(attr_scores)), (-1, +1))
        # return attr_scores

    analyzers = ["InputMargin"] # , "OccEmpty"
    seeds = [42, 100, 200, 300]
    task_name = "ESNLI"

    pickle_fp = "pickle_files/bert-base-uncased/{}/sanity_check_final_dev_random_classifier/{}/{}/" + FILLED_EXAMPLES + ".pickle"

    print("Average/STD Spearman correlation between heatmaps:")

    for analyzer in analyzers:

        with open(pickle_fp.format(task_name, analyzer, seeds[0]), "rb") as file_path:
            examples1 = pickle.load(file_path)

        for idx, seed in enumerate(seeds[1:]):
            with open(pickle_fp.format(task_name, analyzer, seed), "rb") as file_path:
                examples2 = pickle.load(file_path)

            examples1_filtered, examples2_filtered = [], []
            acc_1, acc_2 = [], []
            spearman_corr_scores = []
            changed_scores = []
            avg_abs_diff = []

            '''
            for temp, (example1, example2) in enumerate(zip(examples1, examples2)):
                if temp == 129:
                    print("DEBUG")

                if task_name == "SST":
                    gt1 = 0 if float(example1['ori_example'].label) < 0.5 else 1
                    gt2 = 0 if float(example2['ori_example'].label) < 0.5 else 1
                else:
                    gt1 = int(example1['ori_example'].label)
                    gt2 = int(example2['ori_example'].label)

                acc_1.append(1 if gt1 == example1['ori_example'].pred_label else 0)
                acc_2.append(1 if gt2 == example2['ori_example'].pred_label else 0)

                if gt1 != example1['ori_example'].pred_label or gt2 != example2['ori_example'].pred_label:
                    continue

                normalized_attribution_scores1 = normalize_attribution_scores(example1['ori_example'].get_attribution_scores())
                normalized_attribution_scores2 = normalize_attribution_scores(example2['ori_example'].get_attribution_scores())

                avg_abs_diff_sent = []
                for attr_score1, attr_score2 in zip(normalized_attribution_scores1, normalized_attribution_scores2):
                    avg_abs_diff_sent.append(abs(attr_score1 - attr_score2))
                avg_abs_diff.append(statistics.mean(avg_abs_diff_sent))

                corr_score, pvalue = stats.spearmanr(normalized_attribution_scores1, normalized_attribution_scores2)
                spearman_corr_scores.append(abs(corr_score))    # ABS
                # spearman_corr_scores.append(corr_score)       # Without ABS

                count_changed_tokens = np.logical_xor(normalized_attribution_scores1 < 0, normalized_attribution_scores2 < 0)
                changed_score = sum(count_changed_tokens) * 1.0 / len(count_changed_tokens)
                changed_scores.append(changed_score)

                examples1_filtered.append(example1)
                examples2_filtered.append(example2)

            print("Number of examples BEFORE and AFTER:\t{}\t{}".format(len(examples1), len(examples1_filtered)))
            print("Accuracy:\t{:.4f}\t{:.4f}".format(statistics.mean(acc_1), statistics.mean(acc_2)))
            print("Spearman correlation:\t{:.4f}\t{:.4f}".format(statistics.mean(spearman_corr_scores), statistics.stdev(spearman_corr_scores)))
            print("Changed score Mean/STD:\t{:.4f}\t{:.4f}".format(statistics.mean(changed_scores), statistics.stdev(changed_scores)))
            print("Average absolute of the differences between the values BEFORE and AFTER randomization:\t{:.4f}\t{:.4f}".format(statistics.mean(avg_abs_diff), statistics.stdev(avg_abs_diff)))
            '''

            if visualize:
                # Print a specific example for paper
                examples1_filtered = np.array([examples1[129]])
                examples2_filtered = np.array([examples2[129]])

                # Print all filtered qualitative examples for checking
                # examples1_filtered = examples1
                # examples2_filtered = examples2

                all_attr_scores1, all_tokens, logits,  pred_labels,  ground_truths = get_attribution_viz_list(examples1_filtered, task_name)
                all_attr_scores2 = get_attribution_viz_list(examples2_filtered, task_name, attr_scores_only=True)

                attribution_viz_list = []
                for idx, (tokens, pred_label, logit, ground_truth, attr_scores1, attr_scores2) in \
                    enumerate(zip(all_tokens, pred_labels, logits, ground_truths, all_attr_scores1, all_attr_scores2)):

                    attribution_viz = AttributionVisualization(tokens, "", pred_label, logit, ground_truth, [], [], [], [],
                                                               normalize_attribution_scores(np.array(attr_scores1)),
                                                               normalize_attribution_scores(np.array(attr_scores2)))
                    attribution_viz.corr_score = spearman_corr_scores[idx] if len(spearman_corr_scores) > 0 else "N/A"
                    attribution_viz_list.append(attribution_viz)

                    if idx == 77:
                        print("DEBUG")

                # Visualize results
                visualize_results_miniQ1(attribution_viz_list, task_name=task_name, model="bert-base-uncased")


def compute_average_top10_likelihoods():
    analyzers = ["InputMargin"]
    seeds = [42]

    # pickle_fp = "pickle_files/bert-base-uncased/ESNLI/sanity_check_final_dev_random_classifier/{}/{}/" + FILLED_EXAMPLES + ".pickle"
    pickle_fp = "pickle_files/bert-base-uncased/MultiRC/final_dev_10/{}/" + "filled_examples_split2.pickle"

    with open(pickle_fp.format(analyzers[0]), "rb") as file_path:
        examples = pickle.load(file_path)

    all_average_likelihoods = []

    for example in examples:
        masked_examples = example["masked_examples"]
        average_likelihoods = []

        for idx, _ in enumerate(example["ori_example"].get_all_attr_tokens()):
            masked_example = masked_examples[idx]
            masked_token = masked_example.get_all_attr_tokens()[idx]
            likelihoods = masked_token.get_candidates()[1][:10]
            average_likelihoods.append(sum(likelihoods))

            # if sum(likelihoods) < 0.5:
            #     print("DEBUG")

        # all_average_likelihoods.append(statistics.mean(average_likelihoods))
        all_average_likelihoods.extend(random.sample(average_likelihoods, k=1))

    print("Average likelihood across SST-2: {}".format(round(statistics.mean(all_average_likelihoods), 4)))


def retrieve_auc_scores():
    import glob
    # file_name = "run_analyzers_logs.txt"
    file_name = "run_evaluation_loo_logs.txt"

    for eval_log_file in glob.glob('pickle_files/bert-base-uncased/SST-2/**/' + file_name, recursive=True):
        print("\n" + eval_log_file)

        input_file = open(eval_log_file, "r")
        for line in input_file:
            if line.strip().startswith("Average AUC score"):
                print(round(float(line.strip().split("=")[1].strip()), 4))

        input_file.close()
def retrieve_roar_scores():
    import glob
    file_name = "eval_results_sst-2.txt"
    results = {}
    keys = [42, 100, 200, 300, 400]

    for eval_log_file in sorted(glob.glob('../../examples/models/bert-base-uncased/SST-2/finetuned_topN_10/**/' + file_name, recursive=True)):
        if eval_log_file.find("original") != -1:
            continue

        print("\n" + eval_log_file)
        items = eval_log_file.split("/")
        seed, analyzer, del_rate = items[-4], items[-3], items[-2]
        if analyzer not in results:
            results[analyzer] = {}

        input_file = open(eval_log_file, "r")
        for line in input_file:
            if line.strip().startswith("eval_acc"):
                acc = round(float(line.strip().split("=")[1].strip()), 4)
                if del_rate not in results[analyzer]:
                    results[analyzer][del_rate] = {}
                results[analyzer][del_rate][seed] = acc
                print(acc)

        input_file.close()

    for analyzer in results.keys():
        print("\nAnalyzer: {}".format(analyzer))
        for del_rate in results[analyzer].keys():
            score_list = []
            for seed in keys:
                if str(seed) in results[analyzer][del_rate]:
                    score_list.append(str(results[analyzer][del_rate][str(seed)]))
            print("\t".join(score_list))
def check_del_examples(roar_rate):
    attr_types = ["InputMargin"] # , "OccEmpty"
    for attr_type in attr_types:
        lite_pickle_fb = "pickle_files/bert-base-uncased/SST-2/final_dev/" + attr_type + "/" + "del_examples_" + str(roar_rate) + "_use_bert.pickle"
        print("********** LOADING PICKLE FILE: " + lite_pickle_fb)
        with open(lite_pickle_fb, "rb") as file_path:
            del_examples = pickle.load(file_path)

        lite_pickle_fb = "pickle_files/bert-base-uncased/SST-2/final_dev/" + attr_type + "/" + FILLED_EXAMPLES + ".pickle"
        print("********** LOADING PICKLE FILE: " + lite_pickle_fb)
        with open(lite_pickle_fb, "rb") as file_path:
            examples = pickle.load(file_path)

        count_identical_cases = 0
        duplicated_rate = []

        for del_example, example in zip(del_examples, examples):
            del_indices = np.argsort(example['ori_example'].attr_scores)[::-1]

            deletion_level = example['ori_example'].get_length() - 1
            deletion_level = math.ceil(deletion_level * roar_rate)
            if deletion_level <= 1:
                deletion_level = 1

            del_indices = del_indices[:deletion_level]
            ori_tokens = (example['ori_example'].get_text_a() + example['ori_example'].get_text_b()).split(" ")
            new_tokens = (del_example.text_a + (" --- " + del_example.text_b if del_example.text_b else "")).split(" ")

            if ori_tokens == new_tokens:
                count_identical_cases += 1

            removed_tokens = [ori_tokens[x] for x in del_indices]
            replaced_tokens = [new_tokens[x] for x in del_indices]

            count = sum([token1 == token2 for token1, token2 in zip(removed_tokens, replaced_tokens)])
            duplicated_rate.append(count / len(removed_tokens))

            print("Original: " + example['ori_example'].get_text_a() + (" --- " + example['ori_example'].get_text_b() if example['ori_example'].get_text_b() else ""))
            print("ROAR_BERT: " + del_example.text_a + (" --- " + del_example.text_b if del_example.text_b else ""))
            print("Removed indices = {}".format(del_indices))
            print("Removed tokens = {}".format(removed_tokens))
            print("Replaced tokens = {}".format(replaced_tokens))
            print("")

        print(count_identical_cases)
        print(len(examples))
        print("Identical cases = {}".format(round(count_identical_cases / len(examples), 4)))
        print("Duplication rate = {}".format(round(statistics.mean(duplicated_rate), 4)))


def verify_ood_problem(topN=10):
    task_names = ["SST", "ESNLI", "MultiRC"]
    analyzers = ["InputMargin", "OccEmpty"]

    label_dict = {"SST": ["0", "1"],
                  "ESNLI": ["0", "1", "2"],
                  "MultiRC": ["0", "1"]}

    for task_name in task_names:
        for analyzer in analyzers:
            prefix = "pickle_files/bert-base-uncased/" + task_name + "/final_dev_10{}/{}/".format("_128" if task_name == "ESNLI" else "", analyzer)
            suffix = "_split2.pickle" if task_name == "MultiRC" else ".pickle"

            lite_pickle_fb = prefix + FILLED_EXAMPLES + suffix
            with open(lite_pickle_fb, "rb") as file_path:
                dev_set = pickle.load(file_path)

            lite_pickle_fb = prefix + ALL_CONF_SCORES + suffix
            with open(lite_pickle_fb, "rb") as file_path:
                all_conf_scores = pickle.load(file_path)

            lite_pickle_fb = prefix + CHUNK_SIZE_LIST + suffix
            with open(lite_pickle_fb, "rb") as file_path:
                chunk_size_list = pickle.load(file_path)

            all_accuracies = []

            for idx in range(len(chunk_size_list)):
                example = dev_set[idx]["ori_example"]
                accuracies = []

                start = 0 if idx == 0 else chunk_size_list[idx - 1]
                end = chunk_size_list[idx]
                conf_scores = all_conf_scores[start:end]
                ori_pred = np.argmax(conf_scores[0])

                assert example.pred_label == ori_pred

                # CORRECT PREDICTIONS ONLY
                if example.label not in label_dict[task_name]:
                    example.label = '1' if float(example.label) >= 0.5 else '0'
                if label_dict[task_name].index(example.label) != example.pred_label:
                    continue

                for idx2, conf_score in enumerate(conf_scores[1:]):
                    if (analyzer == "OccEmpty") or (analyzer == "InputMargin" and idx2 % topN == 0):
                        masked_pred = np.argmax(conf_score)
                        accuracies.append(1 if masked_pred == label_dict[task_name].index(example.label) else 0)

                all_accuracies.append(statistics.mean(accuracies))

            print("Task name: {} --- Number of correctly predicted examples {}/{}".format(task_name, len(all_accuracies), len(chunk_size_list)))
            print("Accuracy drops from 100% to {}%".format(round(statistics.mean(all_accuracies) * 100, 2)))


def verify_ood_problem_lime(task_name, analyzer):
    def generate_lime_examples(explainer, examples, task_name, analyzer, num_samples=1000, max_length=128):
        lime_examples = []

        use_mlm = True if analyzer in ["LIME-BERT", "LIME-BERT-SST2"] else False

        if task_name == "SST":
            for sentence in examples["sentence"]:
                lime_examples.extend(explainer.generate_examples(sentence, num_samples, max_length=max_length, use_mlm=use_mlm,
                                                                 masked_lm=masked_model, masked_tokenizer=masked_tokenizer, masked_token=masked_token))
        elif task_name == "ESNLI":
            for premise, hypothesis in zip(examples["premise"], examples["hypothesis"]):
                occluded_premises = explainer.generate_examples(premise, num_samples, max_length=max_length, use_mlm=use_mlm,
                                                                 masked_lm=masked_model, masked_tokenizer=masked_tokenizer, masked_token=masked_token)
                occluded_hypotheses = explainer.generate_examples(hypothesis, num_samples, max_length=max_length, use_mlm=use_mlm,
                                                                 masked_lm=masked_model, masked_tokenizer=masked_tokenizer, masked_token=masked_token)

                for p, h in zip(occluded_premises, occluded_hypotheses):
                    p_or_h = random.choices([1, 2], k=1)[0]
                    if p_or_h == 1:
                        lime_examples.append([p, hypothesis])
                    else:
                        lime_examples.append([premise, h])
        elif task_name == "MultiRC":
            for paragraph, question, answer in zip(examples["paragraph"], examples["question"], examples["answer"]):
                occluded_paragraphs = explainer.generate_examples(paragraph, num_samples, max_length=max_length, use_mlm=use_mlm,
                                                                 masked_lm=masked_model, masked_tokenizer=masked_tokenizer, masked_token=masked_token)
                occluded_questions = explainer.generate_examples(question, num_samples, max_length=max_length, use_mlm=use_mlm,
                                                                 masked_lm=masked_model, masked_tokenizer=masked_tokenizer, masked_token=masked_token)

                for p, q in zip(occluded_paragraphs, occluded_questions):
                    p_or_q = random.choices([1, 2], k=1)[0]
                    if p_or_q == 1:
                        lime_examples.append([p, question])
                    else:
                        lime_examples.append([paragraph, q])

        return lime_examples

    device = "cuda"
    batch_size = 128
    num_samples = 10

    if task_name == "SST":
        model_path = "../../examples/models/bert-base-uncased/SST-2/finetuned/"
        dataset = load_dataset(task_name.lower())
        max_length = 128
    elif task_name == "ESNLI":
        model_path = "../../examples/models/bert-base-uncased/ESNLI/finetuned_128/"
        dataset = load_dataset(task_name.lower())
        max_length = 128
    else:
        model_path = "pmthangk09/bert-base-uncased-superglue-multirc"
        dataset = load_dataset("super_glue", task_name.lower())
        max_length = 512

    dataset = dataset['validation'] if 'validation' in dataset else dataset['val']
    explainer = LimeTextExplainer()

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    model.eval()

    masked_token = "[MASK]"
    masked_lm_path = "../../examples/models/bert-base-uncased-sst2/" if analyzer == "LIME-BERT-SST2" else "bert-base-uncased"
    masked_tokenizer = AutoTokenizer.from_pretrained(masked_lm_path)
    masked_model = BertForMaskedLM.from_pretrained(masked_lm_path)
    masked_model.to(device)
    masked_model.eval()

    accuracies = []

    for i in tqdm(range(0, len(dataset), batch_size)):
        examples = dataset[i:i+batch_size]
        groundtruths = examples["label"]
        if task_name == "SST":
            groundtruths = [1 if score >= 0.5 else 0 for score in groundtruths]

        # Generate masked examples by different LIME versions depending on the given analyzer
        lime_examples = generate_lime_examples(explainer, examples, task_name, analyzer, num_samples=num_samples, max_length=max_length)
        all_outputs = []

        for j in tqdm(range(0, len(lime_examples), batch_size)):
            lime_sub_examples = lime_examples[j:j+batch_size]
            encoded_inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=lime_sub_examples, max_length=max_length,
                                                         pad_to_max_length=True, return_tensors="pt")

            encoded_inputs.data["input_ids"] = encoded_inputs.data["input_ids"].to(device)
            encoded_inputs.data["attention_mask"] = encoded_inputs.data["attention_mask"].to(device)
            encoded_inputs.data["token_type_ids"] = encoded_inputs.data["token_type_ids"].to(device)

            with torch.no_grad():
                outputs = model(**encoded_inputs)[0].cpu().detach().numpy()
                all_outputs.append(outputs)

        # Get predictions
        predictions = np.argmax(softmax(np.concatenate(all_outputs, axis=0), axis=1), axis=1)
        groundtruths = list(itertools.chain(*[[x] * num_samples for x in groundtruths]))
        accuracies.extend(predictions == groundtruths)

    print("Task name: {} --- Accuracy: {:.4f}".format(task_name, np.array(accuracies).sum() * 1.0 / (len(dataset) * num_samples)))


def compare_cross_entropy_losses_between_masked_language_models():

    def cross_entropy(p, q):
        return -1 * sum(p * np.log2(q))

    def get_distribution_for_masked_sentence(masked_lm, masked_tokenizer, masked_sentence, replaced_token):

        gt_dist = np.zeros(masked_tokenizer.vocab_size)
        gt_dist[masked_tokenizer.vocab[replaced_token]] = 1

        mask_id = masked_tokenizer.convert_tokens_to_ids(masked_token)

        encoded_inputs = masked_tokenizer.batch_encode_plus(batch_text_or_text_pairs=[masked_sentence], max_length=max_length,
                                                            pad_to_max_length=True, return_tensors="pt")

        encoded_inputs.data["input_ids"] = encoded_inputs.data["input_ids"].to(device)
        encoded_inputs.data["attention_mask"] = encoded_inputs.data["attention_mask"].to(device)
        encoded_inputs.data["token_type_ids"] = encoded_inputs.data["token_type_ids"].to(device)

        with torch.no_grad():
            last_hidden_states = masked_lm(**encoded_inputs)[0]

        preds = last_hidden_states[0]
        masked_positions = (np.array(encoded_inputs.data["input_ids"][0].cpu().detach().numpy()) == mask_id).nonzero()[0]
        mask_hidden_state = preds[masked_positions[0]].cpu().detach().numpy()
        pred_dist = softmax(mask_hidden_state)
        # likelihood = pred_dist[masked_tokenizer.vocab[replaced_token]]

        return cross_entropy(p=gt_dist, q=pred_dist)

    dataset = load_dataset("glue", "sst2")
    dataset = dataset['test']

    max_length = 128
    batch_size = 256
    device = "cuda"

    masked_token = "[MASK]"
    new_masked_lm_path = "../../examples/models/bert-base-uncased-sst2/"
    new_masked_tokenizer = AutoTokenizer.from_pretrained(new_masked_lm_path)
    new_masked_model = BertForMaskedLM.from_pretrained(new_masked_lm_path)
    new_masked_model.to(device)
    new_masked_model.eval()

    old_masked_lm_path = "bert-base-uncased"
    old_masked_tokenizer = AutoTokenizer.from_pretrained(old_masked_lm_path)
    old_masked_model = BertForMaskedLM.from_pretrained(old_masked_lm_path)
    old_masked_model.to(device)
    old_masked_model.eval()

    kld_scores_new = []
    kld_scores_old = []

    total_tokens = 0
    num_skipped_tokens = 0

    for i in tqdm(range(0, len(dataset), batch_size)):
        examples = dataset[i:i+batch_size]

        for sentence in examples["sentence"]:
            tokens = sentence.split(" ")
            for j in range(len(tokens)):
                masked_tokens = copy.deepcopy(tokens)
                masked_tokens[j] = masked_token

                total_tokens += 1

                if tokens[j] not in new_masked_tokenizer.vocab or tokens[j] not in old_masked_tokenizer.vocab:
                    num_skipped_tokens += 1
                    continue

                kld_score1 = get_distribution_for_masked_sentence(new_masked_model, new_masked_tokenizer, " ".join(masked_tokens), tokens[j])
                kld_score2 = get_distribution_for_masked_sentence(old_masked_model, old_masked_tokenizer, " ".join(masked_tokens), tokens[j])

                kld_scores_new.append(kld_score1)
                kld_scores_old.append(kld_score2)

    print("Average KL Divergence of bert-base-uncased-SST2: {:.4f} +/- {:.4f}".format(statistics.mean(kld_scores_new), statistics.stdev(kld_scores_new)))
    print("Average KL Divergence of bert-base-uncased: {:.4f} +/- {:.4f}".format(statistics.mean(kld_scores_old), statistics.stdev(kld_scores_old)))
    print("Number of skipped/total tokens = {}/{}".format(num_skipped_tokens, total_tokens))


def merge_qualitative_results_for_analyzing():

    analyzers = ["InputMargin", "OccEmpty"]
    file_dict = {
                 "SST": "qualitative_examples.txt",
                 "ESNLI": "qualitative_examples_lvl1.txt",
                 "MultiRC": "qualitative_examples.txt"
                 }

    fp_format = "pickle_files/bert-base-uncased/{}/final_dev_10{}/{}/{}"

    for task_name, file_name in file_dict.items():

        merged_results = {"InputMargin": {"examples": [], "IoU": [], "precision": [], "recall": []},
                          "OccEmpty": {"examples": [], "IoU": [], "precision": [], "recall": []},
                          "Human": {"examples": []}}

        output_file = open(task_name + "_" + file_name, "w")

        for analyzer in analyzers:
            input_file = open(fp_format.format(task_name, "_128" if task_name == "ESNLI" else "", analyzer, file_name), "r")

            for line in input_file:
                if line.strip().startswith(analyzer + ": "):
                    sent = line.strip().split(analyzer + ": ")[1]
                    merged_results[analyzer]["examples"].append(sent)

                elif analyzer == "InputMargin" and line.strip().startswith("Human: "):
                    sent = line.strip().split("Human: ")[1]
                    merged_results["Human"]["examples"].append(sent)

                elif line.strip().startswith("IoU"):
                    items = line.strip().split(", ")
                    IoU, precision, recall = float(items[0].split(" = ")[1]), float(items[1].split(" = ")[1]), float(items[2].split(" = ")[1])
                    merged_results[analyzer]["IoU"].append(IoU)
                    merged_results[analyzer]["precision"].append(precision)
                    merged_results[analyzer]["recall"].append(recall)

            input_file.close()

            merged_results[analyzer]["examples"] = merged_results[analyzer]["examples"][:-1]
            if analyzer == "InputMargin":
                merged_results["Human"]["examples"] = merged_results["Human"]["examples"][:-1]

        sorted_indices = np.argsort(-(np.array(merged_results["OccEmpty"]["IoU"]) - np.array(merged_results["InputMargin"]["IoU"])))
        for idx in sorted_indices[:100]:
            output_file.write("----- Example {} -----\n".format(idx))

            for analyzer in analyzers:
                IoU = merged_results[analyzer]["IoU"][idx]
                precision = merged_results[analyzer]["precision"][idx]
                recall = merged_results[analyzer]["recall"][idx]

                output_file.write(analyzer + ": " + merged_results[analyzer]["examples"][idx] + "\n")
                output_file.write(analyzer + ": " + "IoU = {}, Precision = {}, Recall = {}\n".format(IoU, precision, recall))

            output_file.write("Human: " + merged_results["Human"]["examples"][idx] + "\n\n")

        output_file.close()

        # print(merged_results)

def analyze_attribution_magnitude():
    analyzers = ["InputMargin", "OccEmpty"]
    file_dict = {
         "SST": ["filled_examples_lite.pickle"],
         "ESNLI": ["filled_examples_lite.pickle"],
         "MultiRC": ["filled_examples_lite_split0.pickle", "filled_examples_lite_split1.pickle", "filled_examples_lite_split2.pickle"]
    }

    fp_format = "pickle_files/bert-base-uncased/{}/final_dev_10{}/{}/{}"

    for task_name, file_names in file_dict.items():
        for file_name in file_names:

            for analyzer in analyzers:
                lite_pickle_fb = fp_format.format(task_name, "_128" if task_name == "ESNLI" else "", analyzer, file_name)
                with open(lite_pickle_fb, "rb") as file_path:
                    dev_set = pickle.load(file_path)

                mean_abs_attr_scores, std_abs_attr_scores = [], []
                top_percent_tokens = 0.2    # 0.2 for 20% and 1.0 for 100%

                for item in tqdm(dev_set):
                    example = item["ori_example"]

                    attr_scores = example.get_attribution_scores()
                    attr_scores = [score if score >= 0 else abs(score) for score in attr_scores]            # Get absolute values
                    attr_scores = attr_scores / max(attr_scores)  # Normalize
                    attr_scores = sorted(attr_scores, reverse=True)[:int(top_percent_tokens * len(attr_scores))]
                    mean_abs_attr_scores.append(0 if len(attr_scores) < 1 else statistics.mean(attr_scores))
                    std_abs_attr_scores.append(0 if len(attr_scores) <= 1 else statistics.stdev(attr_scores))

                print("{}/{}: Mean/STD magnitude of attribution score per word = {:.2f} / {:.2f}".format(task_name, analyzer, statistics.mean(mean_abs_attr_scores), statistics.mean(std_abs_attr_scores)))

def prepare_data_for_FN_analysis(dev_set, groundtruth_sets, task_name, labels, split, max_split, alpha):
    # Each example includes a set of high-attribution words for predicted label (pos/neg)
    sets_1, sets_2 = [], []
    gt_dev_set = [sent for sent in groundtruth_sets if str(sent.split) == '3']   # split = 3 for dev set

    # Skip examples whose lengths after tokenization >= 512 (For MultiRC, max_length of BertMLM = 512)
    # Handle this since gt_dev_set ALWAYS has full examples per split
    skipped_list = skipped_indices[task_name + "_split{}".format(split)] if max_split > 1 else skipped_indices[task_name]
    gt_dev_set = [example for idx, example in enumerate(gt_dev_set) if idx not in skipped_list]

    # USE ONLY FOR ANALYZING ESNLI
    # kept_list = kept_indices[task_name + "_split{}".format(self.model_wrapper.split)] if self.model_wrapper.max_split > 1 else kept_indices[task_name]
    # gt_dev_set = [example for idx, example in enumerate(gt_dev_set) if idx in kept_list]

    # Number of examples must be equal between dev_set and groundtruth
    assert(len(dev_set) == len(gt_dev_set))
    # print("***** Number of evaluated examples in total: {} *****".format(len(self.dev_set)))
    count_correct_preds = 0

    # Generate_high_attribution_set
    for idx, (item, item2) in enumerate(zip(dev_set, gt_dev_set)):
    # for idx, (item, item2) in tqdm(enumerate(zip(self.dev_set, gt_dev_set))):

        example = item["ori_example"]
        pred_label = example.get_pred_label()
        all_tokens = [x.token for x in example.get_all_attr_tokens()]

        # CORRECT PREDICTIONS ONLY
        if example.label not in labels:
            example.label = '1' if float(example.label) >= 0.5 else '0'
        if labels.index(example.label) != example.pred_label:
            sets_1.append([])
            sets_2.append([])
            continue

        count_correct_preds += 1

        # Prepare set0 (baseline) and set1 (RIS or OccEmpty)
        # Normalize + Zero-out and binarize
        if max(example.get_attribution_scores()) > 0:
            attr_scores = example.get_attribution_scores() / max(example.get_attribution_scores())
            attr_scores = [1 if score >= alpha else 0 for score in attr_scores]
        else:
            attr_scores = [0] * len(example.get_attribution_scores())

        # Number of attribution scores and tokens must be equal for each example
        # Attribution score must be in [0, 1]
        assert len(all_tokens) == len(attr_scores)
        assert min(attr_scores) >= 0 and max(attr_scores) <= 1

        high_attr_tokens = [idx for idx, attr_score in enumerate(attr_scores) if attr_score == 1]
        sets_1.append(high_attr_tokens)

        # Prepare set2: Groundtruth
        set2 = []
        if hasattr(item2, "phrases"):   # SST
            for phrase in item2.phrases:
                # positive = 1 or negative = 0
                if (pred_label == 1 and float(phrase.score) >= 0.7) or (pred_label == 0 and float(phrase.score) <= 0.3):
                    doc = nlp(phrase.text)
                    phrase_tokens = [token.text for token in doc]
                    for i in range(len(all_tokens) - len(phrase_tokens) + 1):
                        # if sub tokens are found in all_tokens and its length <= 50% of sentence length
                        if (all_tokens[i:i + len(phrase_tokens)] == phrase_tokens) and (len(phrase_tokens) <= len(all_tokens) / 2):
                            set2.append([idx for token, idx in zip(phrase_tokens, range(i, i + len(phrase_tokens), 1))])
            sets_2.append(list(set(itertools.chain(*set2))))

        elif hasattr(item2, "highlights"):  # ESNLI and MultiRC
            # ThangPM: Temporarily hot fixed.
            # Handle 24 cases that have empty tokens in either hypothesis or premise of sets_1.
            # Reason: Due to redundant spaces in the Huggingface datasets used --> Need to process this next time.
            if item2.processed_length != len(all_tokens):
                del_indices = [i for i, x in enumerate(all_tokens) if x == " "]
                all_tokens = [value for idx, value in enumerate(all_tokens) if idx not in del_indices]
                if item2.processed_length != len(all_tokens):
                    print(all_tokens)
                else:
                    attr_scores = [value for idx, value in enumerate(attr_scores) if idx not in del_indices]
                    high_attr_tokens = [idx for idx, attr_score in enumerate(attr_scores) if attr_score == 1]
                    sets_1[-1] = high_attr_tokens

            set2 = sorted([idx for idx, token in item2.highlights["text_a"] + item2.highlights["text_b"]])
            sets_2.append(set2)

    return sets_1, sets_2

def analyze_mlm_behavior():
    from nltk.corpus import wordnet as wn
    def get_synonyms_antonyms(word):
        synonyms, antonyms = [], []
        for syn in wn.synsets(word):
            for lemma in syn.lemmas():
                synonyms.append(lemma.name().lower())
                if lemma.antonyms():
                    antonyms.append(lemma.antonyms()[0].name().lower())
        return list(set(synonyms)), list(set(antonyms))

    analyzers = ["InputMargin"]
    file_dict = {
         "SST": ["filled_examples.pickle"],
         "ESNLI": ["filled_examples.pickle"],
         "MultiRC": ["filled_examples_split0.pickle", "filled_examples_split1.pickle", "filled_examples_split2.pickle"]
    }
    label_dict = {
        "SST": ["0", "1"],
        "ESNLI": ["0", "1", "2"],
        "MultiRC": ["0", "1"]
    }

    fp_format = "pickle_files/bert-base-uncased/{}/final_dev_10{}/{}/{}"
    gt_format = "pickle_files/{}_preprocessed{}.pickle"
    highlight_format = ';'.join([str(0), str(30), str(43)]) # token's foreground is black, token's background is orange/yellow

    for task_name, file_names in file_dict.items():
        max_split = len(file_names)

        for file_name in file_names:
            # Prepare groundtruth set
            groundtruth_path = gt_format.format(task_name.lower(), ("_1" if task_name.lower() == "esnli" else ""))
            with open(groundtruth_path, "rb") as file_path:
                groundtruth_sets = pickle.load(file_path)

            split = None if max_split == 1 else int(file_name.split("split")[-1].split(".")[0])
            if split != None:
                chunk_size = int(len(groundtruth_sets) / max_split)
                groundtruth_sets = groundtruth_sets[split * chunk_size:(split + 1) * chunk_size]

            for analyzer in analyzers:
                lite_pickle_fb = fp_format.format(task_name, "_128" if task_name.lower() == "esnli" else "", analyzer, file_name)
                with open(lite_pickle_fb, "rb") as file_path:
                    dev_set = pickle.load(file_path)

                avg_synonym, avg_antonym, avg_duplicated = [], [], []
                avg_prob_synonym, avg_prob_antonym, avg_prob_duplicated = [], [], []

                sets_1, sets_2 = prepare_data_for_FN_analysis(dev_set, groundtruth_sets, task_name.lower(), label_dict[task_name], split, max_split, alpha=0.05)
                analysis_dict = {"s1": [], "s2": [], "s1_s2": [], "s2_highlight": []}

                for ex_idx, item in tqdm(enumerate(dev_set)):

                    example = item["ori_example"]
                    masked_examples = item["masked_examples"]

                    tokens = example.get_attr_token_list_from_text_a() + example.get_attr_token_list_from_text_b()
                    count_synonym, count_antonym, count_duplicated = 0, 0, 0
                    prob_synonym, prob_antonym, prob_duplicated = 0, 0, 0

                    # print("\n\n---------- Example {} ----------".format(ex_idx))
                    # print("Original sentence: {} {}".format(example.get_text_a(), example.get_text_b()))

                    s1, s2, s1_s2, s2_highlight = [], [], [], []
                    for idx, token in enumerate(tokens):

                        masked_tokens = masked_examples[idx].get_all_attr_tokens()

                        # print("\nMasked sentence: {} {}".format(masked_examples[idx].get_text_a(), masked_examples[idx].get_text_b()))
                        # print("Masked token: {} --- Top1: {} (MLM prob = {:.4f})".format('\x1b[%sm%s\x1b[0m' % (highlight_format, token.get_token().lower()),
                        #                                                                  '\x1b[%sm%s\x1b[0m' % (highlight_format, masked_tokens[idx].get_candidates()[0][0].lower()),
                        #                                                                  masked_tokens[idx].get_candidates()[1][0]))

                        # Duplications with ONLY human highlighted words
                        if idx in sets_2[ex_idx]:
                            # were not changed
                            if token.get_token().lower() == masked_tokens[idx].get_candidates()[0][0].lower():
                                s2_highlight.append(1)
                            else:
                                s2_highlight.append(0)

                        # Duplication with all tokens in each sentence
                        if token.get_token().lower() == masked_tokens[idx].get_candidates()[0][0].lower():
                            s1.append(1 if idx not in sets_1[ex_idx] else 0)
                            s2.append(1 if idx in sets_2[ex_idx] else 0)
                            s1_s2.append(1 if (idx not in sets_1[ex_idx]) and (idx in sets_2[ex_idx]) else 0)
                            count_duplicated += 1
                            prob_duplicated += masked_tokens[idx].get_candidates()[1][0]    # Get mlm prob of top-1 suggested word
                        else:
                            synonyms, antonyms = get_synonyms_antonyms(token.get_token().lower())
                            if masked_tokens[idx].get_candidates()[0][0].lower() in synonyms:
                                count_synonym += 1
                                prob_synonym += masked_tokens[idx].get_candidates()[1][0]
                            elif masked_tokens[idx].get_candidates()[0][0].lower() in antonyms:
                                count_antonym += 1
                                prob_antonym += masked_tokens[idx].get_candidates()[1][0]

                    if len(s2_highlight) > 0:
                        analysis_dict["s2_highlight"].append(statistics.mean(s2_highlight))
                    if len(s1) > 0:
                        analysis_dict["s1"].append(statistics.mean(s1))
                    if len(s2) > 0:
                        analysis_dict["s2"].append(statistics.mean(s2))
                    if len(s1_s2) > 0:
                        analysis_dict["s1_s2"].append(statistics.mean(s1_s2))

                    # print("Duplicated score = {:.4f}".format(count_duplicated / len(tokens)))
                    avg_duplicated.append(count_duplicated / len(tokens))
                    avg_prob_duplicated.append(0 if count_duplicated == 0 else prob_duplicated / count_duplicated)

                    avg_synonym.append(count_synonym / len(tokens))
                    avg_prob_synonym.append(0 if count_synonym == 0 else prob_synonym / count_synonym)

                    avg_antonym.append(count_antonym / len(tokens))
                    avg_prob_antonym.append(0 if count_antonym == 0 else prob_antonym / count_antonym)

                print("{:.4f} of the time top-1 suggested word is duplicated (avg prob = {:.4f}).".format(statistics.mean(avg_duplicated), statistics.mean(avg_prob_duplicated)))
                # print("{:.4f} of the time top-1 suggested word is synonym (avg prob = {:.4f}).".format(statistics.mean(avg_synonym), statistics.mean(avg_prob_synonym)))
                # print("{:.4f} of the time top-1 suggested word is antonym (avg prob = {:.4f}).".format(statistics.mean(avg_antonym), statistics.mean(avg_prob_antonym)))
                print("{:.4f} duplicated words are unimportant.".format(statistics.mean(analysis_dict["s1"])))
                print("{:.4f} duplicated words are important to human.".format(statistics.mean(analysis_dict["s2"])))
                print("{:.4f} duplicated words are unimportant to analyzer but important to human.".format(statistics.mean(analysis_dict["s1_s2"])))
                print("{}/{} = {:.4f} human-highlighted words were not change after replacing.".format(sum(analysis_dict["s2_highlight"]), len(analysis_dict["s2_highlight"]), statistics.mean(analysis_dict["s2_highlight"])))

def plot_mini_Q1_table():

    fig = plt.figure(figsize=(6.4, 5.2), dpi=100)
    fig.tight_layout()

    RIS = {"precision": [0.3757, 0.4845, 0.5085, 0.5227, 0.534, 0.5412, 0.5446, 0.5549, 0.5638, 0.5688, 0.5717, 0.5722, 0.5784, 0.5796, 0.5799, 0.5833, 0.5868, 0.5854, 0.5883, 0.5901],
           "recall": [0.8015, 0.5588, 0.5022, 0.4684, 0.4468, 0.4307, 0.4198, 0.4089, 0.3991, 0.3884, 0.3816, 0.3738, 0.3683, 0.3625, 0.3555, 0.3527, 0.3478, 0.3434, 0.3414, 0.337]}

    OccEmpty = {"precision": [0.3856, 0.4471, 0.4794, 0.5112, 0.5252, 0.5385, 0.5514, 0.5641, 0.5704, 0.5756, 0.5773, 0.5822, 0.5837, 0.5869, 0.5906, 0.5943, 0.5926, 0.5917, 0.5923, 0.5923],
                "recall": [0.7587, 0.6411, 0.5744, 0.5361, 0.502, 0.4709, 0.4486, 0.4324, 0.419, 0.4076, 0.394, 0.3878, 0.3774, 0.3702, 0.3658, 0.3609, 0.3548, 0.3476, 0.3446, 0.3399]}

    # UNIGRAM ONLY
    plt.plot(RIS["recall"], RIS["precision"], marker="o", markersize=1, fillstyle="none", linestyle="solid", label='RIS', alpha=0.8)
    plt.plot(OccEmpty["recall"], OccEmpty["precision"], marker="^", markersize=1, fillstyle="none", linestyle=":", label='OccEmpty', alpha=0.8)

    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)

    plt.xticks(np.arange(0, 1.1, 0.1), rotation=0, fontsize=15)
    plt.yticks(np.arange(0, 1.1, 0.1), fontsize=15)

    plt.legend(loc='best', fontsize=15)
    plt.grid(True)

    plt.show()
    fig.savefig("precision_recall_plot.jpg")
def plot_human_evaluation():

    def load_data(file_path):
        input_file = open(file_path, "r")
        results = {}
        scores = []

        for idx, line in enumerate(input_file):
            items = line.strip().split("\t")
            items = [item for item in items if item]

            if idx == 0:
                for item in items: results[item] = {}
            elif idx == 1:
                for key in results.keys():
                    for item in items[:4]:
                        results[key][item] = []
            else:
                scores.append([float(item) for item in items])

        scores = np.array(scores).T

        for idx1, key1 in enumerate(results.keys()):
            for idx2, key2 in enumerate(results[key1].keys()):
                if idx2 == 0:
                    results[key1][key2] = scores[0]
                else:
                    results[key1][key2] = scores[idx1*3 + idx2]

        return results

    fig = plt.figure(figsize=(9.6, 6.4), dpi=100)
    fig.tight_layout()

    results = load_data("human_results/sst.txt")

    # UNIGRAM ONLY
    plt.plot(results["IoU"]["Threshold"], results["IoU"]["RIS-10"], marker="o", markersize=5, fillstyle="none", linestyle="solid", label='IM (IoU)', alpha=0.8)
    plt.plot(results["IoU"]["Threshold"], results["IoU"]["OccEmpty"], marker="^", markersize=5, fillstyle="none", linestyle=":", label='OccEmpty (IoU)', alpha=0.8)
    plt.plot(results["F1"]["Threshold"], results["F1"]["RIS-10"], marker="o", markersize=5, fillstyle="none", linestyle="solid", label='IM (F1)', alpha=0.8)
    plt.plot(results["F1"]["Threshold"], results["F1"]["OccEmpty"], marker="^", markersize=5, fillstyle="none", linestyle=":", label='OccEmpty (F1)', alpha=0.8)

    plt.xlabel("Threshold", fontsize=16)
    plt.ylabel("IoU / F1", fontsize=16)

    plt.xticks(np.arange(0, 1.1, 0.1), rotation=0, fontsize=15)
    plt.yticks(np.arange(0, 1.1, 0.1), fontsize=15)

    plt.legend(loc='best', fontsize=15)
    plt.grid(True)

    plt.show()
    fig.savefig("sst_IoU_plot.jpg")
def plot_teaser_and_figures():

    def highlight_text(analyzer, tokens, highlight_set, latex=True):

        # token's foreground is black, token's background is orange/yellow
        format = ';'.join([str(0), str(30), str(43)])
        highlight_tokens_set = copy.deepcopy(tokens)

        for idx, token in enumerate(tokens):
            if not latex:
                highlight_tokens_set[idx] = '\x1b[%sm%s\x1b[0m' % (format, token) if idx in highlight_set else token            # Highlight console
            else:
                highlight_tokens_set[idx] = '\\colorbox{yellow!100}{\\strut %s}' % (token) if idx in highlight_set else token   # Highlight LaTeX

        print(analyzer + ": " + " ".join(highlight_tokens_set))  # highlight important words in the input highlight_set

    def get_lable_name(task_name, label_dict, label_name_dict, label):
        return label_name_dict[task_name][label_dict[task_name].index(label)]

    def intersection(lst1, lst2):
        # return list(set(lst1) & set(lst2))
        return list((Counter(lst1) & Counter(lst2)).elements())

    def union(lst1, lst2):
        # return list(set().union(lst1, lst2))
        return list((Counter(lst1) | Counter(lst2)).elements())

    def compute_scores(sets_1, sets_2):
        IoU_scores = []

        for set1, set2 in zip(sets_1, sets_2):
            if len(union(set1, set2)) == 0:
                IoU_scores.append(1)  # Assume that this example has no important words
                continue

            IoU_score = len(intersection(set1, set2)) / len(union(set1, set2))
            IoU_scores.append(IoU_score)

        precision_scores, recall_scores = [], []
        for set1, set2 in zip(sets_1, sets_2):
            if len(set2) == 0:
                precision_score = 1 if len(set1) == 0 else 0
                recall_score = 1
            elif len(set1) == 0:
                precision_score = recall_score = 0
            else:
                precision_score = len(intersection(set1, set2)) / len(set1)
                recall_score = len(intersection(set1, set2)) / len(set2)

            precision_scores.append(precision_score)
            recall_scores.append(recall_score)

        avg_IoU_score = round(sum(IoU_scores) / len(IoU_scores), 4)
        avg_precision_score = round(sum(precision_scores) / len(precision_scores), 4)
        avg_recall_score = round(sum(recall_scores) / len(recall_scores), 4)

        return avg_IoU_score, avg_precision_score, avg_recall_score

    def plot_figure_one():
        sample_size = 3
        duplicate_indices = []

        for idx, token in enumerate(all_tokens):

            # CRITERIA 1 + 2: Duplicate AND Important to human but was missing by IM
            if token.lower() == example.get_all_attr_tokens()[idx].get_candidates()[0][0].lower():              # Top-1 BERT token is duplicate with the original one
                if idx in highlight_dict["gt"][ex_idx] and idx not in highlight_dict["InputMargin"][ex_idx]:    # Important to human but was missing by IM
                    duplicate_indices.append(idx)

        random.shuffle(duplicate_indices)

        counter = 0
        for idx in duplicate_indices:

            # CRITERIA 3: ONLY get examples whose first likelihood > 0.9
            if example.get_all_attr_tokens()[idx].get_candidates()[1][0] < 0.7:
                continue

            # token's foreground is black, token's background is orange/yellow
            format = ';'.join([str(0), str(30), str(43)])
            highlight_tokens_set1 = copy.deepcopy(all_tokens)
            highlight_tokens_set1[idx] = '\x1b[%sm%s\x1b[0m' % (format, all_tokens[idx])
            print("\n" + analyzers[0] + ": " + " ".join(highlight_tokens_set1))

            # Print out the first three suggestions from BERT
            for candidate_idx in range(sample_size):
                # Prediction and confidence score for BERT suggestions
                pred_label = label_name_dict[task_name][np.argmax(example.get_all_attr_tokens()[idx].get_candidates()[2][candidate_idx])]
                conf_score = round(np.max(example.get_all_attr_tokens()[idx].get_candidates()[2][candidate_idx]), 4)

                candidate = example.get_all_attr_tokens()[idx].get_candidates()[0][candidate_idx]
                likelihood = example.get_all_attr_tokens()[idx].get_candidates()[1][candidate_idx]
                print("{:.4f}\t\t{}".format(likelihood, candidate))

            counter += 1

        if counter >= 3:
            print("CHECK HERE")

    def plot_figure_two():

        set1_im = highlight_dict["InputMargin"][ex_idx]
        set1_loo = highlight_dict["OccEmpty"][ex_idx]
        set2_gt = highlight_dict["gt"][ex_idx]

        IoU_im, prec_im, recall_im = compute_scores([set1_im], [set2_gt])
        IoU_loo, prec_loo, recall_loo = compute_scores([set1_loo], [set2_gt])

        if IoU_loo > IoU_im and prec_loo > prec_im and recall_loo > recall_im:
            for analyzer in analyzers:
                highlight_text(analyzer, all_tokens, highlight_dict[analyzer][ex_idx])

            print("CHECK HERE {} vs {}, {} vs {}, {} vs {}".format(IoU_loo, IoU_im, prec_loo, prec_im, recall_loo, recall_im))

    def plot_figure_three(latex=True):
        item_loo = data_dict["OccEmpty"][ex_idx]["ori_example"]

        for idx in range(len(all_tokens)):
            # token's foreground is black, token's background is orange/yellow
            format = ';'.join([str(0), str(30), str(43)])
            highlight_tokens_set = copy.deepcopy(all_tokens)

            if not latex:
                highlight_tokens_set[idx] = '\x1b[%sm%s\x1b[0m' % (format, all_tokens[idx])                 # Highlight console
            else:
                highlight_tokens_set[idx] = '\\colorbox{green!50}{\\strut \\st{%s}}' % (all_tokens[idx])    # Highlight LaTeX

            pred_label = label_name_dict[task_name][np.argmax(item_loo.get_all_attr_tokens()[idx].get_candidates()[2][idx])]
            conf_score = round(np.max(item_loo.get_all_attr_tokens()[idx].get_candidates()[2][idx]), 4)
            print("{}\t{}\t{:.2f}".format(" ".join(highlight_tokens_set), pred_label, conf_score))


    analyzers = ["InputMargin", "OccEmpty"]
    file_dict = {
         # "SST": ["filled_examples_lite.pickle"],
         "ESNLI": ["filled_examples_lite.pickle"],
         # "MultiRC": ["filled_examples_lite_split0.pickle", "filled_examples_lite_split1.pickle", "filled_examples_lite_split2.pickle"]
    }
    label_dict = {
        "SST": ["0", "1"],          # ["negative", "positive"]
        "ESNLI": ["0", "1", "2"],   # ["entailment", "neutral", "contradiction"]
        "MultiRC": ["0", "1"]       # ["False", "True"]
    }
    label_name_dict = {
        "SST": ["negative", "positive"],
        "ESNLI": ["entailment", "neutral", "contradiction"],
        "MultiRC": ["False", "True"]
    }

    fp_format = "pickle_files/bert-base-uncased/{}/final_dev_10{}/{}/{}"
    gt_format = "pickle_files/{}_preprocessed{}.pickle"

    for task_name, file_names in file_dict.items():
        max_split = len(file_names)

        print("\n\n\n********** TASK NAME: {} **********".format(task_name))

        for file_name in file_names:
            # Prepare groundtruth set
            groundtruth_path = gt_format.format(task_name.lower(), ("_2" if task_name.lower() == "esnli" else ""))
            with open(groundtruth_path, "rb") as file_path:
                groundtruth_sets = pickle.load(file_path)

            split = None if max_split == 1 else int(file_name.split("split")[-1].split(".")[0])
            if split != None:
                chunk_size = int(len(groundtruth_sets) / max_split)
                groundtruth_sets = groundtruth_sets[split * chunk_size:(split + 1) * chunk_size]

            # Prepare dictionaries for dev set and highlight set
            highlight_dict, data_dict = {}, {}
            for analyzer in analyzers:
                lite_pickle_fb = fp_format.format(task_name, "_128" if task_name.lower() == "esnli" else "", analyzer, file_name)
                with open(lite_pickle_fb, "rb") as file_path:
                    dev_set = pickle.load(file_path)

                if task_name == "MultiRC" and analyzer == "OccEmpty":
                    skipped_list = skipped_indices[task_name.lower() + "_split{}".format(split)] if max_split > 1 else skipped_indices[task_name]
                    dev_set = [example for idx, example in enumerate(dev_set) if idx not in skipped_list]

                sets_1, sets_2 = prepare_data_for_FN_analysis(dev_set, groundtruth_sets, task_name.lower(), label_dict[task_name], split, max_split, alpha=0.05)

                data_dict[analyzer] = dev_set
                highlight_dict[analyzer] = sets_1
                highlight_dict["gt"] = sets_2

            # Visualize examples with Latex
            # visualize_highlights(task_name, analyzers, data_dict, highlight_dict)

            for ex_idx, (item_im, item_loo) in enumerate(zip(data_dict[analyzers[0]], data_dict[analyzers[1]])):

                # FOR DEBUGGING ONLY
                if ex_idx != 129:
                    continue

                example = item_im["ori_example"]
                all_tokens = [x.token for x in example.get_all_attr_tokens()]

                print("\n\n---------- Example {} ---------- GT: {} - Pred: {} - Confidence: {:.2f}".format(ex_idx,
                                                                                                       get_lable_name(task_name, label_dict, label_name_dict, example.get_ground_truth()),
                                                                                                       label_name_dict[task_name][example.get_pred_label()],
                                                                                                       example.get_confidence_score()))
                # CRITERIA 0: GT must highlight less than 1/2 number of tokens
                # if len(highlight_dict["gt"][ex_idx]) >= int(len(all_tokens) / 2) or len(all_tokens) >= 150:
                #     continue

                # Visualize original sentence with Human highlights
                highlight_text("Original sentence", all_tokens, highlight_dict["gt"][ex_idx], latex=False)

                plot_figure_one()
                # plot_figure_two()
                # plot_figure_three()

                # For statistics ONLY
                '''
                duplicates = []
                for idx, token in enumerate(all_tokens):
                    if idx in highlight_dict["gt"][ex_idx]:
                        if token.lower() == example.get_all_attr_tokens()[idx].get_candidates()[0][0].lower():
                            duplicates.append((token, round(example.get_all_attr_tokens()[idx].get_candidates()[1][0], 4),))
                print("\nDuplicates between top-1 suggestion and Human highlights (word, likelihood): \n{}".format(duplicates))
                '''


def plot_lime_comparison_overall():
    fig = plt.figure(figsize=(8, 6), dpi=200)
    fig.tight_layout()

    # RoBERTa
    lime_roar = {"0%": [92.62, 0.3], "10%": [75.51, 0.55], "20%": [75.30, 0.8], "30%": [77.45, 0.7]}
    lime_roarbert = {"0%": [92.62, 0.3], "10%": [78.14, 0.54], "20%": [73.44, 0.65], "30%": [70.57, 0.56]}
    limebert_roar = {"0%": [92.62, 0.3], "10%": [73.99, 0.74], "20%": [72.22, 0.73], "30%": [70.82, 0.86]}
    limebert_roarbert = {"0%": [92.62, 0.3], "10%": [74.13, 0.72], "20%": [70.44, 0.86], "30%": [70.48, 0.63]}

    ranges = []
    lime_roar_data, lime_roarbert_data = {"x": [], "y": [], "yerr": []}, {"x": [], "y": [], "yerr": []}
    limebert_roar_data, limebert_roarbert_data = {"x": [], "y": [], "yerr": []}, {"x": [], "y": [], "yerr": []}

    for _range in lime_roar.keys():
        ranges.append(_range)

        lime_roar_data["x"].append(_range)
        lime_roar_data["y"].append(lime_roar[_range][0])
        lime_roar_data["yerr"].append(lime_roar[_range][1])

        lime_roarbert_data["x"].append(_range)
        lime_roarbert_data["y"].append(lime_roarbert[_range][0])
        lime_roarbert_data["yerr"].append(lime_roarbert[_range][1])

        limebert_roar_data["x"].append(_range)
        limebert_roar_data["y"].append(limebert_roar[_range][0])
        limebert_roar_data["yerr"].append(limebert_roar[_range][1])

        limebert_roarbert_data["x"].append(_range)
        limebert_roarbert_data["y"].append(limebert_roarbert[_range][0])
        limebert_roarbert_data["yerr"].append(limebert_roarbert[_range][1])

    # Average + STD based on 3 models
    datas = [lime_roar_data, limebert_roar_data, lime_roarbert_data, limebert_roarbert_data]
    labels = ["LIME              (ROAR)",
              "LIME_BERT  (ROAR)",
              "LIME              (ROAR-BERT)",
              "LIME_BERT  (ROAR-BERT)"]
    colors = ["tab:red", "tab:green", "tab:blue", "tab:orange"]
    fmts = [":", "-", ":", "-"]
    markers = ["^", "s", "^", "s"]

    for data, label, color, fmt, marker in zip(datas, labels, colors, fmts, markers):
        plt.errorbar(**data, alpha=.75, fmt=fmt, capsize=5, capthick=2, label=label, color=color, marker=marker)
        data = {
            'x': data['x'],
            'y1': [y - e for y, e in zip(data['y'], data['yerr'])],
            'y2': [y + e for y, e in zip(data['y'], data['yerr'])]}
        plt.fill_between(**data, alpha=.25, color=color)

    plt.xlabel("% token removal", fontsize=18)
    plt.ylabel("Accuracy (%)", fontsize=18)

    plt.xticks(np.arange(0, len(ranges), 1), ranges, rotation=0, fontsize=18)
    plt.yticks(np.arange(65, 105, 5), fontsize=18)

    plt.legend(loc='best', fontsize=16)
    plt.grid(True)
    # plt.title('')

    plt.show()
    fig.savefig("lime_vs_limebert.pdf")


@my_timer
def generate_lite_pickles():
    analyzers = ["InputMargin", "OccEmpty"]
    file_dict = {
        "SST": ["filled_examples.pickle"],
        "ESNLI": ["filled_examples.pickle"],
        "MultiRC": ["filled_examples_split0.pickle", "filled_examples_split1.pickle", "filled_examples_split2.pickle"]
    }
    fp_format = "pickle_files/bert-base-uncased/{}/final_dev_10{}/{}/{}"

    for task_name, file_names in file_dict.items():
        for file_name in file_names:
            for analyzer in analyzers:
                print("Handling {} -- {} -- {}".format(task_name, file_name, analyzer))
                pickle_fb = fp_format.format(task_name, "_128" if task_name.lower() == "esnli" else "", analyzer, file_name)
                with open(pickle_fb, "rb") as file_path:
                    dev_set = pickle.load(file_path)
                with open(pickle_fb.replace(FILLED_EXAMPLES, ALL_CONF_SCORES), "rb") as file_path:
                    all_conf_scores = pickle.load(file_path)
                with open(pickle_fb.replace(FILLED_EXAMPLES, CHUNK_SIZE_LIST), "rb") as file_path:
                    chunk_size_list = pickle.load(file_path)

                for ex_idx, item in tqdm(enumerate(dev_set)):
                    example = item["ori_example"]
                    masked_examples = item["masked_examples"]

                    start = 0 if ex_idx == 0 else chunk_size_list[ex_idx - 1]
                    end = chunk_size_list[ex_idx]
                    conf_scores = all_conf_scores[start:end][1:]    # Skip the first conf_score of original example

                    for idx, token in enumerate(example.get_all_attr_tokens()):
                        candidates = list(masked_examples[idx].get_candidates())
                        if analyzer == "InputMargin":
                            top_N = int(len(conf_scores) / len(example.get_all_attr_tokens()))
                            candidates.append(conf_scores[idx*top_N : (idx+1)*top_N])
                        else:
                            candidates = list(([], []))
                            candidates.append(conf_scores)
                        token.set_candidates(candidates)
                    item.pop("masked_examples")
                    item.pop("total_len")

                new_file_name = file_name.replace("filled_examples", "filled_examples_lite")
                lite_pickle_fb = fp_format.format(task_name, "_128" if task_name.lower() == "esnli" else "", analyzer, new_file_name)
                with open(lite_pickle_fb, "wb") as file_path:
                    pickle.dump(dev_set, file_path)


if __name__ == '__main__':
    # run_analyzers(top_N=10, ris_type="local_softmax", threshold=10e-5)

    # compute_spearman_correlation()
    # compute_average_top10_likelihoods()

    # For ANALYZING the results
    # verify_ood_problem()

    # task_names = ["SST", "ESNLI", "MultiRC"]
    # analyzers = ["LIME", "LIME-BERT", "LIME-BERT-SST2"]
    # verify_ood_problem_lime(task_name="ESNLI", analyzer="LIME")
    compare_cross_entropy_losses_between_masked_language_models()

    # plot_lime_comparison_overall()

    # merge_qualitative_results_for_analyzing()
    # analyze_attribution_magnitude()
    # analyze_mlm_behavior()

    # plot_human_evaluation()
    # plot_mini_Q1_table()

    # generate_lite_pickles()
    # plot_teaser_and_figures()

    # retrieve_auc_scores()
    # retrieve_roar_scores()
    # check_del_examples(0.2)

